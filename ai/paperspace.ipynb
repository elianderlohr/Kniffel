{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ezGGzkUkfi",
        "outputId": "40b3577e-018e-4753-bc33-7642ceae0587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl\n",
            "  Downloading keras-rl-0.4.2.tar.gz (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl) (2.8.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-py3-none-any.whl size=48378 sha256=7b2093ffff906e71b6ac5feec80838abae866f3748b029fe9d1840a221c22b4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/23/e9/278c2e59c322236e2bfdf7c792c16f0b4dec24816d27a3f1e4\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 677 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.26.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2022.5.18.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl\n",
        "!pip install keras-rl2\n",
        "!pip install gym\n",
        "!pip install sympy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lop1FAXFctN0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'Kniffel')\n",
        "sys.path.insert(0, 'Kniffel/ai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3A2YD8QYctN0"
      },
      "outputs": [],
      "source": [
        "from ai.hyperparameter import Hyperparameter\n",
        "from ai.ai import KniffelAI\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ILWBfrctN1",
        "outputId": "98cbca70-4223-414d-92c8-33d1717d3d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 2400 combinations to test.\n"
          ]
        }
      ],
      "source": [
        "units = list(range(16, 64, 16))\n",
        "\n",
        "base_hp = {\n",
        "    \"windows_length\": [1],\n",
        "    \"adam_learning_rate\": [0.0001, 0.0005, 0.001, 0.00146, 0.005, 0.01],\n",
        "    \"adam_epsilon\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
        "    \"batch_size\": [32],\n",
        "    \"target_model_update\": [0.0001, 0.0005, 0.001, 0.00146, 0.005, 0.01],\n",
        "    \"dueling_option\": [\"avg\"],\n",
        "    \"activation\": [\"linear\"],\n",
        "    \"layers\": [1, 2],\n",
        "    \"unit_1\": units,\n",
        "    \"unit_2\": units,\n",
        "}\n",
        "\n",
        "ai = KniffelAI(\n",
        "    save=True, load=False, predefined_layers=True, hyperparater_base=base_hp, path_prefix=\"../Kniffel/\"\n",
        ")\n",
        "\n",
        "env_config = {\n",
        "        \"reward_step\": 0,\n",
        "        \"reward_round\": 0.5,\n",
        "        \"reward_roll_dice\": 0.25,\n",
        "        \"reward_game_over\": -10,\n",
        "        \"reward_slash\": -5,\n",
        "        \"reward_bonus\": 2,\n",
        "        \"reward_finish\": 10,\n",
        "        \"reward_zero_dice\": -0.5,\n",
        "        \"reward_one_dice\": -0.2,\n",
        "        \"reward_two_dice\": -0.1,\n",
        "        \"reward_three_dice\": 0.5,\n",
        "        \"reward_four_dice\": 0.6,\n",
        "        \"reward_five_dice\": 0.8,\n",
        "        \"reward_six_dice\": 1,\n",
        "        \"reward_kniffel\": 1.5,\n",
        "        \"reward_small_street\": 1,\n",
        "        \"reward_large_street\": 1.1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJR2jjqxkEvt",
        "outputId": "da925aab-2dd8-4285-b469-707a2f5381e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 41)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "break_counter: 1000\n",
            "AVG. 17.771496625645096\n",
            "MAX: 56\n",
            "MIN: 0\n",
            "AVG rounds: 6.693316084519189\n",
            "Max rounds: 21\n",
            "Min rounds: 1\n"
          ]
        }
      ],
      "source": [
        "# ai.play(path=\"Kniffel/weights/p_date=2022-06-04-06_57_09\", episodes=1000, env_config=env_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dhYYUUbEctN2"
      },
      "outputs": [],
      "source": [
        "# ai.grid_search_test(nb_steps=30_000, env_config=env_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DRxDEGqhfOP5",
        "outputId": "86e264a2-e154-4713-c29f-067298c20eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 41)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                784       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,565\n",
            "Trainable params: 3,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 2000000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 147s 14ms/step - reward: -17.8065\n",
            "1818 episodes - episode_reward: -97.947 [-99.995, -91.770] - loss: 198.313 - mae: 41.264 - mean_q: 0.808\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 155s 15ms/step - reward: -12.3333\n",
            "1274 episodes - episode_reward: -96.808 [-99.974, -90.924] - loss: 191.075 - mae: 44.335 - mean_q: -4.802\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 155s 15ms/step - reward: -11.8643\n",
            "1227 episodes - episode_reward: -96.693 [-99.974, -91.962] - loss: 202.322 - mae: 45.962 - mean_q: -9.989\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 159s 16ms/step - reward: -11.2786\n",
            "1169 episodes - episode_reward: -96.482 [-99.974, -89.718] - loss: 170.044 - mae: 49.159 - mean_q: -16.626\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 162s 16ms/step - reward: -11.2088\n",
            "1162 episodes - episode_reward: -96.459 [-99.974, -90.681] - loss: 144.717 - mae: 52.740 - mean_q: -23.659\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 163s 16ms/step - reward: -11.1843\n",
            "1160 episodes - episode_reward: -96.418 [-99.974, -88.438] - loss: 119.727 - mae: 56.373 - mean_q: -29.328\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 167s 17ms/step - reward: -11.2758\n",
            "1169 episodes - episode_reward: -96.456 [-99.974, -92.026] - loss: 99.477 - mae: 59.606 - mean_q: -35.246\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: -12.5054\n",
            "1291 episodes - episode_reward: -96.865 [-99.974, -91.353] - loss: 91.832 - mae: 63.007 - mean_q: -43.817\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: -11.3894\n",
            "1180 episodes - episode_reward: -96.521 [-99.974, -90.195] - loss: 65.796 - mae: 68.242 - mean_q: -50.670\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: -10.9862\n",
            "1140 episodes - episode_reward: -96.370 [-99.974, -90.924] - loss: 49.648 - mae: 71.349 - mean_q: -55.041\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: -10.6648\n",
            "1108 episodes - episode_reward: -96.254 [-100.142, -89.981] - loss: 39.204 - mae: 73.446 - mean_q: -58.320\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 182s 18ms/step - reward: -10.6236\n",
            "1104 episodes - episode_reward: -96.228 [-99.974, -89.075] - loss: 31.066 - mae: 75.255 - mean_q: -61.522\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 186s 19ms/step - reward: -10.8848\n",
            "1130 episodes - episode_reward: -96.326 [-99.974, -89.555] - loss: 25.506 - mae: 77.103 - mean_q: -64.431\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 190s 19ms/step - reward: -10.7543\n",
            "1117 episodes - episode_reward: -96.280 [-99.974, -91.960] - loss: 21.458 - mae: 78.758 - mean_q: -67.321\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 199s 20ms/step - reward: -11.5567\n",
            "1197 episodes - episode_reward: -96.545 [-99.974, -89.282] - loss: 18.974 - mae: 79.858 - mean_q: -69.392\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 208s 21ms/step - reward: -11.5176\n",
            "1193 episodes - episode_reward: -96.543 [-99.974, -91.038] - loss: 17.532 - mae: 81.136 - mean_q: -72.543\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 210s 21ms/step - reward: -11.3294\n",
            "1174 episodes - episode_reward: -96.503 [-99.974, -90.295] - loss: 14.650 - mae: 82.933 - mean_q: -74.816\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 212s 21ms/step - reward: -11.0268\n",
            "1144 episodes - episode_reward: -96.388 [-99.974, -91.230] - loss: 12.165 - mae: 84.185 - mean_q: -76.587\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -11.5537\n",
            "1196 episodes - episode_reward: -96.603 [-99.974, -90.260] - loss: 10.321 - mae: 85.143 - mean_q: -78.833\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -11.0567\n",
            "1147 episodes - episode_reward: -96.399 [-100.009, -90.836] - loss: 8.676 - mae: 86.392 - mean_q: -80.583\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 188s 19ms/step - reward: -10.9762\n",
            "1139 episodes - episode_reward: -96.365 [-100.109, -87.937] - loss: 7.605 - mae: 87.068 - mean_q: -81.520\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 191s 19ms/step - reward: -10.8230\n",
            "1124 episodes - episode_reward: -96.289 [-99.974, -89.148] - loss: 6.983 - mae: 87.455 - mean_q: -82.193\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 194s 19ms/step - reward: -10.5938\n",
            "1101 episodes - episode_reward: -96.219 [-99.974, -90.505] - loss: 6.371 - mae: 87.729 - mean_q: -82.746\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 228s 23ms/step - reward: -11.0958\n",
            "1151 episodes - episode_reward: -96.402 [-99.974, -90.967] - loss: 5.881 - mae: 88.066 - mean_q: -83.358\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 233s 23ms/step - reward: -10.6738\n",
            "1109 episodes - episode_reward: -96.249 [-100.122, -91.477] - loss: 5.494 - mae: 88.381 - mean_q: -83.918\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 240s 24ms/step - reward: -10.8861\n",
            "1130 episodes - episode_reward: -96.338 [-100.122, -91.733] - loss: 5.224 - mae: 88.573 - mean_q: -84.300\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 243s 24ms/step - reward: -10.8231\n",
            "1124 episodes - episode_reward: -96.287 [-99.974, -90.967] - loss: 5.134 - mae: 88.850 - mean_q: -84.720\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 248s 25ms/step - reward: -10.6926\n",
            "1111 episodes - episode_reward: -96.245 [-99.974, -90.562] - loss: 4.885 - mae: 89.050 - mean_q: -85.086\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 253s 25ms/step - reward: -10.8351\n",
            "1125 episodes - episode_reward: -96.310 [-99.974, -90.853] - loss: 4.716 - mae: 89.216 - mean_q: -85.373\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 256s 26ms/step - reward: -10.6410\n",
            "1106 episodes - episode_reward: -96.214 [-99.974, -90.200] - loss: 4.615 - mae: 89.336 - mean_q: -85.606\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            " 1000/10000 [==>...........................] - ETA: 3:50 - reward: -9.8787done, took 5923.070 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.956, steps: 7\n",
            "Episode 2: reward: -97.406, steps: 7\n",
            "Episode 3: reward: -95.517, steps: 11\n",
            "Episode 4: reward: -93.606, steps: 15\n",
            "Episode 5: reward: -97.156, steps: 7\n",
            "Episode 6: reward: -95.390, steps: 11\n",
            "Episode 7: reward: -97.086, steps: 8\n",
            "Episode 8: reward: -97.406, steps: 7\n",
            "Episode 9: reward: -95.577, steps: 11\n",
            "Episode 10: reward: -95.084, steps: 11\n",
            "Episode 11: reward: -95.260, steps: 11\n",
            "Episode 12: reward: -95.137, steps: 11\n",
            "Episode 13: reward: -97.459, steps: 8\n",
            "Episode 14: reward: -96.926, steps: 8\n",
            "Episode 15: reward: -97.156, steps: 7\n",
            "Episode 16: reward: -95.390, steps: 11\n",
            "Episode 17: reward: -95.390, steps: 11\n",
            "Episode 18: reward: -92.763, steps: 15\n",
            "Episode 19: reward: -93.263, steps: 15\n",
            "Episode 20: reward: -95.264, steps: 11\n",
            "Episode 21: reward: -92.800, steps: 15\n",
            "Episode 22: reward: -92.593, steps: 15\n",
            "Episode 23: reward: -95.717, steps: 11\n",
            "Episode 24: reward: -95.527, steps: 11\n",
            "Episode 25: reward: -95.450, steps: 11\n",
            "Episode 26: reward: -97.156, steps: 7\n",
            "Episode 27: reward: -95.580, steps: 11\n",
            "Episode 28: reward: -95.580, steps: 11\n",
            "Episode 29: reward: -95.327, steps: 11\n",
            "Episode 30: reward: -92.206, steps: 15\n",
            "Episode 31: reward: -95.390, steps: 11\n",
            "Episode 32: reward: -97.206, steps: 7\n",
            "Episode 33: reward: -95.770, steps: 11\n",
            "Episode 34: reward: -93.606, steps: 15\n",
            "Episode 35: reward: -97.673, steps: 8\n",
            "Episode 36: reward: -93.243, steps: 15\n",
            "Episode 37: reward: -93.233, steps: 15\n",
            "Episode 38: reward: -93.360, steps: 15\n",
            "Episode 39: reward: -95.390, steps: 11\n",
            "Episode 40: reward: -93.070, steps: 15\n",
            "Episode 41: reward: -92.363, steps: 15\n",
            "Episode 42: reward: -97.256, steps: 7\n",
            "Episode 43: reward: -96.956, steps: 7\n",
            "Episode 44: reward: -95.454, steps: 11\n",
            "Episode 45: reward: -96.806, steps: 8\n",
            "Episode 46: reward: -95.527, steps: 11\n",
            "Episode 47: reward: -97.256, steps: 7\n",
            "Episode 48: reward: -97.206, steps: 7\n",
            "Episode 49: reward: -95.100, steps: 11\n",
            "Episode 50: reward: -97.356, steps: 7\n",
            "Episode 51: reward: -95.390, steps: 11\n",
            "Episode 52: reward: -95.200, steps: 11\n",
            "Episode 53: reward: -94.204, steps: 11\n",
            "Episode 54: reward: -95.390, steps: 11\n",
            "Episode 55: reward: -95.580, steps: 11\n",
            "Episode 56: reward: -93.450, steps: 15\n",
            "Episode 57: reward: -94.404, steps: 11\n",
            "Episode 58: reward: -95.264, steps: 11\n",
            "Episode 59: reward: -94.944, steps: 11\n",
            "Episode 60: reward: -97.156, steps: 7\n",
            "Episode 61: reward: -95.437, steps: 11\n",
            "Episode 62: reward: -95.310, steps: 11\n",
            "Episode 63: reward: -95.454, steps: 11\n",
            "Episode 64: reward: -95.454, steps: 11\n",
            "Episode 65: reward: -93.110, steps: 15\n",
            "Episode 66: reward: -97.306, steps: 7\n",
            "Episode 67: reward: -93.513, steps: 15\n",
            "Episode 68: reward: -93.200, steps: 15\n",
            "Episode 69: reward: -92.543, steps: 15\n",
            "Episode 70: reward: -95.394, steps: 11\n",
            "Episode 71: reward: -95.200, steps: 11\n",
            "Episode 72: reward: -95.264, steps: 11\n",
            "Episode 73: reward: -95.327, steps: 11\n",
            "Episode 74: reward: -93.856, steps: 15\n",
            "Episode 75: reward: -95.327, steps: 11\n",
            "Episode 76: reward: -95.717, steps: 11\n",
            "Episode 77: reward: -95.247, steps: 11\n",
            "Episode 78: reward: -95.274, steps: 11\n",
            "Episode 79: reward: -93.820, steps: 15\n",
            "Episode 80: reward: -93.513, steps: 15\n",
            "Episode 81: reward: -96.956, steps: 7\n",
            "Episode 82: reward: -96.956, steps: 7\n",
            "Episode 83: reward: -95.517, steps: 11\n",
            "Episode 84: reward: -94.086, steps: 15\n",
            "Episode 85: reward: -92.916, steps: 15\n",
            "Episode 86: reward: -91.146, steps: 15\n",
            "Episode 87: reward: -95.337, steps: 11\n",
            "Episode 88: reward: -93.300, steps: 15\n",
            "Episode 89: reward: -93.723, steps: 15\n",
            "Episode 90: reward: -95.140, steps: 11\n",
            "Episode 91: reward: -95.204, steps: 11\n",
            "Episode 92: reward: -95.577, steps: 11\n",
            "Episode 93: reward: -97.206, steps: 7\n",
            "Episode 94: reward: -95.464, steps: 11\n",
            "Episode 95: reward: -97.306, steps: 7\n",
            "Episode 96: reward: -93.256, steps: 15\n",
            "Episode 97: reward: -95.437, steps: 11\n",
            "Episode 98: reward: -97.106, steps: 7\n",
            "Episode 99: reward: -93.493, steps: 15\n",
            "Episode 100: reward: -96.813, steps: 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'5931.300638;2000000;-96.49934597034957;-87.93692307692308;-100.14205128205128;-95.20542564102566;-91.14641025641026;-97.67256410256411;16.263746505125816;38;0;100;100;1;0.005;32;0.0001;0.1;avg;linear;2;48;16\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hyperparameter = {\n",
        "    \"windows_length\": 1,\n",
        "    \"adam_learning_rate\": 0.005,\n",
        "    \"batch_size\": 32,\n",
        "    \"target_model_update\": 0.0001,\n",
        "    \"adam_epsilon\": 0.1,\n",
        "    \"dueling_option\": \"avg\",\n",
        "    \"activation\": \"linear\",\n",
        "    \"layers\": 2,\n",
        "    \"unit_1\": 48,\n",
        "    \"unit_2\": 16,\n",
        "}\n",
        "\n",
        "ai.train(\n",
        "    hyperparameter=hyperparameter,\n",
        "    nb_steps=2_000_000,\n",
        "    env_config=env_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-FMgpdHFMog"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "ai-model.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7724bf7a85847efc5bb6a1829c232f0862f78efa3c8535cdc9aa81401d00c18c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
