{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "execution": {
                    "iopub.execute_input": "2022-06-22T07:07:27.921422Z",
                    "iopub.status.busy": "2022-06-22T07:07:27.920741Z",
                    "iopub.status.idle": "2022-06-22T07:07:53.671254Z",
                    "shell.execute_reply": "2022-06-22T07:07:53.670603Z",
                    "shell.execute_reply.started": "2022-06-22T07:07:27.921391Z"
                },
                "id": "b_ezGGzkUkfi",
                "outputId": "40b3577e-018e-4753-bc33-7642ceae0587"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting keras-rl\n",
                        "  Downloading keras-rl-0.4.2.tar.gz (40 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
                        "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.8/dist-packages (from keras-rl) (2.7.0)\n",
                        "Building wheels for collected packages: keras-rl\n",
                        "  Building wheel for keras-rl (setup.py) ... \u001b[?25ldone\n",
                        "\u001b[?25h  Created wheel for keras-rl: filename=keras_rl-0.4.2-py3-none-any.whl size=48380 sha256=42b7c6442cea809bca1c25515a1dfdcb5d4fc2a93a5aabda2753b8e4fddbb458\n",
                        "  Stored in directory: /root/.cache/pip/wheels/f7/c4/28/2e6c8cbc197e7b52ec9a7431a42a9bc85424d9873e0f2a0f45\n",
                        "Successfully built keras-rl\n",
                        "Installing collected packages: keras-rl\n",
                        "Successfully installed keras-rl-0.4.2\n",
                        "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
                        "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
                        "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
                        "\u001b[0mCollecting keras-rl2\n",
                        "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras-rl2) (2.7.0+nv22.2)\n",
                        "Requirement already satisfied: absl-py==0.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.12.0)\n",
                        "Requirement already satisfied: tensorboard<2.8,>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.7.0)\n",
                        "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.7.0)\n",
                        "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
                        "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
                        "Requirement already satisfied: tensorflow-estimator<2.8,>=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (2.7.0)\n",
                        "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.12)\n",
                        "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.19.4)\n",
                        "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
                        "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.12.1)\n",
                        "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.39.0)\n",
                        "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
                        "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
                        "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.7.4.3)\n",
                        "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
                        "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
                        "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
                        "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
                        "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (5.0)\n",
                        "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras-rl2) (0.35.1)\n",
                        "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (0.4.6)\n",
                        "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (2.0.3)\n",
                        "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (0.6.1)\n",
                        "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (1.8.1)\n",
                        "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (3.3.6)\n",
                        "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (2.27.1)\n",
                        "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (59.4.0)\n",
                        "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (2.6.0)\n",
                        "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (4.8)\n",
                        "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (0.2.8)\n",
                        "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (5.0.0)\n",
                        "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (1.3.1)\n",
                        "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (4.10.1)\n",
                        "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (1.26.8)\n",
                        "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (2.0.11)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (3.3)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (2021.10.8)\n",
                        "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (3.7.0)\n",
                        "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (0.4.8)\n",
                        "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.8,>=2.7.0->tensorflow->keras-rl2) (3.2.0)\n",
                        "Installing collected packages: keras-rl2\n",
                        "Successfully installed keras-rl2-1.0.5\n",
                        "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
                        "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
                        "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
                        "\u001b[0mCollecting gym\n",
                        "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.4/696.4 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
                        "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
                        "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.19.4)\n",
                        "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (2.0.0)\n",
                        "Collecting gym-notices>=0.0.4\n",
                        "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
                        "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (4.10.1)\n",
                        "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.7.0)\n",
                        "Building wheels for collected packages: gym\n",
                        "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
                        "\u001b[?25h  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793156 sha256=a29228b54ef7351794724e88753179834cc38d4709717c608acba998bb7e6ffe\n",
                        "  Stored in directory: /root/.cache/pip/wheels/5a/e9/0b/5536e77ed2edbbf067ecff287ec039633d40daee4d8dac7716\n",
                        "Successfully built gym\n",
                        "Installing collected packages: gym-notices, gym\n",
                        "Successfully installed gym-0.24.1 gym-notices-0.0.7\n",
                        "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
                        "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
                        "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
                        "\u001b[0mCollecting sympy\n",
                        "  Downloading sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hCollecting mpmath>=0.19\n",
                        "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.6/532.6 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25hInstalling collected packages: mpmath, sympy\n",
                        "Successfully installed mpmath-1.2.1 sympy-1.10.1\n",
                        "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
                        "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.1.2 is available.\n",
                        "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
                        "\u001b[0m"
                    ]
                }
            ],
            "source": [
                "!pip install keras-rl\n",
                "!pip install keras-rl2\n",
                "!pip install gym\n",
                "!pip install sympy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2022-06-22T07:07:53.673383Z",
                    "iopub.status.busy": "2022-06-22T07:07:53.672634Z",
                    "iopub.status.idle": "2022-06-22T07:07:54.191348Z",
                    "shell.execute_reply": "2022-06-22T07:07:54.190843Z",
                    "shell.execute_reply.started": "2022-06-22T07:07:53.673351Z"
                }
            },
            "outputs": [],
            "source": [
                "from hyperparameter import Hyperparameter\n",
                "from ai import KniffelAI\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "execution": {
                    "iopub.execute_input": "2022-06-22T07:10:59.435321Z",
                    "iopub.status.busy": "2022-06-22T07:10:59.434299Z",
                    "iopub.status.idle": "2022-06-22T07:10:59.449037Z",
                    "shell.execute_reply": "2022-06-22T07:10:59.448349Z",
                    "shell.execute_reply.started": "2022-06-22T07:10:59.435272Z"
                },
                "id": "_5ILWBfrctN1",
                "outputId": "98cbca70-4223-414d-92c8-33d1717d3d74"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 2400 combinations to test.\n"
                    ]
                }
            ],
            "source": [
                "units = list(range(16, 96, 16))\n",
                "\n",
                "base_hp = {\n",
                "    \"windows_length\": [1],\n",
                "    \"adam_learning_rate\": [\n",
                "        0.0001,\n",
                "        0.0005,\n",
                "        0.001,\n",
                "        0.005,\n",
                "        0.01,\n",
                "        0.05,\n",
                "        0.1,\n",
                "    ],  # np.arange(0.0001, 0.1, 0.01),\n",
                "    \"adam_epsilon\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
                "    \"batch_size\": [32],\n",
                "    \"target_model_update\": [\n",
                "        50,\n",
                "        100,\n",
                "        200,\n",
                "        300,\n",
                "        400,\n",
                "        500,\n",
                "        750,\n",
                "        1000,\n",
                "    ],  # np.arange(1, 1000, 70),\n",
                "    \"dueling_option\": [\"avg\"],\n",
                "    \"activation\": [\"linear\"],\n",
                "    \"layers\": [3],\n",
                "    \"unit_1\": [96],\n",
                "    \"unit_2\": [80],\n",
                "    \"unit_3\": [64],\n",
                "}\n",
                "\n",
                "ai = KniffelAI(\n",
                "    load=False, predefined_layers=True, hyperparater_base=base_hp, path_prefix=\"../\", config_path=\"Kniffel.CSV\"\n",
                ")\n",
                "\n",
                "env_config = {\n",
                "        \"reward_step\": 0,\n",
                "        \"reward_round\": 4,\n",
                "        \"reward_roll_dice\": 0.5,\n",
                "        \"reward_game_over\": -40,\n",
                "        \"reward_slash\": -10,\n",
                "        \"reward_bonus\": 2,\n",
                "        \"reward_finish\": 10,\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ai.play(path=\"/weights/p_date=2022-06-04-06_57_09\", episodes=1000, env_config=env_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2022-06-22T07:11:05.182465Z",
                    "iopub.status.busy": "2022-06-22T07:11:05.181596Z",
                    "iopub.status.idle": "2022-06-22T07:11:05.251891Z",
                    "shell.execute_reply": "2022-06-22T07:11:05.251103Z",
                    "shell.execute_reply.started": "2022-06-22T07:11:05.182401Z"
                },
                "id": "dhYYUUbEctN2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 150 combinations to test.\n",
                        "windows_length;adam_learning_rate;adam_epsilon;batch_size;target_model_update;dueling_option;activation;layers;unit_1;unit_2;unit_3\n",
                        "Created 150 combinations to test.\n",
                        "\n",
                        "#################\n",
                        "Created 150 combinations to test.\n",
                        "Test 1 from 150\n",
                        "\n",
                        "{'windows_length': 1, 'adam_learning_rate': 0.001, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 3, 'unit_1': 32, 'unit_2': 32, 'unit_3': 16}\n",
                        "\n"
                    ]
                },
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'ai/Kniffel.CSV'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_698/2159385092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/notebooks/ai/ai.py\u001b[0m in \u001b[0;36mgrid_search_test\u001b[0;34m(self, nb_steps, env_config)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             csv = self._train(\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mhyperparameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             )\n",
                        "\u001b[0;32m/notebooks/ai/ai.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, hyperparameter, nb_steps, load_path, env_config)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mdate_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKniffelEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/notebooks/ai/env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_config, config_file_path, logging, reward_step, reward_round, reward_roll_dice, reward_game_over, reward_slash, reward_bonus, reward_finish)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# - https://brefeld.homepage.t-online.de/kniffel.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# - https://brefeld.homepage.t-online.de/kniffel-strategie.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ai/Kniffel.CSV'"
                    ]
                }
            ],
            "source": [
                "ai.grid_search_test(nb_steps=50_000, env_config=env_config, r)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "DRxDEGqhfOP5",
                "outputId": "86e264a2-e154-4713-c29f-067298c20eb9"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: \"sequential\"\n",
                        "_________________________________________________________________\n",
                        " Layer (type)                Output Shape              Param #   \n",
                        "=================================================================\n",
                        " flatten (Flatten)           (None, 41)                0         \n",
                        "                                                                 \n",
                        " dense (Dense)               (None, 48)                2016      \n",
                        "                                                                 \n",
                        " dense_1 (Dense)             (None, 16)                784       \n",
                        "                                                                 \n",
                        " dense_2 (Dense)             (None, 45)                765       \n",
                        "                                                                 \n",
                        "=================================================================\n",
                        "Total params: 3,565\n",
                        "Trainable params: 3,565\n",
                        "Non-trainable params: 0\n",
                        "_________________________________________________________________\n",
                        "Training for 2000000 steps ...\n",
                        "Interval 1 (0 steps performed)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
                        "  updates=self.state_updates,\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "10000/10000 [==============================] - 147s 14ms/step - reward: -17.8065\n",
                        "1818 episodes - episode_reward: -97.947 [-99.995, -91.770] - loss: 198.313 - mae: 41.264 - mean_q: 0.808\n",
                        "\n",
                        "Interval 2 (10000 steps performed)\n",
                        "10000/10000 [==============================] - 155s 15ms/step - reward: -12.3333\n",
                        "1274 episodes - episode_reward: -96.808 [-99.974, -90.924] - loss: 191.075 - mae: 44.335 - mean_q: -4.802\n",
                        "\n",
                        "Interval 3 (20000 steps performed)\n",
                        "10000/10000 [==============================] - 155s 15ms/step - reward: -11.8643\n",
                        "1227 episodes - episode_reward: -96.693 [-99.974, -91.962] - loss: 202.322 - mae: 45.962 - mean_q: -9.989\n",
                        "\n",
                        "Interval 4 (30000 steps performed)\n",
                        "10000/10000 [==============================] - 159s 16ms/step - reward: -11.2786\n",
                        "1169 episodes - episode_reward: -96.482 [-99.974, -89.718] - loss: 170.044 - mae: 49.159 - mean_q: -16.626\n",
                        "\n",
                        "Interval 5 (40000 steps performed)\n",
                        "10000/10000 [==============================] - 162s 16ms/step - reward: -11.2088\n",
                        "1162 episodes - episode_reward: -96.459 [-99.974, -90.681] - loss: 144.717 - mae: 52.740 - mean_q: -23.659\n",
                        "\n",
                        "Interval 6 (50000 steps performed)\n",
                        "10000/10000 [==============================] - 163s 16ms/step - reward: -11.1843\n",
                        "1160 episodes - episode_reward: -96.418 [-99.974, -88.438] - loss: 119.727 - mae: 56.373 - mean_q: -29.328\n",
                        "\n",
                        "Interval 7 (60000 steps performed)\n",
                        "10000/10000 [==============================] - 167s 17ms/step - reward: -11.2758\n",
                        "1169 episodes - episode_reward: -96.456 [-99.974, -92.026] - loss: 99.477 - mae: 59.606 - mean_q: -35.246\n",
                        "\n",
                        "Interval 8 (70000 steps performed)\n",
                        "10000/10000 [==============================] - 171s 17ms/step - reward: -12.5054\n",
                        "1291 episodes - episode_reward: -96.865 [-99.974, -91.353] - loss: 91.832 - mae: 63.007 - mean_q: -43.817\n",
                        "\n",
                        "Interval 9 (80000 steps performed)\n",
                        "10000/10000 [==============================] - 173s 17ms/step - reward: -11.3894\n",
                        "1180 episodes - episode_reward: -96.521 [-99.974, -90.195] - loss: 65.796 - mae: 68.242 - mean_q: -50.670\n",
                        "\n",
                        "Interval 10 (90000 steps performed)\n",
                        "10000/10000 [==============================] - 176s 18ms/step - reward: -10.9862\n",
                        "1140 episodes - episode_reward: -96.370 [-99.974, -90.924] - loss: 49.648 - mae: 71.349 - mean_q: -55.041\n",
                        "\n",
                        "Interval 11 (100000 steps performed)\n",
                        "10000/10000 [==============================] - 178s 18ms/step - reward: -10.6648\n",
                        "1108 episodes - episode_reward: -96.254 [-100.142, -89.981] - loss: 39.204 - mae: 73.446 - mean_q: -58.320\n",
                        "\n",
                        "Interval 12 (110000 steps performed)\n",
                        "10000/10000 [==============================] - 182s 18ms/step - reward: -10.6236\n",
                        "1104 episodes - episode_reward: -96.228 [-99.974, -89.075] - loss: 31.066 - mae: 75.255 - mean_q: -61.522\n",
                        "\n",
                        "Interval 13 (120000 steps performed)\n",
                        "10000/10000 [==============================] - 186s 19ms/step - reward: -10.8848\n",
                        "1130 episodes - episode_reward: -96.326 [-99.974, -89.555] - loss: 25.506 - mae: 77.103 - mean_q: -64.431\n",
                        "\n",
                        "Interval 14 (130000 steps performed)\n",
                        "10000/10000 [==============================] - 190s 19ms/step - reward: -10.7543\n",
                        "1117 episodes - episode_reward: -96.280 [-99.974, -91.960] - loss: 21.458 - mae: 78.758 - mean_q: -67.321\n",
                        "\n",
                        "Interval 15 (140000 steps performed)\n",
                        "10000/10000 [==============================] - 199s 20ms/step - reward: -11.5567\n",
                        "1197 episodes - episode_reward: -96.545 [-99.974, -89.282] - loss: 18.974 - mae: 79.858 - mean_q: -69.392\n",
                        "\n",
                        "Interval 16 (150000 steps performed)\n",
                        "10000/10000 [==============================] - 208s 21ms/step - reward: -11.5176\n",
                        "1193 episodes - episode_reward: -96.543 [-99.974, -91.038] - loss: 17.532 - mae: 81.136 - mean_q: -72.543\n",
                        "\n",
                        "Interval 17 (160000 steps performed)\n",
                        "10000/10000 [==============================] - 210s 21ms/step - reward: -11.3294\n",
                        "1174 episodes - episode_reward: -96.503 [-99.974, -90.295] - loss: 14.650 - mae: 82.933 - mean_q: -74.816\n",
                        "\n",
                        "Interval 18 (170000 steps performed)\n",
                        "10000/10000 [==============================] - 212s 21ms/step - reward: -11.0268\n",
                        "1144 episodes - episode_reward: -96.388 [-99.974, -91.230] - loss: 12.165 - mae: 84.185 - mean_q: -76.587\n",
                        "\n",
                        "Interval 19 (180000 steps performed)\n",
                        "10000/10000 [==============================] - 217s 22ms/step - reward: -11.5537\n",
                        "1196 episodes - episode_reward: -96.603 [-99.974, -90.260] - loss: 10.321 - mae: 85.143 - mean_q: -78.833\n",
                        "\n",
                        "Interval 20 (190000 steps performed)\n",
                        "10000/10000 [==============================] - 217s 22ms/step - reward: -11.0567\n",
                        "1147 episodes - episode_reward: -96.399 [-100.009, -90.836] - loss: 8.676 - mae: 86.392 - mean_q: -80.583\n",
                        "\n",
                        "Interval 21 (200000 steps performed)\n",
                        "10000/10000 [==============================] - 188s 19ms/step - reward: -10.9762\n",
                        "1139 episodes - episode_reward: -96.365 [-100.109, -87.937] - loss: 7.605 - mae: 87.068 - mean_q: -81.520\n",
                        "\n",
                        "Interval 22 (210000 steps performed)\n",
                        "10000/10000 [==============================] - 191s 19ms/step - reward: -10.8230\n",
                        "1124 episodes - episode_reward: -96.289 [-99.974, -89.148] - loss: 6.983 - mae: 87.455 - mean_q: -82.193\n",
                        "\n",
                        "Interval 23 (220000 steps performed)\n",
                        "10000/10000 [==============================] - 194s 19ms/step - reward: -10.5938\n",
                        "1101 episodes - episode_reward: -96.219 [-99.974, -90.505] - loss: 6.371 - mae: 87.729 - mean_q: -82.746\n",
                        "\n",
                        "Interval 24 (230000 steps performed)\n",
                        "10000/10000 [==============================] - 228s 23ms/step - reward: -11.0958\n",
                        "1151 episodes - episode_reward: -96.402 [-99.974, -90.967] - loss: 5.881 - mae: 88.066 - mean_q: -83.358\n",
                        "\n",
                        "Interval 25 (240000 steps performed)\n",
                        "10000/10000 [==============================] - 233s 23ms/step - reward: -10.6738\n",
                        "1109 episodes - episode_reward: -96.249 [-100.122, -91.477] - loss: 5.494 - mae: 88.381 - mean_q: -83.918\n",
                        "\n",
                        "Interval 26 (250000 steps performed)\n",
                        "10000/10000 [==============================] - 240s 24ms/step - reward: -10.8861\n",
                        "1130 episodes - episode_reward: -96.338 [-100.122, -91.733] - loss: 5.224 - mae: 88.573 - mean_q: -84.300\n",
                        "\n",
                        "Interval 27 (260000 steps performed)\n",
                        "10000/10000 [==============================] - 243s 24ms/step - reward: -10.8231\n",
                        "1124 episodes - episode_reward: -96.287 [-99.974, -90.967] - loss: 5.134 - mae: 88.850 - mean_q: -84.720\n",
                        "\n",
                        "Interval 28 (270000 steps performed)\n",
                        "10000/10000 [==============================] - 248s 25ms/step - reward: -10.6926\n",
                        "1111 episodes - episode_reward: -96.245 [-99.974, -90.562] - loss: 4.885 - mae: 89.050 - mean_q: -85.086\n",
                        "\n",
                        "Interval 29 (280000 steps performed)\n",
                        "10000/10000 [==============================] - 253s 25ms/step - reward: -10.8351\n",
                        "1125 episodes - episode_reward: -96.310 [-99.974, -90.853] - loss: 4.716 - mae: 89.216 - mean_q: -85.373\n",
                        "\n",
                        "Interval 30 (290000 steps performed)\n",
                        "10000/10000 [==============================] - 256s 26ms/step - reward: -10.6410\n",
                        "1106 episodes - episode_reward: -96.214 [-99.974, -90.200] - loss: 4.615 - mae: 89.336 - mean_q: -85.606\n",
                        "\n",
                        "Interval 31 (300000 steps performed)\n",
                        " 1000/10000 [==>...........................] - ETA: 3:50 - reward: -9.8787done, took 5923.070 seconds\n",
                        "Testing for 100 episodes ...\n",
                        "Episode 1: reward: -96.956, steps: 7\n",
                        "Episode 2: reward: -97.406, steps: 7\n",
                        "Episode 3: reward: -95.517, steps: 11\n",
                        "Episode 4: reward: -93.606, steps: 15\n",
                        "Episode 5: reward: -97.156, steps: 7\n",
                        "Episode 6: reward: -95.390, steps: 11\n",
                        "Episode 7: reward: -97.086, steps: 8\n",
                        "Episode 8: reward: -97.406, steps: 7\n",
                        "Episode 9: reward: -95.577, steps: 11\n",
                        "Episode 10: reward: -95.084, steps: 11\n",
                        "Episode 11: reward: -95.260, steps: 11\n",
                        "Episode 12: reward: -95.137, steps: 11\n",
                        "Episode 13: reward: -97.459, steps: 8\n",
                        "Episode 14: reward: -96.926, steps: 8\n",
                        "Episode 15: reward: -97.156, steps: 7\n",
                        "Episode 16: reward: -95.390, steps: 11\n",
                        "Episode 17: reward: -95.390, steps: 11\n",
                        "Episode 18: reward: -92.763, steps: 15\n",
                        "Episode 19: reward: -93.263, steps: 15\n",
                        "Episode 20: reward: -95.264, steps: 11\n",
                        "Episode 21: reward: -92.800, steps: 15\n",
                        "Episode 22: reward: -92.593, steps: 15\n",
                        "Episode 23: reward: -95.717, steps: 11\n",
                        "Episode 24: reward: -95.527, steps: 11\n",
                        "Episode 25: reward: -95.450, steps: 11\n",
                        "Episode 26: reward: -97.156, steps: 7\n",
                        "Episode 27: reward: -95.580, steps: 11\n",
                        "Episode 28: reward: -95.580, steps: 11\n",
                        "Episode 29: reward: -95.327, steps: 11\n",
                        "Episode 30: reward: -92.206, steps: 15\n",
                        "Episode 31: reward: -95.390, steps: 11\n",
                        "Episode 32: reward: -97.206, steps: 7\n",
                        "Episode 33: reward: -95.770, steps: 11\n",
                        "Episode 34: reward: -93.606, steps: 15\n",
                        "Episode 35: reward: -97.673, steps: 8\n",
                        "Episode 36: reward: -93.243, steps: 15\n",
                        "Episode 37: reward: -93.233, steps: 15\n",
                        "Episode 38: reward: -93.360, steps: 15\n",
                        "Episode 39: reward: -95.390, steps: 11\n",
                        "Episode 40: reward: -93.070, steps: 15\n",
                        "Episode 41: reward: -92.363, steps: 15\n",
                        "Episode 42: reward: -97.256, steps: 7\n",
                        "Episode 43: reward: -96.956, steps: 7\n",
                        "Episode 44: reward: -95.454, steps: 11\n",
                        "Episode 45: reward: -96.806, steps: 8\n",
                        "Episode 46: reward: -95.527, steps: 11\n",
                        "Episode 47: reward: -97.256, steps: 7\n",
                        "Episode 48: reward: -97.206, steps: 7\n",
                        "Episode 49: reward: -95.100, steps: 11\n",
                        "Episode 50: reward: -97.356, steps: 7\n",
                        "Episode 51: reward: -95.390, steps: 11\n",
                        "Episode 52: reward: -95.200, steps: 11\n",
                        "Episode 53: reward: -94.204, steps: 11\n",
                        "Episode 54: reward: -95.390, steps: 11\n",
                        "Episode 55: reward: -95.580, steps: 11\n",
                        "Episode 56: reward: -93.450, steps: 15\n",
                        "Episode 57: reward: -94.404, steps: 11\n",
                        "Episode 58: reward: -95.264, steps: 11\n",
                        "Episode 59: reward: -94.944, steps: 11\n",
                        "Episode 60: reward: -97.156, steps: 7\n",
                        "Episode 61: reward: -95.437, steps: 11\n",
                        "Episode 62: reward: -95.310, steps: 11\n",
                        "Episode 63: reward: -95.454, steps: 11\n",
                        "Episode 64: reward: -95.454, steps: 11\n",
                        "Episode 65: reward: -93.110, steps: 15\n",
                        "Episode 66: reward: -97.306, steps: 7\n",
                        "Episode 67: reward: -93.513, steps: 15\n",
                        "Episode 68: reward: -93.200, steps: 15\n",
                        "Episode 69: reward: -92.543, steps: 15\n",
                        "Episode 70: reward: -95.394, steps: 11\n",
                        "Episode 71: reward: -95.200, steps: 11\n",
                        "Episode 72: reward: -95.264, steps: 11\n",
                        "Episode 73: reward: -95.327, steps: 11\n",
                        "Episode 74: reward: -93.856, steps: 15\n",
                        "Episode 75: reward: -95.327, steps: 11\n",
                        "Episode 76: reward: -95.717, steps: 11\n",
                        "Episode 77: reward: -95.247, steps: 11\n",
                        "Episode 78: reward: -95.274, steps: 11\n",
                        "Episode 79: reward: -93.820, steps: 15\n",
                        "Episode 80: reward: -93.513, steps: 15\n",
                        "Episode 81: reward: -96.956, steps: 7\n",
                        "Episode 82: reward: -96.956, steps: 7\n",
                        "Episode 83: reward: -95.517, steps: 11\n",
                        "Episode 84: reward: -94.086, steps: 15\n",
                        "Episode 85: reward: -92.916, steps: 15\n",
                        "Episode 86: reward: -91.146, steps: 15\n",
                        "Episode 87: reward: -95.337, steps: 11\n",
                        "Episode 88: reward: -93.300, steps: 15\n",
                        "Episode 89: reward: -93.723, steps: 15\n",
                        "Episode 90: reward: -95.140, steps: 11\n",
                        "Episode 91: reward: -95.204, steps: 11\n",
                        "Episode 92: reward: -95.577, steps: 11\n",
                        "Episode 93: reward: -97.206, steps: 7\n",
                        "Episode 94: reward: -95.464, steps: 11\n",
                        "Episode 95: reward: -97.306, steps: 7\n",
                        "Episode 96: reward: -93.256, steps: 15\n",
                        "Episode 97: reward: -95.437, steps: 11\n",
                        "Episode 98: reward: -97.106, steps: 7\n",
                        "Episode 99: reward: -93.493, steps: 15\n",
                        "Episode 100: reward: -96.813, steps: 8\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.google.colaboratory.intrinsic+json": {
                            "type": "string"
                        },
                        "text/plain": [
                            "'5931.300638;2000000;-96.49934597034957;-87.93692307692308;-100.14205128205128;-95.20542564102566;-91.14641025641026;-97.67256410256411;16.263746505125816;38;0;100;100;1;0.005;32;0.0001;0.1;avg;linear;2;48;16\\n'"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "hyperparameter = {\n",
                "    \"windows_length\": 1,\n",
                "    \"adam_learning_rate\": 0.0071,\n",
                "    \"batch_size\": 128,\n",
                "    \"target_model_update\": 0.0081,\n",
                "    \"adam_epsilon\": 0.001,\n",
                "    \"dueling_option\": \"avg\",\n",
                "    \"activation\": \"linear\",\n",
                "    \"layers\": 3,\n",
                "    \"unit_1\": 32,\n",
                "    \"unit_2\": 16,\n",
                "    \"unit_3\": 16,\n",
                "}\n",
                "\n",
                "# ai.train(\n",
                "#    hyperparameter=hyperparameter,\n",
                "#    nb_steps=3_000_000,\n",
                "#    env_config=env_config\n",
                "# )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "J-FMgpdHFMog"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "include_colab_link": true,
            "name": "ai-model.ipynb",
            "provenance": []
        },
        "interpreter": {
            "hash": "7724bf7a85847efc5bb6a1829c232f0862f78efa3c8535cdc9aa81401d00c18c"
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
