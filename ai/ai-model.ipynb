{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/Kniffel'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, 'kniffel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai import KniffelAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 208)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               26752     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 44)                2860      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,868\n",
      "Trainable params: 37,868\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training for 250000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   54/10000 [..............................] - ETA: 18s - reward: -1.2593"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elias\\anaconda3\\envs\\ai\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 54s 5ms/step - reward: -0.0654\n",
      "1352 episodes - episode_reward: -0.490 [-10.400, 14.200] - loss: 1.938 - mae: 6.632 - mean_q: 4.573\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: 0.3658\n",
      "948 episodes - episode_reward: 3.855 [-10.100, 17.500] - loss: 2.179 - mae: 7.461 - mean_q: 5.290\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: 0.4086\n",
      "900 episodes - episode_reward: 4.551 [-8.700, 20.300] - loss: 2.293 - mae: 7.814 - mean_q: 6.001\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.4375\n",
      "872 episodes - episode_reward: 5.009 [-10.100, 21.900] - loss: 2.187 - mae: 7.777 - mean_q: 5.677\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.4529\n",
      "847 episodes - episode_reward: 5.350 [-10.100, 21.100] - loss: 2.102 - mae: 7.638 - mean_q: 5.262\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      " 2240/10000 [=====>........................] - ETA: 54s - reward: 0.4807done, took 317.557 seconds\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 5.700, steps: 11\n",
      "Episode 2: reward: 4.100, steps: 11\n",
      "Episode 3: reward: 16.300, steps: 19\n",
      "Episode 4: reward: 4.400, steps: 12\n",
      "Episode 5: reward: 0.800, steps: 7\n",
      "Episode 6: reward: 8.600, steps: 14\n",
      "Episode 7: reward: 0.800, steps: 7\n",
      "Episode 8: reward: 5.100, steps: 11\n",
      "Episode 9: reward: 4.800, steps: 11\n",
      "Episode 10: reward: 8.200, steps: 15\n",
      "Episode 11: reward: 13.700, steps: 19\n",
      "Episode 12: reward: 1.800, steps: 7\n",
      "Episode 13: reward: 1.200, steps: 7\n",
      "Episode 14: reward: 2.100, steps: 10\n",
      "Episode 15: reward: 1.200, steps: 7\n",
      "Episode 16: reward: 8.100, steps: 11\n",
      "Episode 17: reward: 4.300, steps: 11\n",
      "Episode 18: reward: 2.200, steps: 9\n",
      "Episode 19: reward: 6.500, steps: 14\n",
      "Episode 20: reward: 8.500, steps: 15\n",
      "Episode 21: reward: 10.600, steps: 14\n",
      "Episode 22: reward: 5.300, steps: 11\n",
      "Episode 23: reward: 6.500, steps: 15\n",
      "Episode 24: reward: 6.200, steps: 13\n",
      "Episode 25: reward: 3.100, steps: 11\n",
      "Episode 26: reward: 2.800, steps: 10\n",
      "Episode 27: reward: 0.800, steps: 7\n",
      "Episode 28: reward: 11.000, steps: 16\n",
      "Episode 29: reward: 5.300, steps: 11\n",
      "Episode 30: reward: 8.400, steps: 15\n",
      "Episode 31: reward: 1.400, steps: 7\n",
      "Episode 32: reward: 0.400, steps: 6\n",
      "Episode 33: reward: 9.800, steps: 15\n",
      "Episode 34: reward: -0.400, steps: 7\n",
      "Episode 35: reward: 5.300, steps: 14\n",
      "Episode 36: reward: 7.300, steps: 15\n",
      "Episode 37: reward: 16.100, steps: 19\n",
      "Episode 38: reward: 11.800, steps: 18\n",
      "Episode 39: reward: 7.900, steps: 14\n",
      "Episode 40: reward: -8.700, steps: 2\n",
      "Episode 41: reward: 9.700, steps: 15\n",
      "Episode 42: reward: 12.400, steps: 19\n",
      "Episode 43: reward: 8.200, steps: 14\n",
      "Episode 44: reward: 4.700, steps: 10\n",
      "Episode 45: reward: 0.600, steps: 7\n",
      "Episode 46: reward: 0.200, steps: 7\n",
      "Episode 47: reward: 4.100, steps: 11\n",
      "Episode 48: reward: 6.400, steps: 13\n",
      "Episode 49: reward: 9.400, steps: 15\n",
      "Episode 50: reward: 4.500, steps: 11\n",
      "Episode 51: reward: 8.900, steps: 15\n",
      "Episode 52: reward: 5.600, steps: 11\n",
      "Episode 53: reward: 4.900, steps: 11\n",
      "Episode 54: reward: 9.500, steps: 15\n",
      "Episode 55: reward: 7.000, steps: 14\n",
      "Episode 56: reward: 6.500, steps: 10\n",
      "Episode 57: reward: 3.900, steps: 11\n",
      "Episode 58: reward: 0.600, steps: 7\n",
      "Episode 59: reward: 0.300, steps: 9\n",
      "Episode 60: reward: 6.100, steps: 13\n",
      "Episode 61: reward: 4.600, steps: 11\n",
      "Episode 62: reward: 9.600, steps: 15\n",
      "Episode 63: reward: 8.200, steps: 15\n",
      "Episode 64: reward: 3.400, steps: 11\n",
      "Episode 65: reward: 9.100, steps: 15\n",
      "Episode 66: reward: -8.700, steps: 2\n",
      "Episode 67: reward: 8.200, steps: 15\n",
      "Episode 68: reward: 8.000, steps: 14\n",
      "Episode 69: reward: 4.800, steps: 11\n",
      "Episode 70: reward: 0.600, steps: 7\n",
      "Episode 71: reward: 8.300, steps: 10\n",
      "Episode 72: reward: 1.800, steps: 7\n",
      "Episode 73: reward: -10.100, steps: 1\n",
      "Episode 74: reward: 4.300, steps: 11\n",
      "Episode 75: reward: 4.300, steps: 11\n",
      "Episode 76: reward: -10.100, steps: 1\n",
      "Episode 77: reward: 1.000, steps: 7\n",
      "Episode 78: reward: 7.000, steps: 14\n",
      "Episode 79: reward: 17.500, steps: 21\n",
      "Episode 80: reward: 3.700, steps: 10\n",
      "Episode 81: reward: 8.600, steps: 15\n",
      "Episode 82: reward: 5.400, steps: 11\n",
      "Episode 83: reward: 8.800, steps: 15\n",
      "Episode 84: reward: 7.100, steps: 14\n",
      "Episode 85: reward: 2.700, steps: 10\n",
      "Episode 86: reward: 12.200, steps: 19\n",
      "Episode 87: reward: 9.000, steps: 15\n",
      "Episode 88: reward: 3.100, steps: 11\n",
      "Episode 89: reward: 8.800, steps: 15\n",
      "Episode 90: reward: 8.700, steps: 11\n",
      "Episode 91: reward: 12.500, steps: 19\n",
      "Episode 92: reward: 2.300, steps: 9\n",
      "Episode 93: reward: 2.300, steps: 10\n",
      "Episode 94: reward: -10.100, steps: 1\n",
      "Episode 95: reward: 7.100, steps: 15\n",
      "Episode 96: reward: 4.200, steps: 11\n",
      "Episode 97: reward: -7.300, steps: 3\n",
      "Episode 98: reward: 4.600, steps: 11\n",
      "Episode 99: reward: 13.200, steps: 19\n",
      "Episode 100: reward: 4.300, steps: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'319.531531;250000;3.3461086061556564;21.89999999999999;-10.4;5.1579999999999995;17.499999999999993;-10.1;21.106535362578335;58;0;100;100;1;0.0007;32;0.001;avg;linear;2;128;64;64\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai = KniffelAI(save=True, load=False)\n",
    "\n",
    "# ai.play(path=\"weights\\p_date=2022-05-04-14_31_58\", episodes=1_000)\n",
    "\n",
    "# ai.grid_search_test(nb_steps=10_00x0)\n",
    "\n",
    "hyperparameter = {\n",
    "    \"windows_length\": 1,\n",
    "    \"adam_learning_rate\": 0.0007,\n",
    "    \"batch_size\": 32,\n",
    "    \"target_model_update\": 1e-3,\n",
    "    \"dueling_option\": \"avg\",\n",
    "    \"activation\": \"linear\",\n",
    "    \"layers\": 2,\n",
    "    \"unit_1\": 128,\n",
    "    \"unit_2\": 64,\n",
    "    \"unit_3\": 64,\n",
    "}\n",
    "\n",
    "ai.train(\n",
    "    hyperparameter=hyperparameter,\n",
    "    nb_steps=250_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7724bf7a85847efc5bb6a1829c232f0862f78efa3c8535cdc9aa81401d00c18c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
