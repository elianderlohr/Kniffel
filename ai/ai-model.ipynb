{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elianderlohr/Kniffel/blob/feature%2Fai/ai/ai-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZbB2tTectNx",
        "outputId": "ff3ee564-7e83-4a77-9016-bd649c3251ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Git\n",
            "Wed May 18 15:06:15 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd '/content/drive/MyDrive/Git'\n",
        "\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n",
        "except ImportError as e:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ezGGzkUkfi",
        "outputId": "ec3c2d73-d30e-43c1-cf78-30fcb9378e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-rl\n",
            "  Downloading keras-rl-0.4.2.tar.gz (40 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20 kB 38.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl) (2.8.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-py3-none-any.whl size=48378 sha256=17990126e6c163d87ffefbfcbcc879b1168278e9ef8cdb185dfab37e4afb757c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/23/e9/278c2e59c322236e2bfdf7c792c16f0b4dec24816d27a3f1e4\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.1)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 28.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.25.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lop1FAXFctN0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'Kniffel')\n",
        "sys.path.insert(0, 'Kniffel/ai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3A2YD8QYctN0"
      },
      "outputs": [],
      "source": [
        "from ai.hyperparameter import Hyperparameter\n",
        "from ai.ai import KniffelAI\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ILWBfrctN1",
        "outputId": "4e817956-71bb-458c-86ea-d2803ed75206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 150 combinations to test.\n"
          ]
        }
      ],
      "source": [
        "units = list(range(16, 64, 16))\n",
        "\n",
        "base_hp = {\n",
        "    \"windows_length\": [1],\n",
        "    \"adam_learning_rate\": np.arange(0.0001, 0.001, 0.0002),\n",
        "    \"batch_size\": [32],\n",
        "    \"target_model_update\": np.arange(0.0001, 0.001, 0.0002),\n",
        "    \"dueling_option\": [\"avg\"],\n",
        "    \"activation\": [\"linear\"],\n",
        "    \"layers\": [1, 2],\n",
        "    \"unit_1\": units,\n",
        "    \"unit_2\": units,\n",
        "}\n",
        "\n",
        "ai = KniffelAI(\n",
        "    save=False, load=False, predefined_layers=True, hyperparater_base=base_hp, path_prefix=\"/content/drive/MyDrive/Git/Kniffel/\"\n",
        ")\n",
        "\n",
        "env_config = {\n",
        "        \"reward_step\": 0,\n",
        "        \"reward_round\": 0.5,\n",
        "        \"reward_roll_dice\": 0.25,\n",
        "        \"reward_game_over\": -100,\n",
        "        \"reward_bonus\": 2,\n",
        "        \"reward_finish\": 10,\n",
        "        \"reward_zero_dice\": -0.5,\n",
        "        \"reward_one_dice\": -0.2,\n",
        "        \"reward_two_dice\": -0.1,\n",
        "        \"reward_three_dice\": 0.5,\n",
        "        \"reward_four_dice\": 0.6,\n",
        "        \"reward_five_dice\": 0.8,\n",
        "        \"reward_six_dice\": 1,\n",
        "        \"reward_kniffel\": 1.5,\n",
        "        \"reward_small_street\": 1,\n",
        "        \"reward_large_street\": 1.1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dhYYUUbEctN2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4052e3a-27f1-4839-f5c6-b58352765f04"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 300 combinations to test.\n",
            "windows_length;adam_learning_rate;batch_size;target_model_update;dueling_option;activation;layers;unit_1;unit_2\n",
            "Created 300 combinations to test.\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 1 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005000000000000001, 'batch_size': 32, 'target_model_update': 0.00030000000000000003, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 208)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                6688      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 44)                1452      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,140\n",
            "Trainable params: 8,140\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 99s 10ms/step - reward: -16.2706\n",
            "1665 episodes - episode_reward: -97.723 [-100.142, -92.757] - loss: 254.938 - mae: 50.454 - mean_q: -14.610\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: -11.1145\n",
            "1152 episodes - episode_reward: -96.481 [-99.647, -91.041] - loss: 75.586 - mae: 71.517 - mean_q: -43.222\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 104s 10ms/step - reward: -11.2450\n",
            "1165 episodes - episode_reward: -96.525 [-99.789, -90.867] - loss: 33.393 - mae: 87.970 - mean_q: -66.080\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 104s 10ms/step - reward: -11.4156\n",
            "1182 episodes - episode_reward: -96.576 [-99.974, -91.077] - loss: 19.106 - mae: 94.067 - mean_q: -76.851\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 107s 11ms/step - reward: -11.9500\n",
            "done, took 516.222 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.841, steps: 5\n",
            "Episode 2: reward: -97.591, steps: 5\n",
            "Episode 3: reward: -95.629, steps: 9\n",
            "Episode 4: reward: -95.373, steps: 10\n",
            "Episode 5: reward: -98.562, steps: 6\n",
            "Episode 6: reward: -93.541, steps: 15\n",
            "Episode 7: reward: -98.106, steps: 7\n",
            "Episode 8: reward: -97.256, steps: 7\n",
            "Episode 9: reward: -98.569, steps: 6\n",
            "Episode 10: reward: -98.362, steps: 6\n",
            "Episode 11: reward: -97.673, steps: 7\n",
            "Episode 12: reward: -93.630, steps: 13\n",
            "Episode 13: reward: -94.041, steps: 14\n",
            "Episode 14: reward: -99.295, steps: 3\n",
            "Episode 15: reward: -96.515, steps: 8\n",
            "Episode 16: reward: -97.741, steps: 5\n",
            "Episode 17: reward: -92.068, steps: 17\n",
            "Episode 18: reward: -92.704, steps: 17\n",
            "Episode 19: reward: -94.695, steps: 12\n",
            "Episode 20: reward: -97.165, steps: 7\n",
            "Episode 21: reward: -99.647, steps: 2\n",
            "Episode 22: reward: -96.249, steps: 9\n",
            "Episode 23: reward: -97.103, steps: 10\n",
            "Episode 24: reward: -92.700, steps: 13\n",
            "Episode 25: reward: -97.412, steps: 6\n",
            "Episode 26: reward: -97.862, steps: 6\n",
            "Episode 27: reward: -95.259, steps: 12\n",
            "Episode 28: reward: -95.194, steps: 12\n",
            "Episode 29: reward: -96.695, steps: 8\n",
            "Episode 30: reward: -99.295, steps: 3\n",
            "Episode 31: reward: -97.791, steps: 5\n",
            "Episode 32: reward: -97.641, steps: 5\n",
            "Episode 33: reward: -92.992, steps: 16\n",
            "Episode 34: reward: -99.295, steps: 3\n",
            "Episode 35: reward: -99.295, steps: 3\n",
            "Episode 36: reward: -95.580, steps: 11\n",
            "Episode 37: reward: -93.401, steps: 15\n",
            "Episode 38: reward: -97.841, steps: 5\n",
            "Episode 39: reward: -97.641, steps: 5\n",
            "Episode 40: reward: -97.412, steps: 6\n",
            "Episode 41: reward: -96.261, steps: 8\n",
            "Episode 42: reward: -96.369, steps: 9\n",
            "Episode 43: reward: -96.198, steps: 10\n",
            "Episode 44: reward: -95.910, steps: 10\n",
            "Episode 45: reward: -98.289, steps: 4\n",
            "Episode 46: reward: -98.106, steps: 7\n",
            "Episode 47: reward: -97.662, steps: 6\n",
            "Episode 48: reward: -97.891, steps: 5\n",
            "Episode 49: reward: -97.512, steps: 6\n",
            "Episode 50: reward: -97.791, steps: 5\n",
            "Episode 51: reward: -96.309, steps: 9\n",
            "Episode 52: reward: -93.474, steps: 15\n",
            "Episode 53: reward: -97.288, steps: 7\n",
            "Episode 54: reward: -97.891, steps: 5\n",
            "Episode 55: reward: -99.295, steps: 3\n",
            "Episode 56: reward: -97.666, steps: 7\n",
            "Episode 57: reward: -95.092, steps: 14\n",
            "Episode 58: reward: -97.612, steps: 6\n",
            "Episode 59: reward: -97.841, steps: 5\n",
            "Episode 60: reward: -97.412, steps: 6\n",
            "Episode 61: reward: -97.056, steps: 7\n",
            "Episode 62: reward: -99.295, steps: 3\n",
            "Episode 63: reward: -96.244, steps: 9\n",
            "Episode 64: reward: -97.812, steps: 6\n",
            "Episode 65: reward: -96.171, steps: 10\n",
            "Episode 66: reward: -95.078, steps: 12\n",
            "Episode 67: reward: -96.669, steps: 9\n",
            "Episode 68: reward: -96.137, steps: 10\n",
            "Episode 69: reward: -96.133, steps: 9\n",
            "Episode 70: reward: -95.315, steps: 11\n",
            "Episode 71: reward: -97.906, steps: 7\n",
            "Episode 72: reward: -95.548, steps: 10\n",
            "Episode 73: reward: -97.812, steps: 6\n",
            "Episode 74: reward: -92.575, steps: 16\n",
            "Episode 75: reward: -96.714, steps: 8\n",
            "Episode 76: reward: -96.695, steps: 8\n",
            "Episode 77: reward: -99.295, steps: 3\n",
            "Episode 78: reward: -95.246, steps: 12\n",
            "Episode 79: reward: -92.842, steps: 16\n",
            "Episode 80: reward: -97.662, steps: 6\n",
            "Episode 81: reward: -99.295, steps: 3\n",
            "Episode 82: reward: -96.130, steps: 12\n",
            "Episode 83: reward: -96.488, steps: 10\n",
            "Episode 84: reward: -94.886, steps: 13\n",
            "Episode 85: reward: -95.106, steps: 12\n",
            "Episode 86: reward: -99.295, steps: 3\n",
            "Episode 87: reward: -95.675, steps: 10\n",
            "Episode 88: reward: -92.590, steps: 16\n",
            "Episode 89: reward: -95.991, steps: 10\n",
            "Episode 90: reward: -95.602, steps: 11\n",
            "Episode 91: reward: -92.366, steps: 16\n",
            "Episode 92: reward: -96.856, steps: 7\n",
            "Episode 93: reward: -96.369, steps: 9\n",
            "Episode 94: reward: -97.462, steps: 6\n",
            "Episode 95: reward: -95.428, steps: 12\n",
            "Episode 96: reward: -91.705, steps: 16\n",
            "Episode 97: reward: -95.897, steps: 10\n",
            "Episode 98: reward: -97.673, steps: 8\n",
            "Episode 99: reward: -97.412, steps: 6\n",
            "Episode 100: reward: -95.370, steps: 11\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 2 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0009000000000000002, 'batch_size': 32, 'target_model_update': 0.0009000000000000002, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                6688      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 44)                1452      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,140\n",
            "Trainable params: 8,140\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: -16.3016\n",
            "1667 episodes - episode_reward: -97.790 [-100.142, -93.497] - loss: 174.112 - mae: 65.633 - mean_q: -39.783\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 116s 12ms/step - reward: -15.4586\n",
            "1582 episodes - episode_reward: -97.717 [-100.109, -92.785] - loss: 22.704 - mae: 93.799 - mean_q: -79.771\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -13.7437\n",
            "1410 episodes - episode_reward: -97.471 [-100.109, -90.927] - loss: 13.934 - mae: 94.243 - mean_q: -85.018\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: -10.7518\n",
            "1116 episodes - episode_reward: -96.342 [-99.974, -88.891] - loss: 6.362 - mae: 94.000 - mean_q: -87.471\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 107s 11ms/step - reward: -9.9667\n",
            "done, took 541.136 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -95.462, steps: 10\n",
            "Episode 2: reward: -99.647, steps: 2\n",
            "Episode 3: reward: -99.295, steps: 3\n",
            "Episode 4: reward: -94.010, steps: 15\n",
            "Episode 5: reward: -97.106, steps: 7\n",
            "Episode 6: reward: -96.525, steps: 9\n",
            "Episode 7: reward: -96.103, steps: 10\n",
            "Episode 8: reward: -99.647, steps: 2\n",
            "Episode 9: reward: -99.295, steps: 3\n",
            "Episode 10: reward: -96.001, steps: 8\n",
            "Episode 11: reward: -95.371, steps: 12\n",
            "Episode 12: reward: -99.295, steps: 3\n",
            "Episode 13: reward: -95.514, steps: 11\n",
            "Episode 14: reward: -99.974, steps: 1\n",
            "Episode 15: reward: -95.980, steps: 11\n",
            "Episode 16: reward: -94.808, steps: 13\n",
            "Episode 17: reward: -95.055, steps: 12\n",
            "Episode 18: reward: -95.105, steps: 14\n",
            "Episode 19: reward: -94.168, steps: 14\n",
            "Episode 20: reward: -95.950, steps: 10\n",
            "Episode 21: reward: -94.808, steps: 13\n",
            "Episode 22: reward: -95.588, steps: 12\n",
            "Episode 23: reward: -91.815, steps: 14\n",
            "Episode 24: reward: -94.831, steps: 13\n",
            "Episode 25: reward: -94.069, steps: 14\n",
            "Episode 26: reward: -94.746, steps: 12\n",
            "Episode 27: reward: -95.514, steps: 11\n",
            "Episode 28: reward: -93.883, steps: 15\n",
            "Episode 29: reward: -96.840, steps: 9\n",
            "Episode 30: reward: -98.362, steps: 6\n",
            "Episode 31: reward: -94.154, steps: 14\n",
            "Episode 32: reward: -96.210, steps: 10\n",
            "Episode 33: reward: -94.666, steps: 13\n",
            "Episode 34: reward: -94.570, steps: 11\n",
            "Episode 35: reward: -95.409, steps: 10\n",
            "Episode 36: reward: -99.295, steps: 3\n",
            "Episode 37: reward: -97.204, steps: 8\n",
            "Episode 38: reward: -99.295, steps: 3\n",
            "Episode 39: reward: -95.830, steps: 10\n",
            "Episode 40: reward: -96.001, steps: 8\n",
            "Episode 41: reward: -96.870, steps: 9\n",
            "Episode 42: reward: -95.401, steps: 12\n",
            "Episode 43: reward: -97.011, steps: 8\n",
            "Episode 44: reward: -93.400, steps: 15\n",
            "Episode 45: reward: -90.784, steps: 18\n",
            "Episode 46: reward: -95.260, steps: 11\n",
            "Episode 47: reward: -95.612, steps: 12\n",
            "Episode 48: reward: -95.387, steps: 11\n",
            "Episode 49: reward: -93.514, steps: 14\n",
            "Episode 50: reward: -96.340, steps: 9\n",
            "Episode 51: reward: -94.768, steps: 12\n",
            "Episode 52: reward: -96.416, steps: 10\n",
            "Episode 53: reward: -99.974, steps: 1\n",
            "Episode 54: reward: -92.236, steps: 15\n",
            "Episode 55: reward: -99.295, steps: 3\n",
            "Episode 56: reward: -94.129, steps: 14\n",
            "Episode 57: reward: -96.490, steps: 9\n",
            "Episode 58: reward: -96.239, steps: 10\n",
            "Episode 59: reward: -93.318, steps: 13\n",
            "Episode 60: reward: -96.990, steps: 9\n",
            "Episode 61: reward: -93.894, steps: 14\n",
            "Episode 62: reward: -93.476, steps: 15\n",
            "Episode 63: reward: -95.900, steps: 11\n",
            "Episode 64: reward: -95.950, steps: 10\n",
            "Episode 65: reward: -94.010, steps: 15\n",
            "Episode 66: reward: -96.570, steps: 9\n",
            "Episode 67: reward: -94.538, steps: 12\n",
            "Episode 68: reward: -96.820, steps: 9\n",
            "Episode 69: reward: -95.950, steps: 11\n",
            "Episode 70: reward: -94.009, steps: 14\n",
            "Episode 71: reward: -96.066, steps: 10\n",
            "Episode 72: reward: -99.295, steps: 3\n",
            "Episode 73: reward: -94.358, steps: 14\n",
            "Episode 74: reward: -94.036, steps: 15\n",
            "Episode 75: reward: -95.310, steps: 9\n",
            "Episode 76: reward: -99.974, steps: 1\n",
            "Episode 77: reward: -97.154, steps: 8\n",
            "Episode 78: reward: -95.939, steps: 10\n",
            "Episode 79: reward: -95.525, steps: 12\n",
            "Episode 80: reward: -96.590, steps: 9\n",
            "Episode 81: reward: -94.369, steps: 14\n",
            "Episode 82: reward: -97.020, steps: 9\n",
            "Episode 83: reward: -95.830, steps: 11\n",
            "Episode 84: reward: -96.961, steps: 8\n",
            "Episode 85: reward: -96.117, steps: 10\n",
            "Episode 86: reward: -94.520, steps: 11\n",
            "Episode 87: reward: -92.256, steps: 16\n",
            "Episode 88: reward: -95.324, steps: 11\n",
            "Episode 89: reward: -99.295, steps: 3\n",
            "Episode 90: reward: -96.921, steps: 8\n",
            "Episode 91: reward: -94.520, steps: 11\n",
            "Episode 92: reward: -99.974, steps: 1\n",
            "Episode 93: reward: -93.829, steps: 14\n",
            "Episode 94: reward: -95.810, steps: 10\n",
            "Episode 95: reward: -93.956, steps: 15\n",
            "Episode 96: reward: -95.348, steps: 12\n",
            "Episode 97: reward: -93.829, steps: 14\n",
            "Episode 98: reward: -96.602, steps: 9\n",
            "Episode 99: reward: -95.514, steps: 11\n",
            "Episode 100: reward: -95.791, steps: 12\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 3 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005000000000000001, 'batch_size': 32, 'target_model_update': 0.0007000000000000001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                6688      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 44)                1452      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,140\n",
            "Trainable params: 8,140\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 100s 10ms/step - reward: -18.1429\n",
            "1850 episodes - episode_reward: -98.071 [-100.135, -93.353] - loss: 240.936 - mae: 59.830 - mean_q: -31.490\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 109s 11ms/step - reward: -16.0078\n",
            "1637 episodes - episode_reward: -97.787 [-99.974, -93.509] - loss: 35.974 - mae: 91.784 - mean_q: -77.630\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -17.1767\n",
            "1749 episodes - episode_reward: -98.209 [-99.974, -92.595] - loss: 31.486 - mae: 91.654 - mean_q: -79.625\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 107s 11ms/step - reward: -11.6439\n",
            "1205 episodes - episode_reward: -96.629 [-99.974, -91.349] - loss: 16.808 - mae: 93.015 - mean_q: -84.081\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: -11.8655\n",
            "done, took 536.952 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -93.890, steps: 14\n",
            "Episode 2: reward: -96.088, steps: 10\n",
            "Episode 3: reward: -97.991, steps: 5\n",
            "Episode 4: reward: -98.259, steps: 5\n",
            "Episode 5: reward: -94.149, steps: 14\n",
            "Episode 6: reward: -97.662, steps: 6\n",
            "Episode 7: reward: -99.974, steps: 1\n",
            "Episode 8: reward: -95.086, steps: 12\n",
            "Episode 9: reward: -99.974, steps: 1\n",
            "Episode 10: reward: -99.647, steps: 2\n",
            "Episode 11: reward: -96.691, steps: 9\n",
            "Episode 12: reward: -99.974, steps: 1\n",
            "Episode 13: reward: -94.095, steps: 13\n",
            "Episode 14: reward: -94.278, steps: 13\n",
            "Episode 15: reward: -94.665, steps: 13\n",
            "Episode 16: reward: -95.898, steps: 10\n",
            "Episode 17: reward: -96.654, steps: 8\n",
            "Episode 18: reward: -97.691, steps: 5\n",
            "Episode 19: reward: -95.923, steps: 9\n",
            "Episode 20: reward: -95.288, steps: 12\n",
            "Episode 21: reward: -96.811, steps: 9\n",
            "Episode 22: reward: -98.149, steps: 4\n",
            "Episode 23: reward: -96.182, steps: 10\n",
            "Episode 24: reward: -93.890, steps: 14\n",
            "Episode 25: reward: -94.228, steps: 13\n",
            "Episode 26: reward: -95.824, steps: 9\n",
            "Episode 27: reward: -94.228, steps: 13\n",
            "Episode 28: reward: -94.812, steps: 12\n",
            "Episode 29: reward: -97.691, steps: 5\n",
            "Episode 30: reward: -97.021, steps: 8\n",
            "Episode 31: reward: -95.108, steps: 12\n",
            "Episode 32: reward: -96.148, steps: 10\n",
            "Episode 33: reward: -99.041, steps: 5\n",
            "Episode 34: reward: -96.099, steps: 10\n",
            "Episode 35: reward: -94.699, steps: 13\n",
            "Episode 36: reward: -96.177, steps: 9\n",
            "Episode 37: reward: -98.289, steps: 4\n",
            "Episode 38: reward: -97.712, steps: 6\n",
            "Episode 39: reward: -99.974, steps: 1\n",
            "Episode 40: reward: -97.416, steps: 7\n",
            "Episode 41: reward: -99.289, steps: 4\n",
            "Episode 42: reward: -99.647, steps: 2\n",
            "Episode 43: reward: -95.121, steps: 14\n",
            "Episode 44: reward: -97.741, steps: 5\n",
            "Episode 45: reward: -95.764, steps: 9\n",
            "Episode 46: reward: -97.783, steps: 6\n",
            "Episode 47: reward: -95.668, steps: 10\n",
            "Episode 48: reward: -96.240, steps: 9\n",
            "Episode 49: reward: -94.350, steps: 14\n",
            "Episode 50: reward: -96.560, steps: 9\n",
            "Episode 51: reward: -94.141, steps: 13\n",
            "Episode 52: reward: -97.791, steps: 5\n",
            "Episode 53: reward: -97.156, steps: 7\n",
            "Episode 54: reward: -97.356, steps: 7\n",
            "Episode 55: reward: -99.974, steps: 1\n",
            "Episode 56: reward: -97.462, steps: 6\n",
            "Episode 57: reward: -97.876, steps: 6\n",
            "Episode 58: reward: -94.679, steps: 13\n",
            "Episode 59: reward: -96.511, steps: 9\n",
            "Episode 60: reward: -99.974, steps: 1\n",
            "Episode 61: reward: -99.974, steps: 1\n",
            "Episode 62: reward: -97.406, steps: 7\n",
            "Episode 63: reward: -97.783, steps: 6\n",
            "Episode 64: reward: -96.710, steps: 9\n",
            "Episode 65: reward: -95.865, steps: 14\n",
            "Episode 66: reward: -98.009, steps: 4\n",
            "Episode 67: reward: -99.974, steps: 1\n",
            "Episode 68: reward: -96.594, steps: 8\n",
            "Episode 69: reward: -97.712, steps: 6\n",
            "Episode 70: reward: -99.295, steps: 3\n",
            "Episode 71: reward: -96.069, steps: 10\n",
            "Episode 72: reward: -99.041, steps: 5\n",
            "Episode 73: reward: -99.295, steps: 3\n",
            "Episode 74: reward: -96.096, steps: 10\n",
            "Episode 75: reward: -95.402, steps: 11\n",
            "Episode 76: reward: -99.974, steps: 1\n",
            "Episode 77: reward: -97.512, steps: 6\n",
            "Episode 78: reward: -97.462, steps: 6\n",
            "Episode 79: reward: -99.295, steps: 3\n",
            "Episode 80: reward: -96.717, steps: 8\n",
            "Episode 81: reward: -94.191, steps: 14\n",
            "Episode 82: reward: -94.039, steps: 12\n",
            "Episode 83: reward: -96.111, steps: 10\n",
            "Episode 84: reward: -96.025, steps: 10\n",
            "Episode 85: reward: -97.512, steps: 6\n",
            "Episode 86: reward: -97.896, steps: 6\n",
            "Episode 87: reward: -94.571, steps: 12\n",
            "Episode 88: reward: -95.731, steps: 12\n",
            "Episode 89: reward: -96.429, steps: 9\n",
            "Episode 90: reward: -95.273, steps: 14\n",
            "Episode 91: reward: -94.525, steps: 13\n",
            "Episode 92: reward: -97.396, steps: 7\n",
            "Episode 93: reward: -97.316, steps: 7\n",
            "Episode 94: reward: -94.411, steps: 14\n",
            "Episode 95: reward: -96.910, steps: 9\n",
            "Episode 96: reward: -97.941, steps: 5\n",
            "Episode 97: reward: -99.041, steps: 5\n",
            "Episode 98: reward: -97.206, steps: 7\n",
            "Episode 99: reward: -96.224, steps: 13\n",
            "Episode 100: reward: -91.070, steps: 17\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 4 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005000000000000001, 'batch_size': 32, 'target_model_update': 0.0007000000000000001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_3 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 44)                2156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,188\n",
            "Trainable params: 12,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 99s 10ms/step - reward: -15.3338\n",
            "1571 episodes - episode_reward: -97.606 [-100.135, -92.061] - loss: 206.356 - mae: 57.411 - mean_q: -29.590\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 106s 11ms/step - reward: -11.7498\n",
            "1215 episodes - episode_reward: -96.707 [-99.974, -91.858] - loss: 24.863 - mae: 89.180 - mean_q: -73.996\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -15.3207\n",
            "1567 episodes - episode_reward: -97.769 [-99.974, -91.357] - loss: 17.595 - mae: 94.715 - mean_q: -82.508\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -16.1865\n",
            "1653 episodes - episode_reward: -97.923 [-100.142, -91.645] - loss: 16.608 - mae: 93.553 - mean_q: -84.210\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 107s 11ms/step - reward: -10.9158\n",
            "done, took 534.841 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -92.906, steps: 15\n",
            "Episode 2: reward: -93.232, steps: 14\n",
            "Episode 3: reward: -97.641, steps: 5\n",
            "Episode 4: reward: -95.265, steps: 12\n",
            "Episode 5: reward: -92.907, steps: 14\n",
            "Episode 6: reward: -94.880, steps: 13\n",
            "Episode 7: reward: -94.312, steps: 12\n",
            "Episode 8: reward: -96.670, steps: 9\n",
            "Episode 9: reward: -97.206, steps: 7\n",
            "Episode 10: reward: -99.974, steps: 1\n",
            "Episode 11: reward: -96.129, steps: 9\n",
            "Episode 12: reward: -96.694, steps: 8\n",
            "Episode 13: reward: -98.044, steps: 7\n",
            "Episode 14: reward: -99.429, steps: 2\n",
            "Episode 15: reward: -94.319, steps: 14\n",
            "Episode 16: reward: -97.156, steps: 7\n",
            "Episode 17: reward: -97.462, steps: 6\n",
            "Episode 18: reward: -97.691, steps: 5\n",
            "Episode 19: reward: -97.812, steps: 6\n",
            "Episode 20: reward: -94.459, steps: 12\n",
            "Episode 21: reward: -96.729, steps: 9\n",
            "Episode 22: reward: -93.723, steps: 15\n",
            "Episode 23: reward: -97.034, steps: 8\n",
            "Episode 24: reward: -97.662, steps: 6\n",
            "Episode 25: reward: -96.330, steps: 11\n",
            "Episode 26: reward: -94.477, steps: 13\n",
            "Episode 27: reward: -97.406, steps: 7\n",
            "Episode 28: reward: -96.891, steps: 7\n",
            "Episode 29: reward: -94.733, steps: 13\n",
            "Episode 30: reward: -96.137, steps: 10\n",
            "Episode 31: reward: -97.686, steps: 6\n",
            "Episode 32: reward: -97.206, steps: 7\n",
            "Episode 33: reward: -97.206, steps: 7\n",
            "Episode 34: reward: -94.460, steps: 14\n",
            "Episode 35: reward: -97.256, steps: 7\n",
            "Episode 36: reward: -96.684, steps: 10\n",
            "Episode 37: reward: -96.357, steps: 11\n",
            "Episode 38: reward: -97.006, steps: 7\n",
            "Episode 39: reward: -93.693, steps: 14\n",
            "Episode 40: reward: -99.974, steps: 1\n",
            "Episode 41: reward: -94.293, steps: 13\n",
            "Episode 42: reward: -97.662, steps: 6\n",
            "Episode 43: reward: -97.612, steps: 6\n",
            "Episode 44: reward: -95.658, steps: 11\n",
            "Episode 45: reward: -97.306, steps: 7\n",
            "Episode 46: reward: -97.156, steps: 7\n",
            "Episode 47: reward: -93.137, steps: 14\n",
            "Episode 48: reward: -97.712, steps: 6\n",
            "Episode 49: reward: -99.974, steps: 1\n",
            "Episode 50: reward: -95.191, steps: 12\n",
            "Episode 51: reward: -99.974, steps: 1\n",
            "Episode 52: reward: -95.434, steps: 11\n",
            "Episode 53: reward: -94.055, steps: 13\n",
            "Episode 54: reward: -94.502, steps: 13\n",
            "Episode 55: reward: -97.256, steps: 7\n",
            "Episode 56: reward: -96.870, steps: 9\n",
            "Episode 57: reward: -95.480, steps: 11\n",
            "Episode 58: reward: -96.009, steps: 9\n",
            "Episode 59: reward: -97.306, steps: 7\n",
            "Episode 60: reward: -96.635, steps: 8\n",
            "Episode 61: reward: -93.994, steps: 14\n",
            "Episode 62: reward: -97.118, steps: 7\n",
            "Episode 63: reward: -96.262, steps: 9\n",
            "Episode 64: reward: -96.684, steps: 8\n",
            "Episode 65: reward: -96.862, steps: 9\n",
            "Episode 66: reward: -97.912, steps: 6\n",
            "Episode 67: reward: -96.257, steps: 11\n",
            "Episode 68: reward: -97.562, steps: 6\n",
            "Episode 69: reward: -97.541, steps: 5\n",
            "Episode 70: reward: -93.599, steps: 14\n",
            "Episode 71: reward: -99.322, steps: 2\n",
            "Episode 72: reward: -96.282, steps: 10\n",
            "Episode 73: reward: -96.004, steps: 11\n",
            "Episode 74: reward: -93.950, steps: 14\n",
            "Episode 75: reward: -95.717, steps: 10\n",
            "Episode 76: reward: -100.035, steps: 2\n",
            "Episode 77: reward: -96.189, steps: 9\n",
            "Episode 78: reward: -96.069, steps: 10\n",
            "Episode 79: reward: -96.687, steps: 9\n",
            "Episode 80: reward: -99.974, steps: 1\n",
            "Episode 81: reward: -97.791, steps: 5\n",
            "Episode 82: reward: -97.562, steps: 6\n",
            "Episode 83: reward: -96.429, steps: 10\n",
            "Episode 84: reward: -97.462, steps: 6\n",
            "Episode 85: reward: -97.712, steps: 6\n",
            "Episode 86: reward: -96.669, steps: 9\n",
            "Episode 87: reward: -95.928, steps: 10\n",
            "Episode 88: reward: -97.056, steps: 7\n",
            "Episode 89: reward: -97.156, steps: 7\n",
            "Episode 90: reward: -97.056, steps: 7\n",
            "Episode 91: reward: -94.363, steps: 13\n",
            "Episode 92: reward: -97.912, steps: 6\n",
            "Episode 93: reward: -96.549, steps: 9\n",
            "Episode 94: reward: -97.206, steps: 7\n",
            "Episode 95: reward: -96.687, steps: 9\n",
            "Episode 96: reward: -94.292, steps: 13\n",
            "Episode 97: reward: -97.562, steps: 6\n",
            "Episode 98: reward: -95.433, steps: 11\n",
            "Episode 99: reward: -94.831, steps: 12\n",
            "Episode 100: reward: -95.208, steps: 10\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 5 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0001, 'batch_size': 32, 'target_model_update': 0.0005000000000000001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 44)                1452      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,052\n",
            "Trainable params: 13,052\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 109s 11ms/step - reward: -20.3745\n",
            "2072 episodes - episode_reward: -98.334 [-100.142, -93.922] - loss: 410.566 - mae: 48.742 - mean_q: -29.428\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -14.4813\n",
            "1486 episodes - episode_reward: -97.451 [-99.789, -93.476] - loss: 37.555 - mae: 81.049 - mean_q: -69.170\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 116s 12ms/step - reward: -15.1523\n",
            "1554 episodes - episode_reward: -97.505 [-99.974, -94.576] - loss: 17.178 - mae: 91.681 - mean_q: -85.703\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 117s 12ms/step - reward: -15.1929\n",
            "1557 episodes - episode_reward: -97.580 [-100.142, -93.614] - loss: 12.627 - mae: 93.142 - mean_q: -88.423\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 119s 12ms/step - reward: -16.8797\n",
            "done, took 574.737 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -94.698, steps: 13\n",
            "Episode 2: reward: -95.445, steps: 12\n",
            "Episode 3: reward: -99.974, steps: 1\n",
            "Episode 4: reward: -96.460, steps: 10\n",
            "Episode 5: reward: -99.263, steps: 4\n",
            "Episode 6: reward: -99.974, steps: 1\n",
            "Episode 7: reward: -95.340, steps: 11\n",
            "Episode 8: reward: -96.080, steps: 9\n",
            "Episode 9: reward: -95.750, steps: 11\n",
            "Episode 10: reward: -96.167, steps: 11\n",
            "Episode 11: reward: -99.974, steps: 1\n",
            "Episode 12: reward: -94.726, steps: 13\n",
            "Episode 13: reward: -99.974, steps: 1\n",
            "Episode 14: reward: -99.974, steps: 1\n",
            "Episode 15: reward: -99.263, steps: 4\n",
            "Episode 16: reward: -99.974, steps: 1\n",
            "Episode 17: reward: -94.915, steps: 12\n",
            "Episode 18: reward: -99.974, steps: 1\n",
            "Episode 19: reward: -99.263, steps: 4\n",
            "Episode 20: reward: -98.858, steps: 5\n",
            "Episode 21: reward: -99.974, steps: 1\n",
            "Episode 22: reward: -99.974, steps: 1\n",
            "Episode 23: reward: -99.416, steps: 4\n",
            "Episode 24: reward: -99.974, steps: 1\n",
            "Episode 25: reward: -99.974, steps: 1\n",
            "Episode 26: reward: -99.974, steps: 1\n",
            "Episode 27: reward: -99.974, steps: 1\n",
            "Episode 28: reward: -96.050, steps: 11\n",
            "Episode 29: reward: -99.974, steps: 1\n",
            "Episode 30: reward: -99.974, steps: 1\n",
            "Episode 31: reward: -97.024, steps: 8\n",
            "Episode 32: reward: -99.974, steps: 1\n",
            "Episode 33: reward: -99.974, steps: 1\n",
            "Episode 34: reward: -99.416, steps: 4\n",
            "Episode 35: reward: -95.265, steps: 12\n",
            "Episode 36: reward: -99.974, steps: 1\n",
            "Episode 37: reward: -97.324, steps: 8\n",
            "Episode 38: reward: -94.685, steps: 12\n",
            "Episode 39: reward: -99.974, steps: 1\n",
            "Episode 40: reward: -95.939, steps: 12\n",
            "Episode 41: reward: -99.974, steps: 1\n",
            "Episode 42: reward: -99.974, steps: 1\n",
            "Episode 43: reward: -94.565, steps: 12\n",
            "Episode 44: reward: -99.974, steps: 1\n",
            "Episode 45: reward: -99.974, steps: 1\n",
            "Episode 46: reward: -96.311, steps: 8\n",
            "Episode 47: reward: -95.135, steps: 12\n",
            "Episode 48: reward: -95.945, steps: 12\n",
            "Episode 49: reward: -96.017, steps: 9\n",
            "Episode 50: reward: -94.468, steps: 13\n",
            "Episode 51: reward: -99.974, steps: 1\n",
            "Episode 52: reward: -95.048, steps: 13\n",
            "Episode 53: reward: -99.974, steps: 1\n",
            "Episode 54: reward: -99.974, steps: 1\n",
            "Episode 55: reward: -99.443, steps: 4\n",
            "Episode 56: reward: -99.647, steps: 2\n",
            "Episode 57: reward: -96.154, steps: 11\n",
            "Episode 58: reward: -95.495, steps: 12\n",
            "Episode 59: reward: -95.803, steps: 11\n",
            "Episode 60: reward: -99.974, steps: 1\n",
            "Episode 61: reward: -99.316, steps: 4\n",
            "Episode 62: reward: -99.263, steps: 4\n",
            "Episode 63: reward: -99.416, steps: 4\n",
            "Episode 64: reward: -99.974, steps: 1\n",
            "Episode 65: reward: -95.595, steps: 12\n",
            "Episode 66: reward: -95.863, steps: 11\n",
            "Episode 67: reward: -99.416, steps: 4\n",
            "Episode 68: reward: -98.456, steps: 4\n",
            "Episode 69: reward: -99.974, steps: 1\n",
            "Episode 70: reward: -95.713, steps: 11\n",
            "Episode 71: reward: -96.004, steps: 11\n",
            "Episode 72: reward: -99.974, steps: 1\n",
            "Episode 73: reward: -95.045, steps: 12\n",
            "Episode 74: reward: -99.974, steps: 1\n",
            "Episode 75: reward: -99.974, steps: 1\n",
            "Episode 76: reward: -97.124, steps: 8\n",
            "Episode 77: reward: -99.974, steps: 1\n",
            "Episode 78: reward: -99.263, steps: 4\n",
            "Episode 79: reward: -95.745, steps: 12\n",
            "Episode 80: reward: -95.525, steps: 12\n",
            "Episode 81: reward: -99.974, steps: 1\n",
            "Episode 82: reward: -95.390, steps: 11\n",
            "Episode 83: reward: -99.974, steps: 1\n",
            "Episode 84: reward: -99.974, steps: 1\n",
            "Episode 85: reward: -99.974, steps: 1\n",
            "Episode 86: reward: -96.080, steps: 9\n",
            "Episode 87: reward: -99.974, steps: 1\n",
            "Episode 88: reward: -96.310, steps: 11\n",
            "Episode 89: reward: -99.416, steps: 4\n",
            "Episode 90: reward: -99.974, steps: 1\n",
            "Episode 91: reward: -99.974, steps: 1\n",
            "Episode 92: reward: -99.647, steps: 2\n",
            "Episode 93: reward: -94.738, steps: 14\n",
            "Episode 94: reward: -99.974, steps: 1\n",
            "Episode 95: reward: -99.024, steps: 5\n",
            "Episode 96: reward: -99.974, steps: 1\n",
            "Episode 97: reward: -99.974, steps: 1\n",
            "Episode 98: reward: -99.974, steps: 1\n",
            "Episode 99: reward: -99.974, steps: 1\n",
            "Episode 100: reward: -99.974, steps: 1\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 6 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0007000000000000001, 'batch_size': 32, 'target_model_update': 0.0007000000000000001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_5 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 44)                2156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,188\n",
            "Trainable params: 12,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 105s 10ms/step - reward: -16.8706\n",
            "1724 episodes - episode_reward: -97.858 [-100.129, -93.551] - loss: 197.036 - mae: 56.174 - mean_q: -22.977\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 111s 11ms/step - reward: -12.3036\n",
            "1270 episodes - episode_reward: -96.879 [-99.974, -91.948] - loss: 26.593 - mae: 85.519 - mean_q: -69.942\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 116s 12ms/step - reward: -16.1104\n",
            "1645 episodes - episode_reward: -97.937 [-99.974, -91.206] - loss: 18.620 - mae: 92.651 - mean_q: -81.329\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 114s 11ms/step - reward: -13.7833\n",
            "1415 episodes - episode_reward: -97.407 [-100.122, -91.361] - loss: 14.645 - mae: 92.516 - mean_q: -84.537\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -10.8889\n",
            "done, took 559.452 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -98.119, steps: 5\n",
            "Episode 2: reward: -96.956, steps: 7\n",
            "Episode 3: reward: -97.612, steps: 6\n",
            "Episode 4: reward: -94.618, steps: 12\n",
            "Episode 5: reward: -97.812, steps: 6\n",
            "Episode 6: reward: -97.662, steps: 6\n",
            "Episode 7: reward: -95.297, steps: 11\n",
            "Episode 8: reward: -95.434, steps: 11\n",
            "Episode 9: reward: -96.487, steps: 9\n",
            "Episode 10: reward: -97.969, steps: 6\n",
            "Episode 11: reward: -96.529, steps: 9\n",
            "Episode 12: reward: -97.712, steps: 6\n",
            "Episode 13: reward: -96.649, steps: 9\n",
            "Episode 14: reward: -96.417, steps: 10\n",
            "Episode 15: reward: -93.522, steps: 14\n",
            "Episode 16: reward: -97.612, steps: 6\n",
            "Episode 17: reward: -94.637, steps: 13\n",
            "Episode 18: reward: -97.256, steps: 7\n",
            "Episode 19: reward: -96.234, steps: 9\n",
            "Episode 20: reward: -92.380, steps: 15\n",
            "Episode 21: reward: -97.106, steps: 7\n",
            "Episode 22: reward: -93.146, steps: 15\n",
            "Episode 23: reward: -95.882, steps: 10\n",
            "Episode 24: reward: -94.650, steps: 13\n",
            "Episode 25: reward: -95.548, steps: 10\n",
            "Episode 26: reward: -97.406, steps: 7\n",
            "Episode 27: reward: -97.562, steps: 6\n",
            "Episode 28: reward: -94.162, steps: 14\n",
            "Episode 29: reward: -95.740, steps: 11\n",
            "Episode 30: reward: -97.612, steps: 6\n",
            "Episode 31: reward: -97.932, steps: 5\n",
            "Episode 32: reward: -95.702, steps: 10\n",
            "Episode 33: reward: -94.577, steps: 13\n",
            "Episode 34: reward: -93.537, steps: 14\n",
            "Episode 35: reward: -97.612, steps: 6\n",
            "Episode 36: reward: -97.412, steps: 6\n",
            "Episode 37: reward: -96.062, steps: 10\n",
            "Episode 38: reward: -98.569, steps: 6\n",
            "Episode 39: reward: -92.687, steps: 14\n",
            "Episode 40: reward: -95.928, steps: 10\n",
            "Episode 41: reward: -94.470, steps: 13\n",
            "Episode 42: reward: -95.882, steps: 10\n",
            "Episode 43: reward: -95.348, steps: 10\n",
            "Episode 44: reward: -96.335, steps: 11\n",
            "Episode 45: reward: -95.955, steps: 10\n",
            "Episode 46: reward: -96.114, steps: 9\n",
            "Episode 47: reward: -96.174, steps: 9\n",
            "Episode 48: reward: -97.162, steps: 6\n",
            "Episode 49: reward: -94.597, steps: 13\n",
            "Episode 50: reward: -96.884, steps: 8\n",
            "Episode 51: reward: -97.612, steps: 6\n",
            "Episode 52: reward: -96.162, steps: 8\n",
            "Episode 53: reward: -97.262, steps: 6\n",
            "Episode 54: reward: -95.138, steps: 12\n",
            "Episode 55: reward: -96.421, steps: 10\n",
            "Episode 56: reward: -97.462, steps: 6\n",
            "Episode 57: reward: -97.106, steps: 7\n",
            "Episode 58: reward: -95.208, steps: 12\n",
            "Episode 59: reward: -95.051, steps: 10\n",
            "Episode 60: reward: -94.917, steps: 11\n",
            "Episode 61: reward: -94.797, steps: 13\n",
            "Episode 62: reward: -96.409, steps: 9\n",
            "Episode 63: reward: -95.809, steps: 9\n",
            "Episode 64: reward: -94.303, steps: 13\n",
            "Episode 65: reward: -97.412, steps: 6\n",
            "Episode 66: reward: -97.662, steps: 6\n",
            "Episode 67: reward: -96.294, steps: 9\n",
            "Episode 68: reward: -95.701, steps: 10\n",
            "Episode 69: reward: -97.106, steps: 7\n",
            "Episode 70: reward: -96.594, steps: 9\n",
            "Episode 71: reward: -96.234, steps: 9\n",
            "Episode 72: reward: -94.670, steps: 13\n",
            "Episode 73: reward: -96.614, steps: 9\n",
            "Episode 74: reward: -95.668, steps: 10\n",
            "Episode 75: reward: -96.018, steps: 10\n",
            "Episode 76: reward: -95.610, steps: 11\n",
            "Episode 77: reward: -95.769, steps: 9\n",
            "Episode 78: reward: -97.612, steps: 6\n",
            "Episode 79: reward: -95.601, steps: 10\n",
            "Episode 80: reward: -93.350, steps: 13\n",
            "Episode 81: reward: -97.562, steps: 6\n",
            "Episode 82: reward: -96.055, steps: 10\n",
            "Episode 83: reward: -94.523, steps: 13\n",
            "Episode 84: reward: -95.357, steps: 11\n",
            "Episode 85: reward: -97.962, steps: 6\n",
            "Episode 86: reward: -94.180, steps: 14\n",
            "Episode 87: reward: -97.662, steps: 6\n",
            "Episode 88: reward: -94.723, steps: 13\n",
            "Episode 89: reward: -97.462, steps: 6\n",
            "Episode 90: reward: -96.055, steps: 10\n",
            "Episode 91: reward: -97.406, steps: 7\n",
            "Episode 92: reward: -95.482, steps: 12\n",
            "Episode 93: reward: -97.512, steps: 6\n",
            "Episode 94: reward: -96.956, steps: 7\n",
            "Episode 95: reward: -95.474, steps: 9\n",
            "Episode 96: reward: -96.295, steps: 10\n",
            "Episode 97: reward: -94.323, steps: 13\n",
            "Episode 98: reward: -96.135, steps: 10\n",
            "Episode 99: reward: -96.414, steps: 9\n",
            "Episode 100: reward: -94.490, steps: 13\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 7 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00030000000000000003, 'batch_size': 32, 'target_model_update': 0.0005000000000000001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_6 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                3344      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 44)                748       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,092\n",
            "Trainable params: 4,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -19.0605\n",
            "1942 episodes - episode_reward: -98.150 [-100.142, -95.074] - loss: 373.467 - mae: 52.065 - mean_q: -24.323\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 118s 12ms/step - reward: -15.9242\n",
            "1630 episodes - episode_reward: -97.693 [-99.974, -94.483] - loss: 37.957 - mae: 82.356 - mean_q: -66.537\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 118s 12ms/step - reward: -15.7123\n",
            "1610 episodes - episode_reward: -97.594 [-100.142, -93.710] - loss: 18.482 - mae: 93.246 - mean_q: -85.499\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 118s 12ms/step - reward: -15.4117\n",
            "1580 episodes - episode_reward: -97.540 [-100.142, -93.880] - loss: 13.324 - mae: 94.856 - mean_q: -88.528\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 120s 12ms/step - reward: -15.4320\n",
            "done, took 585.576 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.259, steps: 7\n",
            "Episode 2: reward: -98.523, steps: 4\n",
            "Episode 3: reward: -97.891, steps: 5\n",
            "Episode 4: reward: -97.573, steps: 7\n",
            "Episode 5: reward: -97.001, steps: 8\n",
            "Episode 6: reward: -96.868, steps: 8\n",
            "Episode 7: reward: -98.523, steps: 4\n",
            "Episode 8: reward: -98.072, steps: 5\n",
            "Episode 9: reward: -97.146, steps: 7\n",
            "Episode 10: reward: -96.827, steps: 8\n",
            "Episode 11: reward: -97.005, steps: 8\n",
            "Episode 12: reward: -97.941, steps: 5\n",
            "Episode 13: reward: -96.246, steps: 7\n",
            "Episode 14: reward: -96.755, steps: 9\n",
            "Episode 15: reward: -98.637, steps: 4\n",
            "Episode 16: reward: -98.637, steps: 4\n",
            "Episode 17: reward: -97.891, steps: 5\n",
            "Episode 18: reward: -98.767, steps: 4\n",
            "Episode 19: reward: -97.061, steps: 8\n",
            "Episode 20: reward: -98.196, steps: 4\n",
            "Episode 21: reward: -95.979, steps: 10\n",
            "Episode 22: reward: -96.941, steps: 8\n",
            "Episode 23: reward: -96.673, steps: 7\n",
            "Episode 24: reward: -97.118, steps: 8\n",
            "Episode 25: reward: -96.767, steps: 8\n",
            "Episode 26: reward: -96.616, steps: 7\n",
            "Episode 27: reward: -98.507, steps: 4\n",
            "Episode 28: reward: -98.243, steps: 4\n",
            "Episode 29: reward: -98.336, steps: 4\n",
            "Episode 30: reward: -98.476, steps: 4\n",
            "Episode 31: reward: -98.623, steps: 7\n",
            "Episode 32: reward: -98.212, steps: 5\n",
            "Episode 33: reward: -98.429, steps: 4\n",
            "Episode 34: reward: -97.055, steps: 9\n",
            "Episode 35: reward: -97.991, steps: 5\n",
            "Episode 36: reward: -98.476, steps: 4\n",
            "Episode 37: reward: -97.791, steps: 5\n",
            "Episode 38: reward: -98.243, steps: 4\n",
            "Episode 39: reward: -96.808, steps: 8\n",
            "Episode 40: reward: -97.121, steps: 8\n",
            "Episode 41: reward: -98.623, steps: 7\n",
            "Episode 42: reward: -97.876, steps: 6\n",
            "Episode 43: reward: -97.118, steps: 8\n",
            "Episode 44: reward: -97.629, steps: 7\n",
            "Episode 45: reward: -98.101, steps: 6\n",
            "Episode 46: reward: -96.400, steps: 9\n",
            "Episode 47: reward: -96.413, steps: 9\n",
            "Episode 48: reward: -97.061, steps: 8\n",
            "Episode 49: reward: -96.928, steps: 8\n",
            "Episode 50: reward: -98.885, steps: 3\n",
            "Episode 51: reward: -96.982, steps: 9\n",
            "Episode 52: reward: -98.336, steps: 4\n",
            "Episode 53: reward: -97.991, steps: 5\n",
            "Episode 54: reward: -97.891, steps: 5\n",
            "Episode 55: reward: -96.400, steps: 9\n",
            "Episode 56: reward: -98.897, steps: 4\n",
            "Episode 57: reward: -98.243, steps: 4\n",
            "Episode 58: reward: -95.514, steps: 11\n",
            "Episode 59: reward: -98.523, steps: 4\n",
            "Episode 60: reward: -98.383, steps: 4\n",
            "Episode 61: reward: -97.203, steps: 7\n",
            "Episode 62: reward: -97.991, steps: 5\n",
            "Episode 63: reward: -97.432, steps: 8\n",
            "Episode 64: reward: -96.616, steps: 7\n",
            "Episode 65: reward: -98.289, steps: 4\n",
            "Episode 66: reward: -98.383, steps: 4\n",
            "Episode 67: reward: -97.549, steps: 6\n",
            "Episode 68: reward: -97.516, steps: 7\n",
            "Episode 69: reward: -98.476, steps: 4\n",
            "Episode 70: reward: -98.289, steps: 4\n",
            "Episode 71: reward: -98.383, steps: 4\n",
            "Episode 72: reward: -98.336, steps: 4\n",
            "Episode 73: reward: -98.550, steps: 4\n",
            "Episode 74: reward: -97.506, steps: 7\n",
            "Episode 75: reward: -97.316, steps: 7\n",
            "Episode 76: reward: -99.295, steps: 3\n",
            "Episode 77: reward: -97.001, steps: 8\n",
            "Episode 78: reward: -96.267, steps: 9\n",
            "Episode 79: reward: -98.383, steps: 4\n",
            "Episode 80: reward: -98.523, steps: 4\n",
            "Episode 81: reward: -97.337, steps: 7\n",
            "Episode 82: reward: -96.322, steps: 10\n",
            "Episode 83: reward: -95.210, steps: 11\n",
            "Episode 84: reward: -96.115, steps: 8\n",
            "Episode 85: reward: -98.336, steps: 4\n",
            "Episode 86: reward: -97.991, steps: 5\n",
            "Episode 87: reward: -96.827, steps: 8\n",
            "Episode 88: reward: -99.295, steps: 3\n",
            "Episode 89: reward: -98.523, steps: 4\n",
            "Episode 90: reward: -96.477, steps: 9\n",
            "Episode 91: reward: -96.925, steps: 9\n",
            "Episode 92: reward: -96.948, steps: 8\n",
            "Episode 93: reward: -97.318, steps: 8\n",
            "Episode 94: reward: -96.707, steps: 8\n",
            "Episode 95: reward: -98.243, steps: 4\n",
            "Episode 96: reward: -96.868, steps: 8\n",
            "Episode 97: reward: -97.429, steps: 7\n",
            "Episode 98: reward: -98.476, steps: 4\n",
            "Episode 99: reward: -96.767, steps: 8\n",
            "Episode 100: reward: -96.242, steps: 10\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 8 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0009000000000000002, 'batch_size': 32, 'target_model_update': 0.00030000000000000003, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_7 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 44)                1452      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,052\n",
            "Trainable params: 13,052\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -14.6446\n",
            "1503 episodes - episode_reward: -97.436 [-100.122, -90.088] - loss: 174.191 - mae: 52.902 - mean_q: -9.217\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 118s 12ms/step - reward: -10.1355\n",
            "1055 episodes - episode_reward: -96.072 [-99.974, -90.450] - loss: 94.419 - mae: 68.591 - mean_q: -34.611\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 118s 12ms/step - reward: -10.3086\n",
            "1072 episodes - episode_reward: -96.163 [-99.974, -90.047] - loss: 36.158 - mae: 79.863 - mean_q: -56.179\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 123s 12ms/step - reward: -13.4092\n",
            "1375 episodes - episode_reward: -97.523 [-100.142, -90.998] - loss: 37.416 - mae: 83.955 - mean_q: -67.144\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 120s 12ms/step - reward: -10.9408\n",
            "done, took 589.287 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -95.286, steps: 12\n",
            "Episode 2: reward: -95.949, steps: 11\n",
            "Episode 3: reward: -95.078, steps: 13\n",
            "Episode 4: reward: -90.568, steps: 17\n",
            "Episode 5: reward: -96.768, steps: 10\n",
            "Episode 6: reward: -94.524, steps: 13\n",
            "Episode 7: reward: -96.358, steps: 10\n",
            "Episode 8: reward: -97.156, steps: 7\n",
            "Episode 9: reward: -97.056, steps: 7\n",
            "Episode 10: reward: -94.860, steps: 13\n",
            "Episode 11: reward: -99.647, steps: 2\n",
            "Episode 12: reward: -95.719, steps: 12\n",
            "Episode 13: reward: -92.860, steps: 15\n",
            "Episode 14: reward: -98.578, steps: 6\n",
            "Episode 15: reward: -94.176, steps: 14\n",
            "Episode 16: reward: -93.090, steps: 15\n",
            "Episode 17: reward: -97.406, steps: 7\n",
            "Episode 18: reward: -97.356, steps: 7\n",
            "Episode 19: reward: -98.341, steps: 5\n",
            "Episode 20: reward: -95.560, steps: 11\n",
            "Episode 21: reward: -93.903, steps: 15\n",
            "Episode 22: reward: -91.362, steps: 19\n",
            "Episode 23: reward: -94.112, steps: 13\n",
            "Episode 24: reward: -95.620, steps: 11\n",
            "Episode 25: reward: -97.456, steps: 6\n",
            "Episode 26: reward: -94.655, steps: 13\n",
            "Episode 27: reward: -93.892, steps: 14\n",
            "Episode 28: reward: -94.003, steps: 14\n",
            "Episode 29: reward: -94.363, steps: 14\n",
            "Episode 30: reward: -95.297, steps: 11\n",
            "Episode 31: reward: -94.679, steps: 12\n",
            "Episode 32: reward: -97.562, steps: 6\n",
            "Episode 33: reward: -95.924, steps: 13\n",
            "Episode 34: reward: -95.563, steps: 11\n",
            "Episode 35: reward: -95.690, steps: 11\n",
            "Episode 36: reward: -96.740, steps: 9\n",
            "Episode 37: reward: -92.943, steps: 15\n",
            "Episode 38: reward: -91.538, steps: 18\n",
            "Episode 39: reward: -95.364, steps: 11\n",
            "Episode 40: reward: -98.395, steps: 6\n",
            "Episode 41: reward: -96.068, steps: 10\n",
            "Episode 42: reward: -99.763, steps: 3\n",
            "Episode 43: reward: -94.669, steps: 12\n",
            "Episode 44: reward: -95.170, steps: 14\n",
            "Episode 45: reward: -95.728, steps: 10\n",
            "Episode 46: reward: -98.578, steps: 6\n",
            "Episode 47: reward: -96.868, steps: 10\n",
            "Episode 48: reward: -96.468, steps: 10\n",
            "Episode 49: reward: -99.647, steps: 2\n",
            "Episode 50: reward: -88.725, steps: 19\n",
            "Episode 51: reward: -95.596, steps: 12\n",
            "Episode 52: reward: -96.635, steps: 8\n",
            "Episode 53: reward: -99.647, steps: 2\n",
            "Episode 54: reward: -96.890, steps: 9\n",
            "Episode 55: reward: -96.348, steps: 10\n",
            "Episode 56: reward: -93.043, steps: 14\n",
            "Episode 57: reward: -93.826, steps: 15\n",
            "Episode 58: reward: -98.645, steps: 6\n",
            "Episode 59: reward: -93.820, steps: 14\n",
            "Episode 60: reward: -91.641, steps: 18\n",
            "Episode 61: reward: -92.275, steps: 18\n",
            "Episode 62: reward: -96.008, steps: 10\n",
            "Episode 63: reward: -96.438, steps: 10\n",
            "Episode 64: reward: -99.647, steps: 2\n",
            "Episode 65: reward: -96.458, steps: 10\n",
            "Episode 66: reward: -94.390, steps: 13\n",
            "Episode 67: reward: -94.477, steps: 14\n",
            "Episode 68: reward: -91.979, steps: 17\n",
            "Episode 69: reward: -93.907, steps: 14\n",
            "Episode 70: reward: -98.958, steps: 5\n",
            "Episode 71: reward: -95.918, steps: 10\n",
            "Episode 72: reward: -92.613, steps: 15\n",
            "Episode 73: reward: -98.578, steps: 6\n",
            "Episode 74: reward: -95.469, steps: 12\n",
            "Episode 75: reward: -99.647, steps: 2\n",
            "Episode 76: reward: -97.206, steps: 7\n",
            "Episode 77: reward: -93.666, steps: 15\n",
            "Episode 78: reward: -97.206, steps: 7\n",
            "Episode 79: reward: -95.925, steps: 11\n",
            "Episode 80: reward: -98.395, steps: 6\n",
            "Episode 81: reward: -96.118, steps: 10\n",
            "Episode 82: reward: -92.936, steps: 15\n",
            "Episode 83: reward: -93.750, steps: 15\n",
            "Episode 84: reward: -94.241, steps: 13\n",
            "Episode 85: reward: -94.844, steps: 13\n",
            "Episode 86: reward: -95.809, steps: 9\n",
            "Episode 87: reward: -97.937, steps: 8\n",
            "Episode 88: reward: -93.680, steps: 15\n",
            "Episode 89: reward: -97.356, steps: 7\n",
            "Episode 90: reward: -93.520, steps: 15\n",
            "Episode 91: reward: -98.341, steps: 5\n",
            "Episode 92: reward: -99.647, steps: 2\n",
            "Episode 93: reward: -96.856, steps: 7\n",
            "Episode 94: reward: -99.629, steps: 3\n",
            "Episode 95: reward: -99.647, steps: 2\n",
            "Episode 96: reward: -95.722, steps: 12\n",
            "Episode 97: reward: -94.190, steps: 14\n",
            "Episode 98: reward: -99.647, steps: 2\n",
            "Episode 99: reward: -94.987, steps: 11\n",
            "Episode 100: reward: -97.512, steps: 6\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 9 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0009000000000000002, 'batch_size': 32, 'target_model_update': 0.0005000000000000001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_8 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 44)                2156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,188\n",
            "Trainable params: 12,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -14.7980\n",
            "1518 episodes - episode_reward: -97.486 [-100.142, -93.305] - loss: 175.223 - mae: 63.120 - mean_q: -18.414\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 118s 12ms/step - reward: -11.3736\n",
            "1177 episodes - episode_reward: -96.630 [-99.974, -92.930] - loss: 36.792 - mae: 90.266 - mean_q: -59.750\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 122s 12ms/step - reward: -15.3051\n",
            "1566 episodes - episode_reward: -97.734 [-99.974, -92.906] - loss: 48.068 - mae: 93.978 - mean_q: -68.560\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -13.7545\n",
            "1409 episodes - episode_reward: -97.621 [-99.974, -91.983] - loss: 34.769 - mae: 94.064 - mean_q: -76.535\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -12.1659\n",
            "done, took 591.788 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -94.531, steps: 13\n",
            "Episode 2: reward: -96.271, steps: 9\n",
            "Episode 3: reward: -94.761, steps: 14\n",
            "Episode 4: reward: -97.512, steps: 6\n",
            "Episode 5: reward: -97.891, steps: 5\n",
            "Episode 6: reward: -95.610, steps: 9\n",
            "Episode 7: reward: -97.580, steps: 9\n",
            "Episode 8: reward: -95.753, steps: 9\n",
            "Episode 9: reward: -95.601, steps: 10\n",
            "Episode 10: reward: -98.091, steps: 5\n",
            "Episode 11: reward: -94.278, steps: 13\n",
            "Episode 12: reward: -96.587, steps: 9\n",
            "Episode 13: reward: -96.806, steps: 7\n",
            "Episode 14: reward: -95.170, steps: 11\n",
            "Episode 15: reward: -97.891, steps: 5\n",
            "Episode 16: reward: -95.791, steps: 10\n",
            "Episode 17: reward: -96.447, steps: 10\n",
            "Episode 18: reward: -95.550, steps: 11\n",
            "Episode 19: reward: -94.421, steps: 13\n",
            "Episode 20: reward: -97.941, steps: 5\n",
            "Episode 21: reward: -96.177, steps: 9\n",
            "Episode 22: reward: -96.460, steps: 9\n",
            "Episode 23: reward: -96.367, steps: 9\n",
            "Episode 24: reward: -99.647, steps: 2\n",
            "Episode 25: reward: -94.431, steps: 13\n",
            "Episode 26: reward: -94.345, steps: 13\n",
            "Episode 27: reward: -94.995, steps: 13\n",
            "Episode 28: reward: -99.058, steps: 5\n",
            "Episode 29: reward: -97.941, steps: 5\n",
            "Episode 30: reward: -94.278, steps: 13\n",
            "Episode 31: reward: -95.044, steps: 11\n",
            "Episode 32: reward: -96.197, steps: 9\n",
            "Episode 33: reward: -96.557, steps: 9\n",
            "Episode 34: reward: -95.045, steps: 13\n",
            "Episode 35: reward: -94.431, steps: 13\n",
            "Episode 36: reward: -96.143, steps: 9\n",
            "Episode 37: reward: -96.453, steps: 9\n",
            "Episode 38: reward: -95.307, steps: 9\n",
            "Episode 39: reward: -97.841, steps: 5\n",
            "Episode 40: reward: -96.450, steps: 9\n",
            "Episode 41: reward: -97.991, steps: 5\n",
            "Episode 42: reward: -95.000, steps: 11\n",
            "Episode 43: reward: -93.718, steps: 13\n",
            "Episode 44: reward: -94.790, steps: 11\n",
            "Episode 45: reward: -97.941, steps: 5\n",
            "Episode 46: reward: -96.063, steps: 9\n",
            "Episode 47: reward: -99.647, steps: 2\n",
            "Episode 48: reward: -96.430, steps: 9\n",
            "Episode 49: reward: -97.841, steps: 5\n",
            "Episode 50: reward: -93.811, steps: 14\n",
            "Episode 51: reward: -98.041, steps: 5\n",
            "Episode 52: reward: -95.988, steps: 10\n",
            "Episode 53: reward: -95.463, steps: 11\n",
            "Episode 54: reward: -96.430, steps: 9\n",
            "Episode 55: reward: -97.691, steps: 5\n",
            "Episode 56: reward: -96.650, steps: 9\n",
            "Episode 57: reward: -97.841, steps: 5\n",
            "Episode 58: reward: -98.141, steps: 5\n",
            "Episode 59: reward: -97.306, steps: 7\n",
            "Episode 60: reward: -97.941, steps: 5\n",
            "Episode 61: reward: -98.141, steps: 5\n",
            "Episode 62: reward: -96.460, steps: 9\n",
            "Episode 63: reward: -98.041, steps: 5\n",
            "Episode 64: reward: -96.523, steps: 9\n",
            "Episode 65: reward: -96.523, steps: 9\n",
            "Episode 66: reward: -97.991, steps: 5\n",
            "Episode 67: reward: -97.562, steps: 6\n",
            "Episode 68: reward: -99.295, steps: 3\n",
            "Episode 69: reward: -95.273, steps: 12\n",
            "Episode 70: reward: -99.974, steps: 1\n",
            "Episode 71: reward: -92.017, steps: 15\n",
            "Episode 72: reward: -95.218, steps: 10\n",
            "Episode 73: reward: -93.755, steps: 13\n",
            "Episode 74: reward: -94.772, steps: 13\n",
            "Episode 75: reward: -97.841, steps: 5\n",
            "Episode 76: reward: -97.991, steps: 5\n",
            "Episode 77: reward: -95.310, steps: 11\n",
            "Episode 78: reward: -97.298, steps: 9\n",
            "Episode 79: reward: -94.017, steps: 14\n",
            "Episode 80: reward: -94.185, steps: 13\n",
            "Episode 81: reward: -97.541, steps: 5\n",
            "Episode 82: reward: -97.256, steps: 7\n",
            "Episode 83: reward: -98.924, steps: 5\n",
            "Episode 84: reward: -98.041, steps: 5\n",
            "Episode 85: reward: -97.841, steps: 5\n",
            "Episode 86: reward: -99.974, steps: 1\n",
            "Episode 87: reward: -98.341, steps: 5\n",
            "Episode 88: reward: -97.691, steps: 5\n",
            "Episode 89: reward: -96.620, steps: 9\n",
            "Episode 90: reward: -95.943, steps: 9\n",
            "Episode 91: reward: -98.091, steps: 5\n",
            "Episode 92: reward: -95.601, steps: 10\n",
            "Episode 93: reward: -96.460, steps: 9\n",
            "Episode 94: reward: -95.720, steps: 11\n",
            "Episode 95: reward: -96.523, steps: 9\n",
            "Episode 96: reward: -98.091, steps: 5\n",
            "Episode 97: reward: -97.691, steps: 5\n",
            "Episode 98: reward: -94.980, steps: 11\n",
            "Episode 99: reward: -96.664, steps: 9\n",
            "Episode 100: reward: -95.186, steps: 12\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 10 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0001, 'batch_size': 32, 'target_model_update': 0.0009000000000000002, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_9 (Flatten)         (None, 208)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 16)                784       \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 44)                748       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,564\n",
            "Trainable params: 11,564\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -20.8744\n",
            "2123 episodes - episode_reward: -98.325 [-100.142, -94.184] - loss: 337.799 - mae: 58.333 - mean_q: -44.494\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            " 9633/10000 [===========================>..] - ETA: 4s - reward: -19.9667done, took 248.926 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -98.289, steps: 4\n",
            "Episode 2: reward: -98.091, steps: 5\n",
            "Episode 3: reward: -98.241, steps: 5\n",
            "Episode 4: reward: -98.243, steps: 4\n",
            "Episode 5: reward: -98.103, steps: 4\n",
            "Episode 6: reward: -98.191, steps: 5\n",
            "Episode 7: reward: -98.196, steps: 4\n",
            "Episode 8: reward: -98.196, steps: 4\n",
            "Episode 9: reward: -98.091, steps: 5\n",
            "Episode 10: reward: -98.149, steps: 4\n",
            "Episode 11: reward: -98.429, steps: 4\n",
            "Episode 12: reward: -98.429, steps: 4\n",
            "Episode 13: reward: -98.336, steps: 4\n",
            "Episode 14: reward: -98.391, steps: 5\n",
            "Episode 15: reward: -98.383, steps: 4\n",
            "Episode 16: reward: -98.429, steps: 4\n",
            "Episode 17: reward: -98.091, steps: 5\n",
            "Episode 18: reward: -98.429, steps: 4\n",
            "Episode 19: reward: -98.056, steps: 4\n",
            "Episode 20: reward: -98.476, steps: 4\n",
            "Episode 21: reward: -98.141, steps: 5\n",
            "Episode 22: reward: -98.141, steps: 5\n",
            "Episode 23: reward: -98.191, steps: 5\n",
            "Episode 24: reward: -98.291, steps: 5\n",
            "Episode 25: reward: -98.429, steps: 4\n",
            "Episode 26: reward: -98.091, steps: 5\n",
            "Episode 27: reward: -98.523, steps: 4\n",
            "Episode 28: reward: -98.141, steps: 5\n",
            "Episode 29: reward: -98.289, steps: 4\n",
            "Episode 30: reward: -98.149, steps: 4\n",
            "Episode 31: reward: -98.103, steps: 4\n",
            "Episode 32: reward: -98.289, steps: 4\n",
            "Episode 33: reward: -98.141, steps: 5\n",
            "Episode 34: reward: -98.141, steps: 5\n",
            "Episode 35: reward: -98.391, steps: 5\n",
            "Episode 36: reward: -98.429, steps: 4\n",
            "Episode 37: reward: -98.241, steps: 5\n",
            "Episode 38: reward: -98.041, steps: 5\n",
            "Episode 39: reward: -98.336, steps: 4\n",
            "Episode 40: reward: -98.336, steps: 4\n",
            "Episode 41: reward: -98.616, steps: 4\n",
            "Episode 42: reward: -98.196, steps: 4\n",
            "Episode 43: reward: -98.149, steps: 4\n",
            "Episode 44: reward: -97.991, steps: 5\n",
            "Episode 45: reward: -98.141, steps: 5\n",
            "Episode 46: reward: -98.476, steps: 4\n",
            "Episode 47: reward: -98.391, steps: 5\n",
            "Episode 48: reward: -98.196, steps: 4\n",
            "Episode 49: reward: -98.429, steps: 4\n",
            "Episode 50: reward: -98.429, steps: 4\n",
            "Episode 51: reward: -98.149, steps: 4\n",
            "Episode 52: reward: -98.191, steps: 5\n",
            "Episode 53: reward: -98.191, steps: 5\n",
            "Episode 54: reward: -98.476, steps: 4\n",
            "Episode 55: reward: -98.429, steps: 4\n",
            "Episode 56: reward: -98.336, steps: 4\n",
            "Episode 57: reward: -98.491, steps: 5\n",
            "Episode 58: reward: -98.391, steps: 5\n",
            "Episode 59: reward: -98.141, steps: 5\n",
            "Episode 60: reward: -98.383, steps: 4\n",
            "Episode 61: reward: -98.241, steps: 5\n",
            "Episode 62: reward: -98.336, steps: 4\n",
            "Episode 63: reward: -98.429, steps: 4\n",
            "Episode 64: reward: -98.336, steps: 4\n",
            "Episode 65: reward: -98.241, steps: 5\n",
            "Episode 66: reward: -98.441, steps: 5\n",
            "Episode 67: reward: -98.091, steps: 5\n",
            "Episode 68: reward: -98.383, steps: 4\n",
            "Episode 69: reward: -98.291, steps: 5\n",
            "Episode 70: reward: -98.291, steps: 5\n",
            "Episode 71: reward: -98.383, steps: 4\n",
            "Episode 72: reward: -98.336, steps: 4\n",
            "Episode 73: reward: -98.476, steps: 4\n",
            "Episode 74: reward: -98.383, steps: 4\n",
            "Episode 75: reward: -98.523, steps: 4\n",
            "Episode 76: reward: -98.243, steps: 4\n",
            "Episode 77: reward: -98.103, steps: 4\n",
            "Episode 78: reward: -98.289, steps: 4\n",
            "Episode 79: reward: -98.191, steps: 5\n",
            "Episode 80: reward: -98.141, steps: 5\n",
            "Episode 81: reward: -98.041, steps: 5\n",
            "Episode 82: reward: -98.091, steps: 5\n",
            "Episode 83: reward: -98.243, steps: 4\n",
            "Episode 84: reward: -98.476, steps: 4\n",
            "Episode 85: reward: -98.383, steps: 4\n",
            "Episode 86: reward: -98.429, steps: 4\n",
            "Episode 87: reward: -98.289, steps: 4\n",
            "Episode 88: reward: -98.243, steps: 4\n",
            "Episode 89: reward: -98.289, steps: 4\n",
            "Episode 90: reward: -98.391, steps: 5\n",
            "Episode 91: reward: -98.289, steps: 4\n",
            "Episode 92: reward: -98.289, steps: 4\n",
            "Episode 93: reward: -98.383, steps: 4\n",
            "Episode 94: reward: -98.191, steps: 5\n",
            "Episode 95: reward: -98.241, steps: 5\n",
            "Episode 96: reward: -98.336, steps: 4\n",
            "Episode 97: reward: -98.291, steps: 5\n",
            "Episode 98: reward: -98.341, steps: 5\n",
            "Episode 99: reward: -98.476, steps: 4\n",
            "Episode 100: reward: -98.336, steps: 4\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 11 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0009000000000000002, 'batch_size': 32, 'target_model_update': 0.0001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_10 (Flatten)        (None, 208)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 16)                3344      \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 44)                748       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,364\n",
            "Trainable params: 4,364\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 123s 12ms/step - reward: -15.2431\n",
            "1562 episodes - episode_reward: -97.590 [-100.129, -93.635] - loss: 260.580 - mae: 48.290 - mean_q: -0.790\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 128s 13ms/step - reward: -10.7290\n",
            "1114 episodes - episode_reward: -96.308 [-99.974, -92.110] - loss: 208.020 - mae: 62.064 - mean_q: -3.454\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 124s 12ms/step - reward: -9.3129\n",
            "974 episodes - episode_reward: -95.614 [-98.441, -91.723] - loss: 204.070 - mae: 67.130 - mean_q: -13.530\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 124s 12ms/step - reward: -8.8176\n",
            "925 episodes - episode_reward: -95.327 [-99.295, -91.310] - loss: 152.269 - mae: 72.087 - mean_q: -23.247\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 126s 13ms/step - reward: -9.8310\n",
            "done, took 624.552 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -99.295, steps: 3\n",
            "Episode 2: reward: -94.322, steps: 13\n",
            "Episode 3: reward: -96.934, steps: 9\n",
            "Episode 4: reward: -94.462, steps: 13\n",
            "Episode 5: reward: -94.331, steps: 14\n",
            "Episode 6: reward: -97.456, steps: 7\n",
            "Episode 7: reward: -95.490, steps: 11\n",
            "Episode 8: reward: -97.406, steps: 7\n",
            "Episode 9: reward: -97.156, steps: 7\n",
            "Episode 10: reward: -93.815, steps: 13\n",
            "Episode 11: reward: -97.356, steps: 7\n",
            "Episode 12: reward: -95.863, steps: 10\n",
            "Episode 13: reward: -97.206, steps: 7\n",
            "Episode 14: reward: -94.435, steps: 13\n",
            "Episode 15: reward: -97.006, steps: 7\n",
            "Episode 16: reward: -99.295, steps: 3\n",
            "Episode 17: reward: -99.295, steps: 3\n",
            "Episode 18: reward: -96.103, steps: 10\n",
            "Episode 19: reward: -94.515, steps: 13\n",
            "Episode 20: reward: -97.006, steps: 7\n",
            "Episode 21: reward: -94.435, steps: 13\n",
            "Episode 22: reward: -94.795, steps: 13\n",
            "Episode 23: reward: -96.906, steps: 7\n",
            "Episode 24: reward: -94.172, steps: 13\n",
            "Episode 25: reward: -97.106, steps: 7\n",
            "Episode 26: reward: -94.662, steps: 13\n",
            "Episode 27: reward: -94.165, steps: 13\n",
            "Episode 28: reward: -95.923, steps: 10\n",
            "Episode 29: reward: -97.106, steps: 7\n",
            "Episode 30: reward: -94.075, steps: 14\n",
            "Episode 31: reward: -97.256, steps: 7\n",
            "Episode 32: reward: -96.250, steps: 10\n",
            "Episode 33: reward: -94.833, steps: 12\n",
            "Episode 34: reward: -96.186, steps: 10\n",
            "Episode 35: reward: -97.356, steps: 7\n",
            "Episode 36: reward: -95.146, steps: 12\n",
            "Episode 37: reward: -96.906, steps: 7\n",
            "Episode 38: reward: -97.156, steps: 7\n",
            "Episode 39: reward: -97.306, steps: 7\n",
            "Episode 40: reward: -97.056, steps: 7\n",
            "Episode 41: reward: -96.163, steps: 10\n",
            "Episode 42: reward: -97.106, steps: 7\n",
            "Episode 43: reward: -97.106, steps: 7\n",
            "Episode 44: reward: -97.156, steps: 7\n",
            "Episode 45: reward: -96.424, steps: 9\n",
            "Episode 46: reward: -94.071, steps: 14\n",
            "Episode 47: reward: -94.235, steps: 13\n",
            "Episode 48: reward: -97.306, steps: 7\n",
            "Episode 49: reward: -97.006, steps: 7\n",
            "Episode 50: reward: -96.856, steps: 7\n",
            "Episode 51: reward: -96.311, steps: 9\n",
            "Episode 52: reward: -94.435, steps: 13\n",
            "Episode 53: reward: -96.956, steps: 7\n",
            "Episode 54: reward: -94.392, steps: 13\n",
            "Episode 55: reward: -93.562, steps: 14\n",
            "Episode 56: reward: -94.209, steps: 14\n",
            "Episode 57: reward: -95.364, steps: 11\n",
            "Episode 58: reward: -93.688, steps: 14\n",
            "Episode 59: reward: -94.225, steps: 13\n",
            "Episode 60: reward: -96.103, steps: 10\n",
            "Episode 61: reward: -96.906, steps: 7\n",
            "Episode 62: reward: -95.205, steps: 13\n",
            "Episode 63: reward: -97.056, steps: 7\n",
            "Episode 64: reward: -93.842, steps: 14\n",
            "Episode 65: reward: -93.782, steps: 14\n",
            "Episode 66: reward: -95.863, steps: 10\n",
            "Episode 67: reward: -94.645, steps: 13\n",
            "Episode 68: reward: -96.010, steps: 10\n",
            "Episode 69: reward: -93.356, steps: 15\n",
            "Episode 70: reward: -96.956, steps: 7\n",
            "Episode 71: reward: -94.435, steps: 13\n",
            "Episode 72: reward: -94.725, steps: 13\n",
            "Episode 73: reward: -96.764, steps: 9\n",
            "Episode 74: reward: -93.769, steps: 14\n",
            "Episode 75: reward: -93.619, steps: 13\n",
            "Episode 76: reward: -94.592, steps: 13\n",
            "Episode 77: reward: -94.715, steps: 13\n",
            "Episode 78: reward: -97.656, steps: 7\n",
            "Episode 79: reward: -94.865, steps: 13\n",
            "Episode 80: reward: -97.256, steps: 7\n",
            "Episode 81: reward: -93.209, steps: 13\n",
            "Episode 82: reward: -97.156, steps: 7\n",
            "Episode 83: reward: -95.629, steps: 10\n",
            "Episode 84: reward: -99.295, steps: 3\n",
            "Episode 85: reward: -95.490, steps: 11\n",
            "Episode 86: reward: -93.885, steps: 13\n",
            "Episode 87: reward: -99.295, steps: 3\n",
            "Episode 88: reward: -94.645, steps: 13\n",
            "Episode 89: reward: -92.102, steps: 14\n",
            "Episode 90: reward: -99.295, steps: 3\n",
            "Episode 91: reward: -94.655, steps: 13\n",
            "Episode 92: reward: -97.156, steps: 7\n",
            "Episode 93: reward: -97.006, steps: 7\n",
            "Episode 94: reward: -94.505, steps: 13\n",
            "Episode 95: reward: -93.985, steps: 13\n",
            "Episode 96: reward: -97.456, steps: 7\n",
            "Episode 97: reward: -91.327, steps: 18\n",
            "Episode 98: reward: -94.532, steps: 13\n",
            "Episode 99: reward: -99.295, steps: 3\n",
            "Episode 100: reward: -97.156, steps: 7\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 12 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005000000000000001, 'batch_size': 32, 'target_model_update': 0.0007000000000000001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_11 (Flatten)        (None, 208)               0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 44)                2156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,188\n",
            "Trainable params: 12,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 119s 12ms/step - reward: -16.6713\n",
            "1704 episodes - episode_reward: -97.837 [-100.142, -93.321] - loss: 199.361 - mae: 56.664 - mean_q: -33.700\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 127s 13ms/step - reward: -13.4857\n",
            "1388 episodes - episode_reward: -97.158 [-100.142, -91.489] - loss: 26.745 - mae: 85.974 - mean_q: -72.604\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 128s 13ms/step - reward: -12.6044\n",
            "1300 episodes - episode_reward: -96.957 [-99.974, -92.078] - loss: 10.821 - mae: 94.372 - mean_q: -85.142\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 128s 13ms/step - reward: -12.5758\n",
            "1297 episodes - episode_reward: -96.963 [-99.974, -92.080] - loss: 6.757 - mae: 94.699 - mean_q: -87.318\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 128s 13ms/step - reward: -11.5148\n",
            "done, took 628.829 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -98.025, steps: 5\n",
            "Episode 2: reward: -96.380, steps: 9\n",
            "Episode 3: reward: -92.837, steps: 14\n",
            "Episode 4: reward: -94.341, steps: 14\n",
            "Episode 5: reward: -95.962, steps: 10\n",
            "Episode 6: reward: -93.831, steps: 14\n",
            "Episode 7: reward: -97.306, steps: 7\n",
            "Episode 8: reward: -94.052, steps: 12\n",
            "Episode 9: reward: -95.242, steps: 10\n",
            "Episode 10: reward: -96.416, steps: 10\n",
            "Episode 11: reward: -95.858, steps: 10\n",
            "Episode 12: reward: -96.922, steps: 8\n",
            "Episode 13: reward: -95.554, steps: 11\n",
            "Episode 14: reward: -96.396, steps: 9\n",
            "Episode 15: reward: -95.741, steps: 8\n",
            "Episode 16: reward: -94.872, steps: 12\n",
            "Episode 17: reward: -94.827, steps: 13\n",
            "Episode 18: reward: -97.412, steps: 6\n",
            "Episode 19: reward: -96.215, steps: 10\n",
            "Episode 20: reward: -96.403, steps: 10\n",
            "Episode 21: reward: -96.752, steps: 8\n",
            "Episode 22: reward: -96.113, steps: 9\n",
            "Episode 23: reward: -97.641, steps: 5\n",
            "Episode 24: reward: -97.549, steps: 6\n",
            "Episode 25: reward: -96.176, steps: 10\n",
            "Episode 26: reward: -95.996, steps: 10\n",
            "Episode 27: reward: -96.236, steps: 10\n",
            "Episode 28: reward: -95.624, steps: 11\n",
            "Episode 29: reward: -96.338, steps: 9\n",
            "Episode 30: reward: -97.923, steps: 6\n",
            "Episode 31: reward: -93.188, steps: 14\n",
            "Episode 32: reward: -96.702, steps: 8\n",
            "Episode 33: reward: -94.268, steps: 14\n",
            "Episode 34: reward: -93.967, steps: 14\n",
            "Episode 35: reward: -93.208, steps: 14\n",
            "Episode 36: reward: -97.162, steps: 6\n",
            "Episode 37: reward: -93.934, steps: 14\n",
            "Episode 38: reward: -98.016, steps: 6\n",
            "Episode 39: reward: -96.103, steps: 10\n",
            "Episode 40: reward: -95.182, steps: 12\n",
            "Episode 41: reward: -94.674, steps: 13\n",
            "Episode 42: reward: -95.829, steps: 10\n",
            "Episode 43: reward: -93.787, steps: 14\n",
            "Episode 44: reward: -97.006, steps: 7\n",
            "Episode 45: reward: -94.517, steps: 13\n",
            "Episode 46: reward: -94.644, steps: 13\n",
            "Episode 47: reward: -95.936, steps: 10\n",
            "Episode 48: reward: -99.702, steps: 3\n",
            "Episode 49: reward: -97.662, steps: 6\n",
            "Episode 50: reward: -94.197, steps: 13\n",
            "Episode 51: reward: -97.156, steps: 7\n",
            "Episode 52: reward: -93.500, steps: 14\n",
            "Episode 53: reward: -97.979, steps: 5\n",
            "Episode 54: reward: -94.841, steps: 13\n",
            "Episode 55: reward: -95.835, steps: 10\n",
            "Episode 56: reward: -94.504, steps: 13\n",
            "Episode 57: reward: -97.783, steps: 6\n",
            "Episode 58: reward: -95.657, steps: 11\n",
            "Episode 59: reward: -96.283, steps: 9\n",
            "Episode 60: reward: -97.885, steps: 5\n",
            "Episode 61: reward: -97.591, steps: 5\n",
            "Episode 62: reward: -97.783, steps: 6\n",
            "Episode 63: reward: -97.689, steps: 6\n",
            "Episode 64: reward: -97.829, steps: 6\n",
            "Episode 65: reward: -93.974, steps: 14\n",
            "Episode 66: reward: -96.385, steps: 9\n",
            "Episode 67: reward: -96.776, steps: 9\n",
            "Episode 68: reward: -95.961, steps: 10\n",
            "Episode 69: reward: -95.162, steps: 10\n",
            "Episode 70: reward: -94.906, steps: 12\n",
            "Episode 71: reward: -97.156, steps: 9\n",
            "Episode 72: reward: -97.006, steps: 7\n",
            "Episode 73: reward: -96.533, steps: 9\n",
            "Episode 74: reward: -95.771, steps: 10\n",
            "Episode 75: reward: -97.736, steps: 6\n",
            "Episode 76: reward: -96.869, steps: 9\n",
            "Episode 77: reward: -94.316, steps: 12\n",
            "Episode 78: reward: -96.956, steps: 7\n",
            "Episode 79: reward: -99.647, steps: 2\n",
            "Episode 80: reward: -94.389, steps: 13\n",
            "Episode 81: reward: -95.668, steps: 10\n",
            "Episode 82: reward: -95.413, steps: 13\n",
            "Episode 83: reward: -95.668, steps: 10\n",
            "Episode 84: reward: -95.404, steps: 11\n",
            "Episode 85: reward: -97.412, steps: 6\n",
            "Episode 86: reward: -95.835, steps: 10\n",
            "Episode 87: reward: -95.128, steps: 10\n",
            "Episode 88: reward: -93.362, steps: 14\n",
            "Episode 89: reward: -100.035, steps: 2\n",
            "Episode 90: reward: -97.562, steps: 6\n",
            "Episode 91: reward: -97.503, steps: 6\n",
            "Episode 92: reward: -96.202, steps: 10\n",
            "Episode 93: reward: -95.594, steps: 11\n",
            "Episode 94: reward: -99.456, steps: 4\n",
            "Episode 95: reward: -94.666, steps: 13\n",
            "Episode 96: reward: -99.343, steps: 4\n",
            "Episode 97: reward: -96.050, steps: 9\n",
            "Episode 98: reward: -96.835, steps: 8\n",
            "Episode 99: reward: -94.561, steps: 14\n",
            "Episode 100: reward: -97.689, steps: 6\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 13 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0001, 'batch_size': 32, 'target_model_update': 0.0009000000000000002, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_12 (Flatten)        (None, 208)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 48)                10032     \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 44)                1452      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,052\n",
            "Trainable params: 13,052\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 130s 13ms/step - reward: -22.6784\n",
            "2303 episodes - episode_reward: -98.474 [-100.142, -96.031] - loss: 350.997 - mae: 60.027 - mean_q: -46.753\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 139s 14ms/step - reward: -19.6944\n",
            "2006 episodes - episode_reward: -98.178 [-99.974, -94.140] - loss: 25.153 - mae: 93.891 - mean_q: -89.383\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -17.4355\n",
            "1780 episodes - episode_reward: -97.952 [-99.974, -94.962] - loss: 14.004 - mae: 94.600 - mean_q: -90.806\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 140s 14ms/step - reward: -17.8957\n",
            "1823 episodes - episode_reward: -98.166 [-100.129, -94.170] - loss: 10.037 - mae: 93.920 - mean_q: -90.293\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -15.2709\n",
            "done, took 684.109 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.956, steps: 7\n",
            "Episode 2: reward: -99.647, steps: 2\n",
            "Episode 3: reward: -97.006, steps: 7\n",
            "Episode 4: reward: -97.356, steps: 7\n",
            "Episode 5: reward: -96.906, steps: 7\n",
            "Episode 6: reward: -97.406, steps: 7\n",
            "Episode 7: reward: -99.295, steps: 3\n",
            "Episode 8: reward: -97.206, steps: 7\n",
            "Episode 9: reward: -97.969, steps: 6\n",
            "Episode 10: reward: -97.206, steps: 7\n",
            "Episode 11: reward: -99.295, steps: 3\n",
            "Episode 12: reward: -97.156, steps: 7\n",
            "Episode 13: reward: -97.056, steps: 7\n",
            "Episode 14: reward: -97.106, steps: 7\n",
            "Episode 15: reward: -97.206, steps: 7\n",
            "Episode 16: reward: -96.308, steps: 9\n",
            "Episode 17: reward: -97.306, steps: 7\n",
            "Episode 18: reward: -97.256, steps: 7\n",
            "Episode 19: reward: -99.295, steps: 3\n",
            "Episode 20: reward: -97.406, steps: 7\n",
            "Episode 21: reward: -97.056, steps: 7\n",
            "Episode 22: reward: -97.156, steps: 7\n",
            "Episode 23: reward: -97.406, steps: 7\n",
            "Episode 24: reward: -97.006, steps: 7\n",
            "Episode 25: reward: -97.056, steps: 7\n",
            "Episode 26: reward: -97.656, steps: 7\n",
            "Episode 27: reward: -97.206, steps: 7\n",
            "Episode 28: reward: -97.206, steps: 7\n",
            "Episode 29: reward: -97.206, steps: 7\n",
            "Episode 30: reward: -96.956, steps: 7\n",
            "Episode 31: reward: -96.408, steps: 9\n",
            "Episode 32: reward: -97.406, steps: 7\n",
            "Episode 33: reward: -97.156, steps: 7\n",
            "Episode 34: reward: -99.295, steps: 3\n",
            "Episode 35: reward: -97.056, steps: 7\n",
            "Episode 36: reward: -97.156, steps: 7\n",
            "Episode 37: reward: -97.156, steps: 7\n",
            "Episode 38: reward: -96.906, steps: 7\n",
            "Episode 39: reward: -97.256, steps: 7\n",
            "Episode 40: reward: -97.783, steps: 6\n",
            "Episode 41: reward: -97.206, steps: 7\n",
            "Episode 42: reward: -97.056, steps: 7\n",
            "Episode 43: reward: -97.106, steps: 7\n",
            "Episode 44: reward: -97.006, steps: 7\n",
            "Episode 45: reward: -97.256, steps: 7\n",
            "Episode 46: reward: -97.206, steps: 7\n",
            "Episode 47: reward: -97.456, steps: 7\n",
            "Episode 48: reward: -97.056, steps: 7\n",
            "Episode 49: reward: -99.295, steps: 3\n",
            "Episode 50: reward: -97.262, steps: 6\n",
            "Episode 51: reward: -97.006, steps: 7\n",
            "Episode 52: reward: -97.056, steps: 7\n",
            "Episode 53: reward: -97.556, steps: 7\n",
            "Episode 54: reward: -97.212, steps: 6\n",
            "Episode 55: reward: -97.406, steps: 7\n",
            "Episode 56: reward: -97.256, steps: 7\n",
            "Episode 57: reward: -97.316, steps: 6\n",
            "Episode 58: reward: -96.906, steps: 7\n",
            "Episode 59: reward: -97.056, steps: 7\n",
            "Episode 60: reward: -97.356, steps: 7\n",
            "Episode 61: reward: -97.006, steps: 7\n",
            "Episode 62: reward: -97.312, steps: 6\n",
            "Episode 63: reward: -97.456, steps: 7\n",
            "Episode 64: reward: -97.206, steps: 7\n",
            "Episode 65: reward: -96.956, steps: 7\n",
            "Episode 66: reward: -97.456, steps: 7\n",
            "Episode 67: reward: -97.306, steps: 7\n",
            "Episode 68: reward: -97.356, steps: 7\n",
            "Episode 69: reward: -97.312, steps: 6\n",
            "Episode 70: reward: -99.295, steps: 3\n",
            "Episode 71: reward: -97.106, steps: 7\n",
            "Episode 72: reward: -97.356, steps: 7\n",
            "Episode 73: reward: -96.856, steps: 7\n",
            "Episode 74: reward: -97.056, steps: 7\n",
            "Episode 75: reward: -97.406, steps: 7\n",
            "Episode 76: reward: -97.256, steps: 7\n",
            "Episode 77: reward: -97.156, steps: 7\n",
            "Episode 78: reward: -99.647, steps: 2\n",
            "Episode 79: reward: -97.256, steps: 7\n",
            "Episode 80: reward: -99.647, steps: 2\n",
            "Episode 81: reward: -97.306, steps: 7\n",
            "Episode 82: reward: -97.106, steps: 7\n",
            "Episode 83: reward: -99.295, steps: 3\n",
            "Episode 84: reward: -95.630, steps: 11\n",
            "Episode 85: reward: -99.295, steps: 3\n",
            "Episode 86: reward: -97.006, steps: 7\n",
            "Episode 87: reward: -97.256, steps: 7\n",
            "Episode 88: reward: -97.006, steps: 7\n",
            "Episode 89: reward: -97.456, steps: 7\n",
            "Episode 90: reward: -97.306, steps: 7\n",
            "Episode 91: reward: -97.456, steps: 7\n",
            "Episode 92: reward: -99.295, steps: 3\n",
            "Episode 93: reward: -97.106, steps: 7\n",
            "Episode 94: reward: -97.056, steps: 7\n",
            "Episode 95: reward: -96.906, steps: 7\n",
            "Episode 96: reward: -97.256, steps: 7\n",
            "Episode 97: reward: -96.756, steps: 7\n",
            "Episode 98: reward: -96.956, steps: 7\n",
            "Episode 99: reward: -97.106, steps: 7\n",
            "Episode 100: reward: -99.295, steps: 3\n",
            "\n",
            "#################\n",
            "Created 300 combinations to test.\n",
            "Test 14 from 300\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005000000000000001, 'batch_size': 32, 'target_model_update': 0.0001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_13 (Flatten)        (None, 208)               0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 16)                3344      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 44)                748       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,092\n",
            "Trainable params: 4,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 50000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 124s 12ms/step - reward: -17.4632\n",
            "1782 episodes - episode_reward: -97.998 [-100.142, -94.482] - loss: 337.086 - mae: 51.190 - mean_q: -7.933\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 133s 13ms/step - reward: -14.3035\n",
            "1469 episodes - episode_reward: -97.370 [-98.943, -95.015] - loss: 114.245 - mae: 61.843 - mean_q: -16.247\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 131s 13ms/step - reward: -13.6007\n",
            "1398 episodes - episode_reward: -97.285 [-98.896, -93.661] - loss: 88.260 - mae: 65.357 - mean_q: -28.672\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            " 3995/10000 [==========>...................] - ETA: 1:18 - reward: -11.7956done, took 440.155 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -94.666, steps: 13\n",
            "Episode 2: reward: -96.734, steps: 9\n",
            "Episode 3: reward: -96.202, steps: 10\n",
            "Episode 4: reward: -97.301, steps: 7\n",
            "Episode 5: reward: -96.374, steps: 9\n",
            "Episode 6: reward: -97.551, steps: 7\n",
            "Episode 7: reward: -97.456, steps: 6\n",
            "Episode 8: reward: -95.652, steps: 10\n",
            "Episode 9: reward: -96.074, steps: 9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4c10fd279c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Git/Kniffel/ai/ai.py\u001b[0m in \u001b[0;36mgrid_search_test\u001b[0;34m(self, nb_steps, env_config)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             csv = self.train(\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mhyperparameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             )\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Git/Kniffel/ai/ai.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, hyperparameter, nb_steps, load_path, env_config)\u001b[0m\n\u001b[1;32m    265\u001b[0m         )\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         csv = self.get_configuration(\n",
            "\u001b[0;32m/content/drive/MyDrive/Git/Kniffel/ai/ai.py\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(self, agent, env)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m# print(np.mean(scores.history[\"episode_reward\"]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Select an action.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/agents/dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4275\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 4276\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   4277\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4278\u001b[0m     output_structure = tf.nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ai.grid_search_test(nb_steps=50_000, env_config=env_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRxDEGqhfOP5"
      },
      "outputs": [],
      "source": [
        "hyperparameter = {\n",
        "    \"windows_length\": 1,\n",
        "    \"adam_learning_rate\": 0.0005000000000000001,\n",
        "    \"batch_size\": 35,\n",
        "    \"target_model_update\": 0.00030000000000000003,\n",
        "    \"dueling_option\": \"avg\",\n",
        "    \"activation\": \"linear\",\n",
        "    \"layers\": 1,\n",
        "    \"unit_1\": 64,\n",
        "}\n",
        "\n",
        "#ai.train(\n",
        "#    hyperparameter=hyperparameter,\n",
        "#    nb_steps=1_000_000,\n",
        "#    env_config=env_config\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-FMgpdHFMog"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "ai-model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "7724bf7a85847efc5bb6a1829c232f0862f78efa3c8535cdc9aa81401d00c18c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}