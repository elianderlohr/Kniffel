{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elianderlohr/Kniffel/blob/feature%2Fnew-ai/ai/ai-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZbB2tTectNx",
        "outputId": "4cf9504d-9ff8-49ba-b126-65eda2dff677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Git\n",
            "Sat Jun  4 06:55:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd '/content/drive/MyDrive/Git'\n",
        "\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n",
        "except ImportError as e:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ezGGzkUkfi",
        "outputId": "61ad1916-9132-4e36-96bc-8a910f37cb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl\n",
            "  Downloading keras-rl-0.4.2.tar.gz (40 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl) (2.8.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-py3-none-any.whl size=48378 sha256=5accedfbdf0a0dd634c6c6f9c7ab2c5ce844013e0e5bd08c677c0594154b53d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/23/e9/278c2e59c322236e2bfdf7c792c16f0b4dec24816d27a3f1e4\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 774 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.26.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lop1FAXFctN0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'Kniffel')\n",
        "sys.path.insert(0, 'Kniffel/ai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3A2YD8QYctN0"
      },
      "outputs": [],
      "source": [
        "from ai.hyperparameter import Hyperparameter\n",
        "from ai.ai import KniffelAI\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ILWBfrctN1",
        "outputId": "1d81af2d-5c1f-4704-ba0e-412b47cabf4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 2400 combinations to test.\n"
          ]
        }
      ],
      "source": [
        "units = list(range(16, 64, 16))\n",
        "\n",
        "base_hp = {\n",
        "    \"windows_length\": [1],\n",
        "    \"adam_learning_rate\": [0.0001, 0.0005, 0.001, 0.00146, 0.005, 0.01],\n",
        "    \"adam_epsilon\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
        "    \"batch_size\": [32],\n",
        "    \"target_model_update\": [0.0001, 0.0005, 0.001, 0.00146, 0.005, 0.01],\n",
        "    \"dueling_option\": [\"avg\"],\n",
        "    \"activation\": [\"linear\"],\n",
        "    \"layers\": [1, 2],\n",
        "    \"unit_1\": units,\n",
        "    \"unit_2\": units,\n",
        "}\n",
        "\n",
        "ai = KniffelAI(\n",
        "    save=True, load=False, predefined_layers=True, hyperparater_base=base_hp, path_prefix=\"/content/drive/MyDrive/Git/Kniffel/\"\n",
        ")\n",
        "\n",
        "env_config = {\n",
        "        \"reward_step\": 0,\n",
        "        \"reward_round\": 0.5,\n",
        "        \"reward_roll_dice\": 0.25,\n",
        "        \"reward_game_over\": -100,\n",
        "        \"reward_bonus\": 2,\n",
        "        \"reward_finish\": 10,\n",
        "        \"reward_zero_dice\": -0.5,\n",
        "        \"reward_one_dice\": -0.2,\n",
        "        \"reward_two_dice\": -0.1,\n",
        "        \"reward_three_dice\": 0.5,\n",
        "        \"reward_four_dice\": 0.6,\n",
        "        \"reward_five_dice\": 0.8,\n",
        "        \"reward_six_dice\": 1,\n",
        "        \"reward_kniffel\": 1.5,\n",
        "        \"reward_small_street\": 1,\n",
        "        \"reward_large_street\": 1.1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vJR2jjqxkEvt",
        "outputId": "da925aab-2dd8-4285-b469-707a2f5381e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 41)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "break_counter: 1000\n",
            "AVG. 17.771496625645096\n",
            "MAX: 56\n",
            "MIN: 0\n",
            "AVG rounds: 6.693316084519189\n",
            "Max rounds: 21\n",
            "Min rounds: 1\n"
          ]
        }
      ],
      "source": [
        "ai.play(path=\"Kniffel/weights/p_date=2022-06-04-06_57_09\", episodes=1000, env_config=env_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dhYYUUbEctN2"
      },
      "outputs": [],
      "source": [
        "# ai.grid_search_test(nb_steps=30_000, env_config=env_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DRxDEGqhfOP5",
        "outputId": "d8a32bbb-3559-4881-9e63-df271db23f7d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 41)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 2000000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 115s 11ms/step - reward: -22.7495\n",
            "2308 episodes - episode_reward: -98.568 [-100.142, -91.900] - loss: 101.474 - mae: 81.974 - mean_q: -72.572\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 120s 12ms/step - reward: -14.4569\n",
            "1485 episodes - episode_reward: -97.354 [-100.022, -92.207] - loss: 10.960 - mae: 88.752 - mean_q: -84.068\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 119s 12ms/step - reward: -13.3460\n",
            "1374 episodes - episode_reward: -97.131 [-100.135, -91.170] - loss: 9.420 - mae: 88.506 - mean_q: -83.999\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -12.9473\n",
            "1334 episodes - episode_reward: -97.058 [-100.142, -92.703] - loss: 8.536 - mae: 88.561 - mean_q: -84.185\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -12.4414\n",
            "1284 episodes - episode_reward: -96.893 [-100.142, -91.438] - loss: 8.581 - mae: 88.240 - mean_q: -83.768\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 124s 12ms/step - reward: -12.3895\n",
            "1279 episodes - episode_reward: -96.870 [-100.142, -89.761] - loss: 8.549 - mae: 87.909 - mean_q: -83.363\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 130s 13ms/step - reward: -12.0999\n",
            "1250 episodes - episode_reward: -96.801 [-100.142, -90.331] - loss: 7.921 - mae: 88.103 - mean_q: -83.745\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 135s 13ms/step - reward: -12.1086\n",
            "1251 episodes - episode_reward: -96.790 [-100.135, -90.325] - loss: 7.837 - mae: 87.989 - mean_q: -83.655\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -11.7902\n",
            "1219 episodes - episode_reward: -96.720 [-100.135, -90.864] - loss: 7.409 - mae: 88.002 - mean_q: -83.805\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 139s 14ms/step - reward: -11.4379\n",
            "1184 episodes - episode_reward: -96.602 [-100.109, -89.433] - loss: 7.323 - mae: 88.005 - mean_q: -83.906\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 143s 14ms/step - reward: -11.5777\n",
            "1198 episodes - episode_reward: -96.644 [-100.142, -91.095] - loss: 7.352 - mae: 87.939 - mean_q: -83.789\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 145s 15ms/step - reward: -11.5805\n",
            "1198 episodes - episode_reward: -96.663 [-100.109, -90.354] - loss: 6.944 - mae: 88.105 - mean_q: -84.047\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 148s 15ms/step - reward: -11.6436\n",
            "1204 episodes - episode_reward: -96.710 [-100.129, -90.748] - loss: 6.775 - mae: 88.103 - mean_q: -84.089\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 147s 15ms/step - reward: -11.1088\n",
            "1151 episodes - episode_reward: -96.520 [-100.109, -90.704] - loss: 6.397 - mae: 88.266 - mean_q: -84.341\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 154s 15ms/step - reward: -11.1807\n",
            "1158 episodes - episode_reward: -96.544 [-100.142, -90.015] - loss: 6.481 - mae: 88.102 - mean_q: -84.076\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 155s 16ms/step - reward: -11.1891\n",
            "1159 episodes - episode_reward: -96.540 [-100.142, -89.751] - loss: 6.165 - mae: 88.293 - mean_q: -84.482\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 158s 16ms/step - reward: -11.2197\n",
            "1162 episodes - episode_reward: -96.558 [-100.135, -90.178] - loss: 6.037 - mae: 88.343 - mean_q: -84.558\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 157s 16ms/step - reward: -11.1001\n",
            "1150 episodes - episode_reward: -96.521 [-100.135, -90.588] - loss: 6.202 - mae: 88.146 - mean_q: -84.211\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 154s 15ms/step - reward: -10.8494\n",
            "1125 episodes - episode_reward: -96.441 [-100.142, -90.354] - loss: 6.029 - mae: 88.191 - mean_q: -84.405\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 158s 16ms/step - reward: -10.7487\n",
            "1115 episodes - episode_reward: -96.400 [-100.142, -90.280] - loss: 6.070 - mae: 88.042 - mean_q: -84.193\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: -10.8893\n",
            "1129 episodes - episode_reward: -96.453 [-100.142, -89.991] - loss: 6.142 - mae: 88.015 - mean_q: -84.175\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: -10.7070\n",
            "1111 episodes - episode_reward: -96.373 [-100.142, -90.385] - loss: 6.014 - mae: 88.022 - mean_q: -84.208\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: -10.6692\n",
            "1107 episodes - episode_reward: -96.379 [-100.135, -90.318] - loss: 6.129 - mae: 87.943 - mean_q: -84.063\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: -10.4366\n",
            "1084 episodes - episode_reward: -96.277 [-99.974, -89.718] - loss: 6.243 - mae: 87.897 - mean_q: -84.067\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 185s 18ms/step - reward: -10.4608\n",
            "1086 episodes - episode_reward: -96.324 [-100.142, -87.401] - loss: 6.576 - mae: 87.642 - mean_q: -83.642\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 187s 19ms/step - reward: -10.5103\n",
            "1091 episodes - episode_reward: -96.335 [-100.142, -91.038] - loss: 6.214 - mae: 87.894 - mean_q: -84.092\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 189s 19ms/step - reward: -10.2967\n",
            "1070 episodes - episode_reward: -96.232 [-100.142, -87.095] - loss: 5.877 - mae: 87.905 - mean_q: -84.141\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 198s 20ms/step - reward: -10.4205\n",
            "1082 episodes - episode_reward: -96.308 [-100.142, -88.764] - loss: 6.034 - mae: 87.914 - mean_q: -84.073\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 199s 20ms/step - reward: -10.5412\n",
            "1094 episodes - episode_reward: -96.353 [-100.142, -90.161] - loss: 5.667 - mae: 88.090 - mean_q: -84.360\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 203s 20ms/step - reward: -10.1214\n",
            "1052 episodes - episode_reward: -96.213 [-100.142, -86.511] - loss: 5.462 - mae: 88.186 - mean_q: -84.525\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 206s 21ms/step - reward: -10.1434\n",
            "1054 episodes - episode_reward: -96.239 [-100.142, -88.358] - loss: 5.539 - mae: 88.092 - mean_q: -84.415\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 201s 20ms/step - reward: -10.2531\n",
            "1065 episodes - episode_reward: -96.272 [-100.142, -88.694] - loss: 5.585 - mae: 87.959 - mean_q: -84.202\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 204s 20ms/step - reward: -10.0520\n",
            "1045 episodes - episode_reward: -96.192 [-100.142, -89.698] - loss: 5.351 - mae: 88.188 - mean_q: -84.563\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 209s 21ms/step - reward: -10.1026\n",
            "1050 episodes - episode_reward: -96.213 [-100.142, -89.666] - loss: 5.348 - mae: 88.142 - mean_q: -84.513\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -9.8385\n",
            "1024 episodes - episode_reward: -96.079 [-100.142, -90.142] - loss: 5.379 - mae: 88.093 - mean_q: -84.436\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 225s 22ms/step - reward: -10.1738\n",
            "1057 episodes - episode_reward: -96.252 [-100.142, -90.086] - loss: 5.275 - mae: 88.065 - mean_q: -84.393\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 229s 23ms/step - reward: -10.0113\n",
            "1041 episodes - episode_reward: -96.170 [-100.142, -89.911] - loss: 5.416 - mae: 88.019 - mean_q: -84.335\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 224s 22ms/step - reward: -9.8798\n",
            "1028 episodes - episode_reward: -96.107 [-100.142, -90.568] - loss: 5.346 - mae: 88.049 - mean_q: -84.377\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 226s 23ms/step - reward: -10.1106\n",
            "1051 episodes - episode_reward: -96.204 [-100.142, -89.241] - loss: 5.592 - mae: 87.867 - mean_q: -84.064\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 233s 23ms/step - reward: -10.0566\n",
            "1045 episodes - episode_reward: -96.231 [-100.142, -90.192] - loss: 5.705 - mae: 87.796 - mean_q: -83.990\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 232s 23ms/step - reward: -10.1066\n",
            "1050 episodes - episode_reward: -96.255 [-100.142, -90.366] - loss: 5.568 - mae: 87.854 - mean_q: -84.140\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 232s 23ms/step - reward: -9.9415\n",
            "1034 episodes - episode_reward: -96.146 [-100.142, -90.081] - loss: 5.185 - mae: 88.089 - mean_q: -84.513\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 229s 23ms/step - reward: -10.1777\n",
            "1057 episodes - episode_reward: -96.291 [-100.142, -87.091] - loss: 5.567 - mae: 87.737 - mean_q: -83.982\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 242s 24ms/step - reward: -9.6990\n",
            "1010 episodes - episode_reward: -96.029 [-100.142, -88.231] - loss: 5.134 - mae: 88.055 - mean_q: -84.466\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 235s 24ms/step - reward: -9.8637\n",
            "1026 episodes - episode_reward: -96.137 [-100.142, -90.031] - loss: 5.227 - mae: 87.946 - mean_q: -84.247\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 236s 24ms/step - reward: -9.9726\n",
            "1037 episodes - episode_reward: -96.166 [-100.142, -90.171] - loss: 5.135 - mae: 88.006 - mean_q: -84.439\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 242s 24ms/step - reward: -9.5871\n",
            "999 episodes - episode_reward: -95.966 [-100.142, -87.911] - loss: 5.275 - mae: 87.914 - mean_q: -84.234\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 251s 25ms/step - reward: -9.9505\n",
            "1035 episodes - episode_reward: -96.145 [-100.142, -89.812] - loss: 5.061 - mae: 88.017 - mean_q: -84.410\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 251s 25ms/step - reward: -10.0599\n",
            "1046 episodes - episode_reward: -96.174 [-100.142, -87.114] - loss: 5.145 - mae: 87.959 - mean_q: -84.334\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 262s 26ms/step - reward: -10.0290\n",
            "1043 episodes - episode_reward: -96.155 [-100.142, -89.504] - loss: 5.136 - mae: 87.991 - mean_q: -84.370\n",
            "\n",
            "Interval 51 (500000 steps performed)\n",
            "10000/10000 [==============================] - 262s 26ms/step - reward: -9.8008\n",
            "1020 episodes - episode_reward: -96.084 [-100.142, -90.381] - loss: 4.982 - mae: 88.049 - mean_q: -84.476\n",
            "\n",
            "Interval 52 (510000 steps performed)\n",
            "10000/10000 [==============================] - 246s 25ms/step - reward: -9.7565\n",
            "1016 episodes - episode_reward: -96.028 [-100.142, -90.321] - loss: 5.011 - mae: 88.016 - mean_q: -84.421\n",
            "\n",
            "Interval 53 (520000 steps performed)\n",
            "10000/10000 [==============================] - 238s 24ms/step - reward: -9.9493\n",
            "1035 episodes - episode_reward: -96.129 [-100.142, -87.917] - loss: 5.079 - mae: 87.942 - mean_q: -84.359\n",
            "\n",
            "Interval 54 (530000 steps performed)\n",
            "10000/10000 [==============================] - 232s 23ms/step - reward: -10.0900\n",
            "1049 episodes - episode_reward: -96.186 [-100.022, -88.931] - loss: 5.141 - mae: 87.927 - mean_q: -84.357\n",
            "\n",
            "Interval 55 (540000 steps performed)\n",
            "10000/10000 [==============================] - 224s 22ms/step - reward: -9.8075\n",
            "1021 episodes - episode_reward: -96.057 [-100.142, -86.538] - loss: 5.204 - mae: 87.883 - mean_q: -84.245\n",
            "\n",
            "Interval 56 (550000 steps performed)\n",
            "10000/10000 [==============================] - 224s 22ms/step - reward: -9.8414\n",
            "1024 episodes - episode_reward: -96.111 [-100.142, -89.558] - loss: 5.141 - mae: 88.041 - mean_q: -84.454\n",
            "\n",
            "Interval 57 (560000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -9.6427\n",
            "1005 episodes - episode_reward: -95.945 [-100.142, -89.718] - loss: 5.436 - mae: 87.667 - mean_q: -83.899\n",
            "\n",
            "Interval 58 (570000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -9.7269\n",
            "1013 episodes - episode_reward: -96.023 [-100.142, -89.578] - loss: 5.345 - mae: 87.793 - mean_q: -84.119\n",
            "\n",
            "Interval 59 (580000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.7656\n",
            "1017 episodes - episode_reward: -96.021 [-100.142, -88.318] - loss: 5.484 - mae: 87.655 - mean_q: -83.878\n",
            "\n",
            "Interval 60 (590000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -9.7651\n",
            "1017 episodes - episode_reward: -96.021 [-100.142, -88.925] - loss: 5.292 - mae: 87.818 - mean_q: -84.148\n",
            "\n",
            "Interval 61 (600000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -9.6471\n",
            "1005 episodes - episode_reward: -95.992 [-100.142, -90.068] - loss: 5.605 - mae: 87.543 - mean_q: -83.738\n",
            "\n",
            "Interval 62 (610000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -9.6651\n",
            "1007 episodes - episode_reward: -95.978 [-100.142, -90.001] - loss: 5.642 - mae: 87.544 - mean_q: -83.726\n",
            "\n",
            "Interval 63 (620000 steps performed)\n",
            "10000/10000 [==============================] - 218s 22ms/step - reward: -9.8246\n",
            "1023 episodes - episode_reward: -96.035 [-100.142, -88.419] - loss: 6.114 - mae: 87.386 - mean_q: -83.496\n",
            "\n",
            "Interval 64 (630000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -9.6334\n",
            "1004 episodes - episode_reward: -95.949 [-100.142, -87.335] - loss: 5.493 - mae: 87.632 - mean_q: -83.909\n",
            "\n",
            "Interval 65 (640000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.7333\n",
            "1014 episodes - episode_reward: -95.993 [-100.142, -88.694] - loss: 5.543 - mae: 87.533 - mean_q: -83.728\n",
            "\n",
            "Interval 66 (650000 steps performed)\n",
            "10000/10000 [==============================] - 219s 22ms/step - reward: -9.7279\n",
            "1013 episodes - episode_reward: -96.026 [-100.142, -87.785] - loss: 5.359 - mae: 87.708 - mean_q: -83.962\n",
            "\n",
            "Interval 67 (660000 steps performed)\n",
            "10000/10000 [==============================] - 219s 22ms/step - reward: -9.8641\n",
            "1027 episodes - episode_reward: -96.051 [-100.135, -89.431] - loss: 5.467 - mae: 87.611 - mean_q: -83.811\n",
            "\n",
            "Interval 68 (670000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -10.4201\n",
            "1081 episodes - episode_reward: -96.390 [-100.142, -89.562] - loss: 5.907 - mae: 87.120 - mean_q: -83.144\n",
            "\n",
            "Interval 69 (680000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -9.7566\n",
            "1016 episodes - episode_reward: -96.031 [-100.142, -89.048] - loss: 5.231 - mae: 87.705 - mean_q: -83.977\n",
            "\n",
            "Interval 70 (690000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.9046\n",
            "1031 episodes - episode_reward: -96.068 [-100.142, -89.393] - loss: 5.602 - mae: 87.383 - mean_q: -83.565\n",
            "\n",
            "Interval 71 (700000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.7739\n",
            "1017 episodes - episode_reward: -96.109 [-100.142, -88.178] - loss: 5.689 - mae: 87.251 - mean_q: -83.308\n",
            "\n",
            "Interval 72 (710000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.7497\n",
            "1015 episodes - episode_reward: -96.056 [-100.142, -89.829] - loss: 5.151 - mae: 87.792 - mean_q: -84.171\n",
            "\n",
            "Interval 73 (720000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.7448\n",
            "1015 episodes - episode_reward: -96.008 [-100.142, -89.142] - loss: 5.137 - mae: 87.733 - mean_q: -84.042\n",
            "\n",
            "Interval 74 (730000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -10.0673\n",
            "1046 episodes - episode_reward: -96.245 [-100.142, -90.221] - loss: 5.377 - mae: 87.572 - mean_q: -83.854\n",
            "\n",
            "Interval 75 (740000 steps performed)\n",
            "10000/10000 [==============================] - 225s 22ms/step - reward: -9.7783\n",
            "1018 episodes - episode_reward: -96.051 [-100.142, -87.938] - loss: 5.165 - mae: 87.749 - mean_q: -84.057\n",
            "\n",
            "Interval 76 (750000 steps performed)\n",
            "10000/10000 [==============================] - 226s 23ms/step - reward: -9.4441\n",
            "985 episodes - episode_reward: -95.881 [-100.142, -88.968] - loss: 5.228 - mae: 87.673 - mean_q: -83.944\n",
            "\n",
            "Interval 77 (760000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.7267\n",
            "1013 episodes - episode_reward: -96.017 [-100.135, -87.158] - loss: 5.159 - mae: 87.681 - mean_q: -83.992\n",
            "\n",
            "Interval 78 (770000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.5836\n",
            "999 episodes - episode_reward: -95.931 [-100.142, -87.918] - loss: 5.137 - mae: 87.736 - mean_q: -84.028\n",
            "\n",
            "Interval 79 (780000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.6752\n",
            "1008 episodes - episode_reward: -95.985 [-100.142, -86.343] - loss: 5.188 - mae: 87.722 - mean_q: -84.059\n",
            "\n",
            "Interval 80 (790000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.8590\n",
            "1026 episodes - episode_reward: -96.095 [-100.142, -87.744] - loss: 5.119 - mae: 87.822 - mean_q: -84.168\n",
            "\n",
            "Interval 81 (800000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -10.1947\n",
            "1059 episodes - episode_reward: -96.266 [-100.142, -90.604] - loss: 6.000 - mae: 87.007 - mean_q: -83.052\n",
            "\n",
            "Interval 82 (810000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.6582\n",
            "1006 episodes - episode_reward: -96.007 [-100.142, -88.221] - loss: 5.344 - mae: 87.543 - mean_q: -83.743\n",
            "\n",
            "Interval 83 (820000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.5970\n",
            "1000 episodes - episode_reward: -95.968 [-100.142, -88.708] - loss: 5.015 - mae: 87.872 - mean_q: -84.273\n",
            "\n",
            "Interval 84 (830000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.6261\n",
            "1003 episodes - episode_reward: -95.975 [-100.142, -87.899] - loss: 5.186 - mae: 87.658 - mean_q: -83.948\n",
            "\n",
            "Interval 85 (840000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.5275\n",
            "993 episodes - episode_reward: -95.945 [-100.142, -88.332] - loss: 5.175 - mae: 87.729 - mean_q: -84.051\n",
            "\n",
            "Interval 86 (850000 steps performed)\n",
            " 9011/10000 [==========================>...] - ETA: 22s - reward: -9.5399done, took 17291.935 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -94.333, steps: 14\n",
            "Episode 2: reward: -90.524, steps: 18\n",
            "Episode 3: reward: -96.258, steps: 10\n",
            "Episode 4: reward: -91.831, steps: 18\n",
            "Episode 5: reward: -93.707, steps: 14\n",
            "Episode 6: reward: -95.398, steps: 10\n",
            "Episode 7: reward: -93.873, steps: 14\n",
            "Episode 8: reward: -96.358, steps: 10\n",
            "Episode 9: reward: -91.904, steps: 18\n",
            "Episode 10: reward: -95.285, steps: 10\n",
            "Episode 11: reward: -95.475, steps: 10\n",
            "Episode 12: reward: -94.150, steps: 14\n",
            "Episode 13: reward: -92.933, steps: 14\n",
            "Episode 14: reward: -96.588, steps: 10\n",
            "Episode 15: reward: -93.827, steps: 14\n",
            "Episode 16: reward: -95.948, steps: 10\n",
            "Episode 17: reward: -95.695, steps: 10\n",
            "Episode 18: reward: -99.647, steps: 2\n",
            "Episode 19: reward: -96.538, steps: 10\n",
            "Episode 20: reward: -93.687, steps: 14\n",
            "Episode 21: reward: -94.120, steps: 14\n",
            "Episode 22: reward: -91.438, steps: 18\n",
            "Episode 23: reward: -93.047, steps: 14\n",
            "Episode 24: reward: -94.838, steps: 10\n",
            "Episode 25: reward: -93.865, steps: 13\n",
            "Episode 26: reward: -94.473, steps: 14\n",
            "Episode 27: reward: -94.017, steps: 14\n",
            "Episode 28: reward: -89.454, steps: 22\n",
            "Episode 29: reward: -95.641, steps: 10\n",
            "Episode 30: reward: -96.011, steps: 10\n",
            "Episode 31: reward: -91.034, steps: 18\n",
            "Episode 32: reward: -94.828, steps: 10\n",
            "Episode 33: reward: -95.765, steps: 10\n",
            "Episode 34: reward: -95.948, steps: 10\n",
            "Episode 35: reward: -93.423, steps: 14\n",
            "Episode 36: reward: -96.010, steps: 9\n",
            "Episode 37: reward: -90.455, steps: 17\n",
            "Episode 38: reward: -97.312, steps: 6\n",
            "Episode 39: reward: -92.890, steps: 14\n",
            "Episode 40: reward: -90.748, steps: 18\n",
            "Episode 41: reward: -90.074, steps: 18\n",
            "Episode 42: reward: -87.574, steps: 22\n",
            "Episode 43: reward: -96.608, steps: 10\n",
            "Episode 44: reward: -94.473, steps: 14\n",
            "Episode 45: reward: -95.948, steps: 10\n",
            "Episode 46: reward: -89.881, steps: 18\n",
            "Episode 47: reward: -96.730, steps: 9\n",
            "Episode 48: reward: -91.828, steps: 18\n",
            "Episode 49: reward: -96.158, steps: 10\n",
            "Episode 50: reward: -90.348, steps: 18\n",
            "Episode 51: reward: -93.533, steps: 14\n",
            "Episode 52: reward: -93.903, steps: 14\n",
            "Episode 53: reward: -92.890, steps: 14\n",
            "Episode 54: reward: -95.895, steps: 10\n",
            "Episode 55: reward: -90.964, steps: 18\n",
            "Episode 56: reward: -94.280, steps: 14\n",
            "Episode 57: reward: -93.467, steps: 14\n",
            "Episode 58: reward: -95.870, steps: 9\n",
            "Episode 59: reward: -94.350, steps: 14\n",
            "Episode 60: reward: -95.701, steps: 10\n",
            "Episode 61: reward: -96.538, steps: 10\n",
            "Episode 62: reward: -95.695, steps: 10\n",
            "Episode 63: reward: -93.477, steps: 14\n",
            "Episode 64: reward: -93.630, steps: 14\n",
            "Episode 65: reward: -90.538, steps: 18\n",
            "Episode 66: reward: -93.827, steps: 14\n",
            "Episode 67: reward: -94.633, steps: 14\n",
            "Episode 68: reward: -93.810, steps: 14\n",
            "Episode 69: reward: -93.947, steps: 14\n",
            "Episode 70: reward: -97.362, steps: 6\n",
            "Episode 71: reward: -93.740, steps: 14\n",
            "Episode 72: reward: -94.588, steps: 13\n",
            "Episode 73: reward: -92.148, steps: 18\n",
            "Episode 74: reward: -90.894, steps: 18\n",
            "Episode 75: reward: -95.228, steps: 10\n",
            "Episode 76: reward: -95.215, steps: 10\n",
            "Episode 77: reward: -96.580, steps: 9\n",
            "Episode 78: reward: -93.687, steps: 14\n",
            "Episode 79: reward: -96.538, steps: 10\n",
            "Episode 80: reward: -93.630, steps: 14\n",
            "Episode 81: reward: -93.533, steps: 14\n",
            "Episode 82: reward: -95.551, steps: 10\n",
            "Episode 83: reward: -96.738, steps: 10\n",
            "Episode 84: reward: -94.281, steps: 13\n",
            "Episode 85: reward: -94.347, steps: 14\n",
            "Episode 86: reward: -95.948, steps: 10\n",
            "Episode 87: reward: -93.877, steps: 14\n",
            "Episode 88: reward: -94.267, steps: 14\n",
            "Episode 89: reward: -93.610, steps: 14\n",
            "Episode 90: reward: -91.254, steps: 18\n",
            "Episode 91: reward: -94.650, steps: 14\n",
            "Episode 92: reward: -94.713, steps: 14\n",
            "Episode 93: reward: -93.863, steps: 14\n",
            "Episode 94: reward: -96.288, steps: 10\n",
            "Episode 95: reward: -92.517, steps: 14\n",
            "Episode 96: reward: -96.201, steps: 10\n",
            "Episode 97: reward: -91.078, steps: 18\n",
            "Episode 98: reward: -94.097, steps: 14\n",
            "Episode 99: reward: -96.011, steps: 10\n",
            "Episode 100: reward: -90.931, steps: 18\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'17299.409345;2000000;-96.34152733108866;-86.34307692307692;-100.14205128205128;-94.0920846153846;-87.57435897435897;-99.64743589743588;17.85206143896524;55;0;100;100;1;0.00146;32;0.01;1e-05;avg;linear;2;48;48\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "hyperparameter = {\n",
        "    \"windows_length\": 1,\n",
        "    \"adam_learning_rate\": 0.00146,\n",
        "    \"batch_size\": 32,\n",
        "    \"target_model_update\": 0.01,\n",
        "    \"adam_epsilon\": 0.00001,\n",
        "    \"dueling_option\": \"avg\",\n",
        "    \"activation\": \"linear\",\n",
        "    \"layers\": 2,\n",
        "    \"unit_1\": 48,\n",
        "    \"unit_2\": 48,\n",
        "}\n",
        "\n",
        "ai.train(\n",
        "    hyperparameter=hyperparameter,\n",
        "    nb_steps=2_000_000,\n",
        "    env_config=env_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-FMgpdHFMog"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ai-model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "7724bf7a85847efc5bb6a1829c232f0862f78efa3c8535cdc9aa81401d00c18c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}