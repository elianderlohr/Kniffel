{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elianderlohr/Kniffel/blob/feature%2Fai/ai/ai-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZbB2tTectNx",
        "outputId": "17ee8721-780b-45db-8d79-294a6802389c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Git\n",
            "Wed May 18 12:54:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd '/content/drive/MyDrive/Git'\n",
        "\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n",
        "except ImportError as e:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ezGGzkUkfi",
        "outputId": "d2a8f847-7cd5-4b1e-e10a-22238af554b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-rl\n",
            "  Downloading keras-rl-0.4.2.tar.gz (40 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 35.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl) (2.8.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-py3-none-any.whl size=48378 sha256=ccfc7656765a5944f3faef80d11fd9e53e2389ceb19d2ca5d36a025fb3aee466\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/23/e9/278c2e59c322236e2bfdf7c792c16f0b4dec24816d27a3f1e4\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 867 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.25.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lop1FAXFctN0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'Kniffel')\n",
        "sys.path.insert(0, 'Kniffel/ai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3A2YD8QYctN0"
      },
      "outputs": [],
      "source": [
        "from ai.hyperparameter import Hyperparameter\n",
        "from ai.ai import KniffelAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ILWBfrctN1",
        "outputId": "d4d93d27-6978-40ce-e4e6-99398b66e68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 75 combinations to test.\n"
          ]
        }
      ],
      "source": [
        "ai = KniffelAI(save=True, load=False, predefined_layers=True, path_prefix=\"/content/drive/MyDrive/Git/Kniffel/\")\n",
        "\n",
        "env_config = {\n",
        "        \"reward_step\": 0,\n",
        "        \"reward_round\": 0.5,\n",
        "        \"reward_roll_dice\": 0.25,\n",
        "        \"reward_game_over\": -100,\n",
        "        \"reward_bonus\": 2,\n",
        "        \"reward_finish\": 10,\n",
        "        \"reward_zero_dice\": -0.5,\n",
        "        \"reward_one_dice\": -0.2,\n",
        "        \"reward_two_dice\": -0.1,\n",
        "        \"reward_three_dice\": 0.5,\n",
        "        \"reward_four_dice\": 0.6,\n",
        "        \"reward_five_dice\": 0.8,\n",
        "        \"reward_six_dice\": 1,\n",
        "        \"reward_kniffel\": 1.5,\n",
        "        \"reward_small_street\": 1,\n",
        "        \"reward_large_street\": 1.1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dhYYUUbEctN2"
      },
      "outputs": [],
      "source": [
        "#ai.grid_search_test(nb_steps=20_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DRxDEGqhfOP5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f407923-2e04-4d0c-d87d-786043919ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 208)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                13376     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 44)                2860      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,236\n",
            "Trainable params: 16,236\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 1000000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 98s 10ms/step - reward: -17.2522\n",
            "1762 episodes - episode_reward: -97.914 [-100.135, -94.519] - loss: 261.917 - mae: 51.072 - mean_q: -13.396\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: -11.0261\n",
            "1144 episodes - episode_reward: -96.384 [-99.398, -94.275] - loss: 65.508 - mae: 75.417 - mean_q: -44.636\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 103s 10ms/step - reward: -10.9084\n",
            "1132 episodes - episode_reward: -96.366 [-99.647, -92.217] - loss: 21.662 - mae: 92.048 - mean_q: -68.587\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 104s 10ms/step - reward: -11.7083\n",
            "1211 episodes - episode_reward: -96.680 [-99.647, -91.616] - loss: 17.869 - mae: 96.088 - mean_q: -77.094\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -13.9930\n",
            "1435 episodes - episode_reward: -97.514 [-99.974, -91.074] - loss: 18.921 - mae: 95.692 - mean_q: -78.644\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 108s 11ms/step - reward: -13.6770\n",
            "1404 episodes - episode_reward: -97.413 [-99.974, -91.615] - loss: 23.123 - mae: 93.002 - mean_q: -77.648\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 110s 11ms/step - reward: -11.1640\n",
            "1156 episodes - episode_reward: -96.573 [-100.129, -89.568] - loss: 16.239 - mae: 93.219 - mean_q: -80.898\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -10.8383\n",
            "1124 episodes - episode_reward: -96.428 [-100.142, -90.545] - loss: 10.867 - mae: 94.190 - mean_q: -83.960\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -10.0786\n",
            "1049 episodes - episode_reward: -96.077 [-100.135, -89.195] - loss: 7.916 - mae: 94.431 - mean_q: -85.669\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 109s 11ms/step - reward: -9.6582\n",
            "1008 episodes - episode_reward: -95.818 [-99.974, -89.632] - loss: 6.424 - mae: 94.125 - mean_q: -86.302\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -9.3506\n",
            "978 episodes - episode_reward: -95.608 [-99.974, -90.085] - loss: 5.533 - mae: 93.817 - mean_q: -86.576\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 112s 11ms/step - reward: -9.1432\n",
            "957 episodes - episode_reward: -95.547 [-99.974, -89.132] - loss: 5.047 - mae: 93.552 - mean_q: -86.793\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 113s 11ms/step - reward: -9.0298\n",
            "946 episodes - episode_reward: -95.449 [-99.974, -89.182] - loss: 4.790 - mae: 93.353 - mean_q: -86.979\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 115s 11ms/step - reward: -8.9006\n",
            "933 episodes - episode_reward: -95.394 [-99.974, -89.405] - loss: 4.596 - mae: 93.179 - mean_q: -87.018\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 119s 12ms/step - reward: -8.7518\n",
            "919 episodes - episode_reward: -95.235 [-99.974, -88.788] - loss: 4.483 - mae: 93.042 - mean_q: -86.979\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 122s 12ms/step - reward: -8.7010\n",
            "914 episodes - episode_reward: -95.195 [-99.974, -87.109] - loss: 4.443 - mae: 92.910 - mean_q: -86.875\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 122s 12ms/step - reward: -8.5211\n",
            "896 episodes - episode_reward: -95.103 [-100.142, -87.552] - loss: 4.527 - mae: 92.583 - mean_q: -86.606\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 126s 13ms/step - reward: -8.3524\n",
            "879 episodes - episode_reward: -95.025 [-99.295, -87.669] - loss: 4.585 - mae: 92.578 - mean_q: -86.455\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 129s 13ms/step - reward: -8.4341\n",
            "887 episodes - episode_reward: -95.084 [-99.974, -87.484] - loss: 4.681 - mae: 92.376 - mean_q: -86.290\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 132s 13ms/step - reward: -8.3510\n",
            "879 episodes - episode_reward: -95.005 [-99.974, -86.376] - loss: 4.684 - mae: 92.294 - mean_q: -86.157\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 137s 14ms/step - reward: -8.3724\n",
            "881 episodes - episode_reward: -95.038 [-99.974, -87.973] - loss: 4.710 - mae: 92.138 - mean_q: -86.017\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 140s 14ms/step - reward: -8.1698\n",
            "861 episodes - episode_reward: -94.880 [-99.974, -86.562] - loss: 4.766 - mae: 92.122 - mean_q: -85.895\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 142s 14ms/step - reward: -8.2112\n",
            "865 episodes - episode_reward: -94.926 [-99.295, -86.962] - loss: 4.752 - mae: 92.009 - mean_q: -85.863\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 142s 14ms/step - reward: -8.1933\n",
            "863 episodes - episode_reward: -94.940 [-99.647, -85.798] - loss: 4.793 - mae: 91.916 - mean_q: -85.787\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 147s 15ms/step - reward: -8.0488\n",
            "849 episodes - episode_reward: -94.805 [-99.647, -85.839] - loss: 4.728 - mae: 91.892 - mean_q: -85.720\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 151s 15ms/step - reward: -8.0601\n",
            "850 episodes - episode_reward: -94.828 [-99.647, -85.489] - loss: 4.826 - mae: 91.750 - mean_q: -85.610\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 153s 15ms/step - reward: -8.0375\n",
            "848 episodes - episode_reward: -94.777 [-99.974, -86.269] - loss: 4.818 - mae: 91.737 - mean_q: -85.578\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 157s 16ms/step - reward: -7.9392\n",
            "838 episodes - episode_reward: -94.741 [-99.647, -86.802] - loss: 4.752 - mae: 91.734 - mean_q: -85.518\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 161s 16ms/step - reward: -7.9500\n",
            "839 episodes - episode_reward: -94.756 [-99.647, -84.349] - loss: 4.699 - mae: 91.679 - mean_q: -85.518\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            " 5631/10000 [===============>..............] - ETA: 1:10 - reward: -7.9224done, took 3694.616 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.643, steps: 6\n",
            "Episode 2: reward: -93.742, steps: 14\n",
            "Episode 3: reward: -97.156, steps: 7\n",
            "Episode 4: reward: -91.402, steps: 18\n",
            "Episode 5: reward: -97.156, steps: 7\n",
            "Episode 6: reward: -90.478, steps: 19\n",
            "Episode 7: reward: -87.702, steps: 22\n",
            "Episode 8: reward: -90.085, steps: 19\n",
            "Episode 9: reward: -97.006, steps: 7\n",
            "Episode 10: reward: -92.254, steps: 17\n",
            "Episode 11: reward: -96.956, steps: 7\n",
            "Episode 12: reward: -93.600, steps: 15\n",
            "Episode 13: reward: -91.033, steps: 18\n",
            "Episode 14: reward: -90.215, steps: 19\n",
            "Episode 15: reward: -90.868, steps: 18\n",
            "Episode 16: reward: -93.050, steps: 15\n",
            "Episode 17: reward: -90.642, steps: 19\n",
            "Episode 18: reward: -95.640, steps: 11\n",
            "Episode 19: reward: -93.366, steps: 15\n",
            "Episode 20: reward: -96.906, steps: 7\n",
            "Episode 21: reward: -93.815, steps: 14\n",
            "Episode 22: reward: -88.732, steps: 19\n",
            "Episode 23: reward: -98.376, steps: 6\n",
            "Episode 24: reward: -93.961, steps: 14\n",
            "Episode 25: reward: -92.048, steps: 17\n",
            "Episode 26: reward: -93.460, steps: 15\n",
            "Episode 27: reward: -97.056, steps: 7\n",
            "Episode 28: reward: -93.356, steps: 15\n",
            "Episode 29: reward: -93.228, steps: 14\n",
            "Episode 30: reward: -93.889, steps: 14\n",
            "Episode 31: reward: -97.156, steps: 7\n",
            "Episode 32: reward: -95.254, steps: 12\n",
            "Episode 33: reward: -95.114, steps: 11\n",
            "Episode 34: reward: -95.434, steps: 11\n",
            "Episode 35: reward: -95.427, steps: 11\n",
            "Episode 36: reward: -92.278, steps: 17\n",
            "Episode 37: reward: -93.316, steps: 15\n",
            "Episode 38: reward: -93.944, steps: 14\n",
            "Episode 39: reward: -91.463, steps: 15\n",
            "Episode 40: reward: -96.856, steps: 7\n",
            "Episode 41: reward: -92.103, steps: 15\n",
            "Episode 42: reward: -95.427, steps: 11\n",
            "Episode 43: reward: -91.615, steps: 18\n",
            "Episode 44: reward: -97.006, steps: 7\n",
            "Episode 45: reward: -90.147, steps: 18\n",
            "Episode 46: reward: -93.401, steps: 13\n",
            "Episode 47: reward: -93.046, steps: 15\n",
            "Episode 48: reward: -97.206, steps: 7\n",
            "Episode 49: reward: -95.297, steps: 11\n",
            "Episode 50: reward: -91.646, steps: 15\n",
            "Episode 51: reward: -96.790, steps: 9\n",
            "Episode 52: reward: -90.708, steps: 19\n",
            "Episode 53: reward: -90.772, steps: 18\n",
            "Episode 54: reward: -95.530, steps: 11\n",
            "Episode 55: reward: -92.375, steps: 18\n",
            "Episode 56: reward: -95.365, steps: 12\n",
            "Episode 57: reward: -93.594, steps: 14\n",
            "Episode 58: reward: -97.106, steps: 7\n",
            "Episode 59: reward: -95.495, steps: 9\n",
            "Episode 60: reward: -95.360, steps: 11\n",
            "Episode 61: reward: -97.306, steps: 7\n",
            "Episode 62: reward: -90.238, steps: 19\n",
            "Episode 63: reward: -95.530, steps: 11\n",
            "Episode 64: reward: -95.230, steps: 11\n",
            "Episode 65: reward: -90.143, steps: 18\n",
            "Episode 66: reward: -94.641, steps: 13\n",
            "Episode 67: reward: -92.870, steps: 15\n",
            "Episode 68: reward: -87.215, steps: 22\n",
            "Episode 69: reward: -95.310, steps: 11\n",
            "Episode 70: reward: -94.984, steps: 11\n",
            "Episode 71: reward: -95.597, steps: 10\n",
            "Episode 72: reward: -93.030, steps: 15\n",
            "Episode 73: reward: -93.236, steps: 15\n",
            "Episode 74: reward: -94.162, steps: 14\n",
            "Episode 75: reward: -87.422, steps: 22\n",
            "Episode 76: reward: -92.444, steps: 16\n",
            "Episode 77: reward: -90.582, steps: 19\n",
            "Episode 78: reward: -97.206, steps: 7\n",
            "Episode 79: reward: -93.177, steps: 14\n",
            "Episode 80: reward: -93.280, steps: 15\n",
            "Episode 81: reward: -92.128, steps: 17\n",
            "Episode 82: reward: -97.643, steps: 6\n",
            "Episode 83: reward: -95.817, steps: 11\n",
            "Episode 84: reward: -97.256, steps: 7\n",
            "Episode 85: reward: -96.906, steps: 7\n",
            "Episode 86: reward: -96.956, steps: 7\n",
            "Episode 87: reward: -95.107, steps: 11\n",
            "Episode 88: reward: -95.564, steps: 11\n",
            "Episode 89: reward: -95.362, steps: 10\n",
            "Episode 90: reward: -95.497, steps: 11\n",
            "Episode 91: reward: -95.117, steps: 13\n",
            "Episode 92: reward: -95.107, steps: 11\n",
            "Episode 93: reward: -86.972, steps: 22\n",
            "Episode 94: reward: -97.503, steps: 6\n",
            "Episode 95: reward: -93.566, steps: 15\n",
            "Episode 96: reward: -95.780, steps: 11\n",
            "Episode 97: reward: -97.306, steps: 7\n",
            "Episode 98: reward: -91.485, steps: 18\n",
            "Episode 99: reward: -94.670, steps: 11\n",
            "Episode 100: reward: -92.475, steps: 17\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3702.081362;1000000;-95.80336521024813;-84.34897435897436;-100.14205128205128;-93.92878461538459;-86.97179487179487;-98.37589743589743;21.618505338078293;58;0;100;100;1;0.0005000000000000001;35;0.00030000000000000003;avg;linear;1;64\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "hyperparameter = {\n",
        "    \"windows_length\": 1,\n",
        "    \"adam_learning_rate\": 0.0005000000000000001,\n",
        "    \"batch_size\": 35,\n",
        "    \"target_model_update\": 0.00030000000000000003,\n",
        "    \"dueling_option\": \"avg\",\n",
        "    \"activation\": \"linear\",\n",
        "    \"layers\": 1,\n",
        "    \"unit_1\": 64,\n",
        "}\n",
        "\n",
        "ai.train(\n",
        "    hyperparameter=hyperparameter,\n",
        "    nb_steps=1_000_000,\n",
        "    env_config=env_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-FMgpdHFMog"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "ai-model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "7724bf7a85847efc5bb6a1829c232f0862f78efa3c8535cdc9aa81401d00c18c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}