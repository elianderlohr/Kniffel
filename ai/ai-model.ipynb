{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elianderlohr/Kniffel/blob/feature%2Fnew-ai/ai/ai-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZbB2tTectNx",
        "outputId": "8285557a-e2e7-49c2-f34b-8b5b74235d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Git\n",
            "Sun Jun  5 07:12:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd '/content/drive/MyDrive/Git'\n",
        "\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Not connected to a GPU')\n",
        "    else:\n",
        "      print(gpu_info)\n",
        "except ImportError as e:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ezGGzkUkfi",
        "outputId": "6523757e-1923-4dc6-e756-0f6e5cd87345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl\n",
            "  Downloading keras-rl-0.4.2.tar.gz (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.7/dist-packages (from keras-rl) (2.8.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-py3-none-any.whl size=48378 sha256=028e4fa804e316f1d7616799e8e7e4e284a1ec1ff3fe59e73368820fbc04cf1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/23/e9/278c2e59c322236e2bfdf7c792c16f0b4dec24816d27a3f1e4\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (14.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.46.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.26.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl\n",
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lop1FAXFctN0"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'Kniffel')\n",
        "sys.path.insert(0, 'Kniffel/ai')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3A2YD8QYctN0"
      },
      "outputs": [],
      "source": [
        "from ai.hyperparameter import Hyperparameter\n",
        "from ai.ai import KniffelAI\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ILWBfrctN1",
        "outputId": "4e18cb41-63fe-456f-f55a-526c7f3e0dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 2400 combinations to test.\n"
          ]
        }
      ],
      "source": [
        "units = list(range(16, 64, 16))\n",
        "\n",
        "base_hp = {\n",
        "    \"windows_length\": [1],\n",
        "    \"adam_learning_rate\": [0.0001, 0.0005, 0.001, 0.00146, 0.005, 0.01],\n",
        "    \"adam_epsilon\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
        "    \"batch_size\": [32],\n",
        "    \"target_model_update\": [0.0001, 0.0005, 0.001, 0.00146, 0.005, 0.01],\n",
        "    \"dueling_option\": [\"avg\"],\n",
        "    \"activation\": [\"linear\"],\n",
        "    \"layers\": [1, 2],\n",
        "    \"unit_1\": units,\n",
        "    \"unit_2\": units,\n",
        "}\n",
        "\n",
        "ai = KniffelAI(\n",
        "    save=True, load=False, predefined_layers=True, hyperparater_base=base_hp, path_prefix=\"/content/drive/MyDrive/Git/Kniffel/\"\n",
        ")\n",
        "\n",
        "env_config = {\n",
        "        \"reward_step\": 0,\n",
        "        \"reward_round\": 0.5,\n",
        "        \"reward_roll_dice\": 0.25,\n",
        "        \"reward_game_over\": -100,\n",
        "        \"reward_bonus\": 2,\n",
        "        \"reward_finish\": 10,\n",
        "        \"reward_zero_dice\": -0.5,\n",
        "        \"reward_one_dice\": -0.2,\n",
        "        \"reward_two_dice\": -0.1,\n",
        "        \"reward_three_dice\": 0.5,\n",
        "        \"reward_four_dice\": 0.6,\n",
        "        \"reward_five_dice\": 0.8,\n",
        "        \"reward_six_dice\": 1,\n",
        "        \"reward_kniffel\": 1.5,\n",
        "        \"reward_small_street\": 1,\n",
        "        \"reward_large_street\": 1.1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJR2jjqxkEvt",
        "outputId": "da925aab-2dd8-4285-b469-707a2f5381e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 41)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "break_counter: 1000\n",
            "AVG. 17.771496625645096\n",
            "MAX: 56\n",
            "MIN: 0\n",
            "AVG rounds: 6.693316084519189\n",
            "Max rounds: 21\n",
            "Min rounds: 1\n"
          ]
        }
      ],
      "source": [
        "# ai.play(path=\"Kniffel/weights/p_date=2022-06-04-06_57_09\", episodes=1000, env_config=env_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dhYYUUbEctN2",
        "outputId": "3b77ea2f-7564-49c7-d244-213df981872a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 2160 combinations to test.\n",
            "windows_length;adam_learning_rate;adam_epsilon;batch_size;target_model_update;dueling_option;activation;layers;unit_1;unit_2\n",
            "Created 2160 combinations to test.\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 1 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0001, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 32, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 41)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,637\n",
            "Trainable params: 2,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mDie letzten 5000 Zeilen der Streamingausgabe wurden abgeschnitten.\u001b[0m\n",
            "Episode 33: reward: -99.647, steps: 2\n",
            "Episode 34: reward: -97.462, steps: 6\n",
            "Episode 35: reward: -99.647, steps: 2\n",
            "Episode 36: reward: -99.647, steps: 2\n",
            "Episode 37: reward: -99.295, steps: 3\n",
            "Episode 38: reward: -99.647, steps: 2\n",
            "Episode 39: reward: -98.595, steps: 6\n",
            "Episode 40: reward: -95.898, steps: 10\n",
            "Episode 41: reward: -99.647, steps: 2\n",
            "Episode 42: reward: -97.741, steps: 5\n",
            "Episode 43: reward: -99.295, steps: 3\n",
            "Episode 44: reward: -99.647, steps: 2\n",
            "Episode 45: reward: -96.448, steps: 10\n",
            "Episode 46: reward: -97.941, steps: 5\n",
            "Episode 47: reward: -96.368, steps: 9\n",
            "Episode 48: reward: -97.941, steps: 5\n",
            "Episode 49: reward: -95.829, steps: 10\n",
            "Episode 50: reward: -95.978, steps: 10\n",
            "Episode 51: reward: -99.295, steps: 3\n",
            "Episode 52: reward: -99.295, steps: 3\n",
            "Episode 53: reward: -95.728, steps: 10\n",
            "Episode 54: reward: -99.769, steps: 3\n",
            "Episode 55: reward: -95.358, steps: 10\n",
            "Episode 56: reward: -98.428, steps: 6\n",
            "Episode 57: reward: -98.595, steps: 6\n",
            "Episode 58: reward: -95.390, steps: 11\n",
            "Episode 59: reward: -98.289, steps: 4\n",
            "Episode 60: reward: -97.841, steps: 5\n",
            "Episode 61: reward: -95.778, steps: 10\n",
            "Episode 62: reward: -99.391, steps: 4\n",
            "Episode 63: reward: -99.295, steps: 3\n",
            "Episode 64: reward: -97.741, steps: 5\n",
            "Episode 65: reward: -99.643, steps: 3\n",
            "Episode 66: reward: -99.647, steps: 2\n",
            "Episode 67: reward: -99.295, steps: 3\n",
            "Episode 68: reward: -99.295, steps: 3\n",
            "Episode 69: reward: -97.791, steps: 5\n",
            "Episode 70: reward: -99.647, steps: 2\n",
            "Episode 71: reward: -99.647, steps: 2\n",
            "Episode 72: reward: -97.641, steps: 5\n",
            "Episode 73: reward: -95.948, steps: 10\n",
            "Episode 74: reward: -97.741, steps: 5\n",
            "Episode 75: reward: -97.941, steps: 5\n",
            "Episode 76: reward: -98.595, steps: 6\n",
            "Episode 77: reward: -97.641, steps: 5\n",
            "Episode 78: reward: -97.791, steps: 5\n",
            "Episode 79: reward: -97.841, steps: 5\n",
            "Episode 80: reward: -97.862, steps: 6\n",
            "Episode 81: reward: -95.528, steps: 10\n",
            "Episode 82: reward: -99.647, steps: 2\n",
            "Episode 83: reward: -97.741, steps: 5\n",
            "Episode 84: reward: -99.295, steps: 3\n",
            "Episode 85: reward: -99.295, steps: 3\n",
            "Episode 86: reward: -96.768, steps: 9\n",
            "Episode 87: reward: -96.028, steps: 10\n",
            "Episode 88: reward: -99.647, steps: 2\n",
            "Episode 89: reward: -95.998, steps: 10\n",
            "Episode 90: reward: -97.891, steps: 5\n",
            "Episode 91: reward: -99.295, steps: 3\n",
            "Episode 92: reward: -97.791, steps: 5\n",
            "Episode 93: reward: -99.647, steps: 2\n",
            "Episode 94: reward: -97.143, steps: 7\n",
            "Episode 95: reward: -95.948, steps: 10\n",
            "Episode 96: reward: -96.198, steps: 10\n",
            "Episode 97: reward: -97.891, steps: 5\n",
            "Episode 98: reward: -96.618, steps: 9\n",
            "Episode 99: reward: -99.647, steps: 2\n",
            "Episode 100: reward: -99.295, steps: 3\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 25 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.01, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_24 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,069\n",
            "Trainable params: 5,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 171s 17ms/step - reward: -21.2584\n",
            "2160 episodes - episode_reward: -98.419 [-100.142, -92.150] - loss: 46.791 - mae: 85.050 - mean_q: -77.245\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: -17.0456\n",
            "1740 episodes - episode_reward: -97.963 [-100.109, -91.376] - loss: 8.579 - mae: 90.108 - mean_q: -86.280\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: -16.5980\n",
            "done, took 527.832 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -98.141, steps: 5\n",
            "Episode 2: reward: -94.597, steps: 11\n",
            "Episode 3: reward: -95.624, steps: 11\n",
            "Episode 4: reward: -99.295, steps: 3\n",
            "Episode 5: reward: -98.458, steps: 4\n",
            "Episode 6: reward: -98.106, steps: 7\n",
            "Episode 7: reward: -98.106, steps: 7\n",
            "Episode 8: reward: -98.106, steps: 7\n",
            "Episode 9: reward: -98.491, steps: 5\n",
            "Episode 10: reward: -97.106, steps: 7\n",
            "Episode 11: reward: -95.560, steps: 11\n",
            "Episode 12: reward: -94.470, steps: 11\n",
            "Episode 13: reward: -99.378, steps: 4\n",
            "Episode 14: reward: -95.434, steps: 11\n",
            "Episode 15: reward: -97.906, steps: 7\n",
            "Episode 16: reward: -95.434, steps: 11\n",
            "Episode 17: reward: -97.106, steps: 7\n",
            "Episode 18: reward: -98.341, steps: 5\n",
            "Episode 19: reward: -96.587, steps: 9\n",
            "Episode 20: reward: -96.577, steps: 9\n",
            "Episode 21: reward: -98.106, steps: 7\n",
            "Episode 22: reward: -98.106, steps: 7\n",
            "Episode 23: reward: -99.378, steps: 4\n",
            "Episode 24: reward: -98.106, steps: 7\n",
            "Episode 25: reward: -98.106, steps: 7\n",
            "Episode 26: reward: -97.206, steps: 7\n",
            "Episode 27: reward: -97.906, steps: 7\n",
            "Episode 28: reward: -99.295, steps: 3\n",
            "Episode 29: reward: -98.191, steps: 5\n",
            "Episode 30: reward: -97.256, steps: 7\n",
            "Episode 31: reward: -99.378, steps: 4\n",
            "Episode 32: reward: -97.006, steps: 7\n",
            "Episode 33: reward: -97.906, steps: 7\n",
            "Episode 34: reward: -98.106, steps: 7\n",
            "Episode 35: reward: -99.378, steps: 4\n",
            "Episode 36: reward: -97.056, steps: 7\n",
            "Episode 37: reward: -99.295, steps: 3\n",
            "Episode 38: reward: -97.156, steps: 7\n",
            "Episode 39: reward: -99.378, steps: 4\n",
            "Episode 40: reward: -98.106, steps: 7\n",
            "Episode 41: reward: -98.658, steps: 4\n",
            "Episode 42: reward: -98.191, steps: 5\n",
            "Episode 43: reward: -96.840, steps: 9\n",
            "Episode 44: reward: -99.378, steps: 4\n",
            "Episode 45: reward: -99.295, steps: 3\n",
            "Episode 46: reward: -97.056, steps: 7\n",
            "Episode 47: reward: -96.387, steps: 9\n",
            "Episode 48: reward: -95.044, steps: 11\n",
            "Episode 49: reward: -98.106, steps: 7\n",
            "Episode 50: reward: -98.106, steps: 7\n",
            "Episode 51: reward: -96.650, steps: 9\n",
            "Episode 52: reward: -98.191, steps: 5\n",
            "Episode 53: reward: -98.141, steps: 5\n",
            "Episode 54: reward: -99.378, steps: 4\n",
            "Episode 55: reward: -99.295, steps: 3\n",
            "Episode 56: reward: -98.418, steps: 4\n",
            "Episode 57: reward: -97.156, steps: 7\n",
            "Episode 58: reward: -98.298, steps: 4\n",
            "Episode 59: reward: -98.341, steps: 5\n",
            "Episode 60: reward: -98.041, steps: 5\n",
            "Episode 61: reward: -98.106, steps: 7\n",
            "Episode 62: reward: -97.256, steps: 7\n",
            "Episode 63: reward: -98.241, steps: 5\n",
            "Episode 64: reward: -99.295, steps: 3\n",
            "Episode 65: reward: -99.295, steps: 3\n",
            "Episode 66: reward: -98.418, steps: 4\n",
            "Episode 67: reward: -96.577, steps: 9\n",
            "Episode 68: reward: -97.712, steps: 6\n",
            "Episode 69: reward: -98.106, steps: 7\n",
            "Episode 70: reward: -97.940, steps: 7\n",
            "Episode 71: reward: -97.256, steps: 7\n",
            "Episode 72: reward: -98.241, steps: 5\n",
            "Episode 73: reward: -96.777, steps: 9\n",
            "Episode 74: reward: -98.191, steps: 5\n",
            "Episode 75: reward: -95.855, steps: 10\n",
            "Episode 76: reward: -98.106, steps: 7\n",
            "Episode 77: reward: -97.156, steps: 7\n",
            "Episode 78: reward: -95.624, steps: 11\n",
            "Episode 79: reward: -95.624, steps: 11\n",
            "Episode 80: reward: -98.498, steps: 4\n",
            "Episode 81: reward: -95.865, steps: 10\n",
            "Episode 82: reward: -99.378, steps: 4\n",
            "Episode 83: reward: -99.295, steps: 3\n",
            "Episode 84: reward: -97.106, steps: 7\n",
            "Episode 85: reward: -97.906, steps: 7\n",
            "Episode 86: reward: -94.344, steps: 11\n",
            "Episode 87: reward: -96.713, steps: 9\n",
            "Episode 88: reward: -98.106, steps: 7\n",
            "Episode 89: reward: -98.091, steps: 5\n",
            "Episode 90: reward: -98.241, steps: 5\n",
            "Episode 91: reward: -97.356, steps: 7\n",
            "Episode 92: reward: -99.378, steps: 4\n",
            "Episode 93: reward: -95.044, steps: 11\n",
            "Episode 94: reward: -98.241, steps: 5\n",
            "Episode 95: reward: -99.295, steps: 3\n",
            "Episode 96: reward: -96.956, steps: 7\n",
            "Episode 97: reward: -97.256, steps: 7\n",
            "Episode 98: reward: -97.712, steps: 6\n",
            "Episode 99: reward: -98.156, steps: 7\n",
            "Episode 100: reward: -97.256, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 26 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.005, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.01, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_25 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 16)                784       \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,565\n",
            "Trainable params: 3,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 173s 17ms/step - reward: -21.5483\n",
            "2186 episodes - episode_reward: -98.575 [-100.142, -94.367] - loss: 110.219 - mae: 77.987 - mean_q: -65.814\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 183s 18ms/step - reward: -16.9916\n",
            "1734 episodes - episode_reward: -97.990 [-100.022, -93.605] - loss: 14.706 - mae: 88.838 - mean_q: -83.799\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: -16.1376\n",
            "done, took 536.959 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -95.750, steps: 11\n",
            "Episode 2: reward: -97.256, steps: 7\n",
            "Episode 3: reward: -95.850, steps: 11\n",
            "Episode 4: reward: -96.806, steps: 7\n",
            "Episode 5: reward: -99.647, steps: 2\n",
            "Episode 6: reward: -96.956, steps: 7\n",
            "Episode 7: reward: -96.690, steps: 9\n",
            "Episode 8: reward: -96.956, steps: 7\n",
            "Episode 9: reward: -97.056, steps: 7\n",
            "Episode 10: reward: -97.056, steps: 7\n",
            "Episode 11: reward: -99.295, steps: 3\n",
            "Episode 12: reward: -96.956, steps: 7\n",
            "Episode 13: reward: -97.006, steps: 7\n",
            "Episode 14: reward: -95.500, steps: 11\n",
            "Episode 15: reward: -98.924, steps: 5\n",
            "Episode 16: reward: -99.058, steps: 5\n",
            "Episode 17: reward: -97.006, steps: 7\n",
            "Episode 18: reward: -97.056, steps: 7\n",
            "Episode 19: reward: -95.500, steps: 11\n",
            "Episode 20: reward: -97.106, steps: 7\n",
            "Episode 21: reward: -97.156, steps: 7\n",
            "Episode 22: reward: -98.628, steps: 6\n",
            "Episode 23: reward: -97.256, steps: 7\n",
            "Episode 24: reward: -95.920, steps: 9\n",
            "Episode 25: reward: -97.056, steps: 7\n",
            "Episode 26: reward: -96.258, steps: 10\n",
            "Episode 27: reward: -95.870, steps: 9\n",
            "Episode 28: reward: -96.756, steps: 7\n",
            "Episode 29: reward: -99.058, steps: 5\n",
            "Episode 30: reward: -95.400, steps: 11\n",
            "Episode 31: reward: -99.295, steps: 3\n",
            "Episode 32: reward: -96.756, steps: 7\n",
            "Episode 33: reward: -97.056, steps: 7\n",
            "Episode 34: reward: -97.156, steps: 7\n",
            "Episode 35: reward: -96.956, steps: 7\n",
            "Episode 36: reward: -99.058, steps: 5\n",
            "Episode 37: reward: -97.106, steps: 7\n",
            "Episode 38: reward: -96.480, steps: 9\n",
            "Episode 39: reward: -96.856, steps: 7\n",
            "Episode 40: reward: -99.405, steps: 4\n",
            "Episode 41: reward: -95.848, steps: 10\n",
            "Episode 42: reward: -99.405, steps: 4\n",
            "Episode 43: reward: -99.295, steps: 3\n",
            "Episode 44: reward: -96.706, steps: 7\n",
            "Episode 45: reward: -97.256, steps: 7\n",
            "Episode 46: reward: -98.173, steps: 7\n",
            "Episode 47: reward: -99.058, steps: 5\n",
            "Episode 48: reward: -96.430, steps: 9\n",
            "Episode 49: reward: -96.956, steps: 7\n",
            "Episode 50: reward: -96.856, steps: 7\n",
            "Episode 51: reward: -96.790, steps: 9\n",
            "Episode 52: reward: -97.006, steps: 7\n",
            "Episode 53: reward: -97.056, steps: 7\n",
            "Episode 54: reward: -95.700, steps: 11\n",
            "Episode 55: reward: -99.974, steps: 1\n",
            "Episode 56: reward: -99.295, steps: 3\n",
            "Episode 57: reward: -99.058, steps: 5\n",
            "Episode 58: reward: -99.405, steps: 4\n",
            "Episode 59: reward: -98.628, steps: 6\n",
            "Episode 60: reward: -96.806, steps: 7\n",
            "Episode 61: reward: -95.848, steps: 10\n",
            "Episode 62: reward: -97.056, steps: 7\n",
            "Episode 63: reward: -99.295, steps: 3\n",
            "Episode 64: reward: -97.006, steps: 7\n",
            "Episode 65: reward: -98.040, steps: 7\n",
            "Episode 66: reward: -96.906, steps: 7\n",
            "Episode 67: reward: -99.974, steps: 1\n",
            "Episode 68: reward: -99.405, steps: 4\n",
            "Episode 69: reward: -97.256, steps: 7\n",
            "Episode 70: reward: -98.173, steps: 7\n",
            "Episode 71: reward: -97.156, steps: 7\n",
            "Episode 72: reward: -96.956, steps: 7\n",
            "Episode 73: reward: -99.974, steps: 1\n",
            "Episode 74: reward: -96.740, steps: 9\n",
            "Episode 75: reward: -96.856, steps: 7\n",
            "Episode 76: reward: -99.295, steps: 3\n",
            "Episode 77: reward: -97.406, steps: 7\n",
            "Episode 78: reward: -97.356, steps: 7\n",
            "Episode 79: reward: -97.206, steps: 7\n",
            "Episode 80: reward: -96.906, steps: 7\n",
            "Episode 81: reward: -97.306, steps: 7\n",
            "Episode 82: reward: -95.534, steps: 10\n",
            "Episode 83: reward: -99.295, steps: 3\n",
            "Episode 84: reward: -96.806, steps: 7\n",
            "Episode 85: reward: -97.206, steps: 7\n",
            "Episode 86: reward: -97.106, steps: 7\n",
            "Episode 87: reward: -99.058, steps: 5\n",
            "Episode 88: reward: -97.106, steps: 7\n",
            "Episode 89: reward: -97.456, steps: 7\n",
            "Episode 90: reward: -97.456, steps: 7\n",
            "Episode 91: reward: -97.006, steps: 7\n",
            "Episode 92: reward: -96.740, steps: 9\n",
            "Episode 93: reward: -99.058, steps: 5\n",
            "Episode 94: reward: -96.706, steps: 7\n",
            "Episode 95: reward: -96.480, steps: 9\n",
            "Episode 96: reward: -96.806, steps: 7\n",
            "Episode 97: reward: -98.924, steps: 5\n",
            "Episode 98: reward: -99.295, steps: 3\n",
            "Episode 99: reward: -99.391, steps: 4\n",
            "Episode 100: reward: -97.006, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 27 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 1e-05, 'batch_size': 32, 'target_model_update': 0.005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_26 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_66 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 176s 18ms/step - reward: -22.7186\n",
            "2306 episodes - episode_reward: -98.520 [-100.142, -94.213] - loss: 88.276 - mae: 80.980 - mean_q: -70.680\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 185s 18ms/step - reward: -18.3195\n",
            "1868 episodes - episode_reward: -98.071 [-100.135, -93.311] - loss: 7.372 - mae: 91.556 - mean_q: -88.616\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 183s 18ms/step - reward: -16.4666\n",
            "done, took 544.675 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.803, steps: 9\n",
            "Episode 2: reward: -97.841, steps: 5\n",
            "Episode 3: reward: -96.383, steps: 9\n",
            "Episode 4: reward: -99.974, steps: 1\n",
            "Episode 5: reward: -96.383, steps: 9\n",
            "Episode 6: reward: -96.320, steps: 9\n",
            "Episode 7: reward: -97.762, steps: 6\n",
            "Episode 8: reward: -98.075, steps: 6\n",
            "Episode 9: reward: -96.670, steps: 9\n",
            "Episode 10: reward: -96.797, steps: 9\n",
            "Episode 11: reward: -96.193, steps: 9\n",
            "Episode 12: reward: -97.941, steps: 5\n",
            "Episode 13: reward: -96.890, steps: 9\n",
            "Episode 14: reward: -98.241, steps: 5\n",
            "Episode 15: reward: -97.741, steps: 5\n",
            "Episode 16: reward: -96.145, steps: 10\n",
            "Episode 17: reward: -98.141, steps: 5\n",
            "Episode 18: reward: -99.647, steps: 2\n",
            "Episode 19: reward: -95.915, steps: 10\n",
            "Episode 20: reward: -95.853, steps: 9\n",
            "Episode 21: reward: -96.844, steps: 8\n",
            "Episode 22: reward: -97.991, steps: 5\n",
            "Episode 23: reward: -96.170, steps: 9\n",
            "Episode 24: reward: -98.191, steps: 5\n",
            "Episode 25: reward: -97.941, steps: 5\n",
            "Episode 26: reward: -96.193, steps: 9\n",
            "Episode 27: reward: -96.700, steps: 9\n",
            "Episode 28: reward: -97.791, steps: 5\n",
            "Episode 29: reward: -96.763, steps: 9\n",
            "Episode 30: reward: -99.974, steps: 1\n",
            "Episode 31: reward: -98.341, steps: 5\n",
            "Episode 32: reward: -96.607, steps: 9\n",
            "Episode 33: reward: -96.447, steps: 9\n",
            "Episode 34: reward: -96.320, steps: 9\n",
            "Episode 35: reward: -98.091, steps: 5\n",
            "Episode 36: reward: -97.991, steps: 5\n",
            "Episode 37: reward: -96.553, steps: 9\n",
            "Episode 38: reward: -98.291, steps: 5\n",
            "Episode 39: reward: -98.191, steps: 5\n",
            "Episode 40: reward: -96.233, steps: 9\n",
            "Episode 41: reward: -96.510, steps: 9\n",
            "Episode 42: reward: -96.480, steps: 9\n",
            "Episode 43: reward: -96.447, steps: 9\n",
            "Episode 44: reward: -98.241, steps: 5\n",
            "Episode 45: reward: -97.891, steps: 5\n",
            "Episode 46: reward: -96.338, steps: 9\n",
            "Episode 47: reward: -97.791, steps: 5\n",
            "Episode 48: reward: -98.241, steps: 5\n",
            "Episode 49: reward: -95.917, steps: 9\n",
            "Episode 50: reward: -97.991, steps: 5\n",
            "Episode 51: reward: -98.041, steps: 5\n",
            "Episode 52: reward: -98.041, steps: 5\n",
            "Episode 53: reward: -96.300, steps: 9\n",
            "Episode 54: reward: -96.637, steps: 9\n",
            "Episode 55: reward: -97.147, steps: 8\n",
            "Episode 56: reward: -96.637, steps: 9\n",
            "Episode 57: reward: -97.791, steps: 5\n",
            "Episode 58: reward: -96.573, steps: 9\n",
            "Episode 59: reward: -96.613, steps: 9\n",
            "Episode 60: reward: -96.303, steps: 9\n",
            "Episode 61: reward: -97.841, steps: 5\n",
            "Episode 62: reward: -96.480, steps: 9\n",
            "Episode 63: reward: -98.041, steps: 5\n",
            "Episode 64: reward: -98.341, steps: 5\n",
            "Episode 65: reward: -98.291, steps: 5\n",
            "Episode 66: reward: -97.891, steps: 5\n",
            "Episode 67: reward: -96.763, steps: 9\n",
            "Episode 68: reward: -96.300, steps: 9\n",
            "Episode 69: reward: -97.017, steps: 9\n",
            "Episode 70: reward: -96.964, steps: 8\n",
            "Episode 71: reward: -98.191, steps: 5\n",
            "Episode 72: reward: -96.320, steps: 9\n",
            "Episode 73: reward: -98.341, steps: 5\n",
            "Episode 74: reward: -97.941, steps: 5\n",
            "Episode 75: reward: -96.670, steps: 9\n",
            "Episode 76: reward: -96.320, steps: 9\n",
            "Episode 77: reward: -98.041, steps: 5\n",
            "Episode 78: reward: -97.050, steps: 9\n",
            "Episode 79: reward: -97.791, steps: 5\n",
            "Episode 80: reward: -96.573, steps: 9\n",
            "Episode 81: reward: -98.091, steps: 5\n",
            "Episode 82: reward: -96.607, steps: 8\n",
            "Episode 83: reward: -97.991, steps: 5\n",
            "Episode 84: reward: -96.740, steps: 9\n",
            "Episode 85: reward: -97.017, steps: 9\n",
            "Episode 86: reward: -97.791, steps: 5\n",
            "Episode 87: reward: -96.257, steps: 9\n",
            "Episode 88: reward: -97.762, steps: 6\n",
            "Episode 89: reward: -97.941, steps: 5\n",
            "Episode 90: reward: -98.141, steps: 5\n",
            "Episode 91: reward: -96.008, steps: 10\n",
            "Episode 92: reward: -97.841, steps: 5\n",
            "Episode 93: reward: -96.487, steps: 9\n",
            "Episode 94: reward: -96.487, steps: 9\n",
            "Episode 95: reward: -97.528, steps: 6\n",
            "Episode 96: reward: -98.241, steps: 5\n",
            "Episode 97: reward: -97.841, steps: 5\n",
            "Episode 98: reward: -96.827, steps: 9\n",
            "Episode 99: reward: -96.637, steps: 9\n",
            "Episode 100: reward: -96.617, steps: 9\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 28 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.0005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_27 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 174s 17ms/step - reward: -19.1764\n",
            "1954 episodes - episode_reward: -98.139 [-100.142, -92.785] - loss: 228.696 - mae: 51.335 - mean_q: -14.303\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 188s 19ms/step - reward: -18.1499\n",
            "1851 episodes - episode_reward: -98.056 [-100.142, -90.946] - loss: 153.804 - mae: 65.498 - mean_q: -40.141\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 188s 19ms/step - reward: -18.7487\n",
            "done, took 549.048 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -99.074, steps: 5\n",
            "Episode 2: reward: -95.408, steps: 12\n",
            "Episode 3: reward: -98.991, steps: 5\n",
            "Episode 4: reward: -99.074, steps: 5\n",
            "Episode 5: reward: -99.074, steps: 5\n",
            "Episode 6: reward: -99.974, steps: 1\n",
            "Episode 7: reward: -99.974, steps: 1\n",
            "Episode 8: reward: -99.074, steps: 5\n",
            "Episode 9: reward: -98.190, steps: 7\n",
            "Episode 10: reward: -99.295, steps: 3\n",
            "Episode 11: reward: -99.295, steps: 3\n",
            "Episode 12: reward: -98.123, steps: 7\n",
            "Episode 13: reward: -97.006, steps: 7\n",
            "Episode 14: reward: -96.716, steps: 7\n",
            "Episode 15: reward: -98.133, steps: 6\n",
            "Episode 16: reward: -98.156, steps: 7\n",
            "Episode 17: reward: -99.008, steps: 5\n",
            "Episode 18: reward: -99.974, steps: 1\n",
            "Episode 19: reward: -99.974, steps: 1\n",
            "Episode 20: reward: -98.645, steps: 6\n",
            "Episode 21: reward: -97.920, steps: 6\n",
            "Episode 22: reward: -96.549, steps: 10\n",
            "Episode 23: reward: -98.190, steps: 7\n",
            "Episode 24: reward: -97.613, steps: 7\n",
            "Episode 25: reward: -99.974, steps: 1\n",
            "Episode 26: reward: -98.958, steps: 5\n",
            "Episode 27: reward: -98.123, steps: 7\n",
            "Episode 28: reward: -98.824, steps: 5\n",
            "Episode 29: reward: -99.074, steps: 5\n",
            "Episode 30: reward: -98.190, steps: 7\n",
            "Episode 31: reward: -99.295, steps: 3\n",
            "Episode 32: reward: -99.074, steps: 5\n",
            "Episode 33: reward: -99.974, steps: 1\n",
            "Episode 34: reward: -99.074, steps: 5\n",
            "Episode 35: reward: -99.295, steps: 3\n",
            "Episode 36: reward: -98.528, steps: 6\n",
            "Episode 37: reward: -98.073, steps: 7\n",
            "Episode 38: reward: -95.121, steps: 12\n",
            "Episode 39: reward: -97.327, steps: 8\n",
            "Episode 40: reward: -98.073, steps: 7\n",
            "Episode 41: reward: -99.974, steps: 1\n",
            "Episode 42: reward: -99.074, steps: 5\n",
            "Episode 43: reward: -96.489, steps: 10\n",
            "Episode 44: reward: -96.770, steps: 9\n",
            "Episode 45: reward: -99.295, steps: 3\n",
            "Episode 46: reward: -99.074, steps: 5\n",
            "Episode 47: reward: -96.806, steps: 7\n",
            "Episode 48: reward: -98.958, steps: 5\n",
            "Episode 49: reward: -94.221, steps: 12\n",
            "Episode 50: reward: -95.154, steps: 12\n",
            "Episode 51: reward: -99.295, steps: 3\n",
            "Episode 52: reward: -98.791, steps: 5\n",
            "Episode 53: reward: -98.341, steps: 5\n",
            "Episode 54: reward: -95.028, steps: 13\n",
            "Episode 55: reward: -97.940, steps: 7\n",
            "Episode 56: reward: -99.647, steps: 2\n",
            "Episode 57: reward: -99.295, steps: 3\n",
            "Episode 58: reward: -97.227, steps: 8\n",
            "Episode 59: reward: -98.958, steps: 5\n",
            "Episode 60: reward: -99.074, steps: 5\n",
            "Episode 61: reward: -99.074, steps: 5\n",
            "Episode 62: reward: -98.958, steps: 5\n",
            "Episode 63: reward: -98.958, steps: 5\n",
            "Episode 64: reward: -98.341, steps: 5\n",
            "Episode 65: reward: -97.011, steps: 8\n",
            "Episode 66: reward: -98.958, steps: 5\n",
            "Episode 67: reward: -99.647, steps: 2\n",
            "Episode 68: reward: -98.958, steps: 5\n",
            "Episode 69: reward: -98.791, steps: 5\n",
            "Episode 70: reward: -97.389, steps: 7\n",
            "Episode 71: reward: -99.074, steps: 5\n",
            "Episode 72: reward: -96.670, steps: 9\n",
            "Episode 73: reward: -99.295, steps: 3\n",
            "Episode 74: reward: -99.974, steps: 1\n",
            "Episode 75: reward: -99.074, steps: 5\n",
            "Episode 76: reward: -98.824, steps: 5\n",
            "Episode 77: reward: -98.190, steps: 7\n",
            "Episode 78: reward: -99.295, steps: 3\n",
            "Episode 79: reward: -98.156, steps: 7\n",
            "Episode 80: reward: -98.958, steps: 5\n",
            "Episode 81: reward: -99.974, steps: 1\n",
            "Episode 82: reward: -98.190, steps: 7\n",
            "Episode 83: reward: -97.613, steps: 7\n",
            "Episode 84: reward: -99.974, steps: 1\n",
            "Episode 85: reward: -99.295, steps: 3\n",
            "Episode 86: reward: -98.958, steps: 5\n",
            "Episode 87: reward: -99.974, steps: 1\n",
            "Episode 88: reward: -99.008, steps: 5\n",
            "Episode 89: reward: -99.074, steps: 5\n",
            "Episode 90: reward: -99.074, steps: 5\n",
            "Episode 91: reward: -98.086, steps: 6\n",
            "Episode 92: reward: -99.074, steps: 5\n",
            "Episode 93: reward: -99.295, steps: 3\n",
            "Episode 94: reward: -98.123, steps: 7\n",
            "Episode 95: reward: -98.190, steps: 7\n",
            "Episode 96: reward: -94.819, steps: 11\n",
            "Episode 97: reward: -97.920, steps: 6\n",
            "Episode 98: reward: -96.489, steps: 10\n",
            "Episode 99: reward: -96.036, steps: 11\n",
            "Episode 100: reward: -98.791, steps: 5\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 29 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_28 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,829\n",
            "Trainable params: 2,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 181s 18ms/step - reward: -20.0918\n",
            "2045 episodes - episode_reward: -98.249 [-100.142, -92.489] - loss: 206.267 - mae: 65.989 - mean_q: -48.273\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 192s 19ms/step - reward: -17.8066\n",
            "1813 episodes - episode_reward: -98.216 [-100.142, -91.948] - loss: 35.728 - mae: 86.860 - mean_q: -79.530\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 191s 19ms/step - reward: -16.6961\n",
            "done, took 563.150 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.920, steps: 9\n",
            "Episode 2: reward: -96.350, steps: 9\n",
            "Episode 3: reward: -95.888, steps: 10\n",
            "Episode 4: reward: -96.508, steps: 10\n",
            "Episode 5: reward: -96.820, steps: 9\n",
            "Episode 6: reward: -99.974, steps: 1\n",
            "Episode 7: reward: -99.278, steps: 4\n",
            "Episode 8: reward: -95.630, steps: 11\n",
            "Episode 9: reward: -96.350, steps: 9\n",
            "Episode 10: reward: -97.841, steps: 5\n",
            "Episode 11: reward: -99.295, steps: 3\n",
            "Episode 12: reward: -98.645, steps: 6\n",
            "Episode 13: reward: -99.398, steps: 4\n",
            "Episode 14: reward: -96.590, steps: 9\n",
            "Episode 15: reward: -96.620, steps: 9\n",
            "Episode 16: reward: -96.638, steps: 10\n",
            "Episode 17: reward: -99.398, steps: 4\n",
            "Episode 18: reward: -96.620, steps: 9\n",
            "Episode 19: reward: -99.398, steps: 4\n",
            "Episode 20: reward: -99.295, steps: 3\n",
            "Episode 21: reward: -96.670, steps: 9\n",
            "Episode 22: reward: -99.295, steps: 3\n",
            "Episode 23: reward: -96.188, steps: 10\n",
            "Episode 24: reward: -96.770, steps: 9\n",
            "Episode 25: reward: -96.870, steps: 9\n",
            "Episode 26: reward: -96.388, steps: 10\n",
            "Episode 27: reward: -96.490, steps: 9\n",
            "Episode 28: reward: -96.638, steps: 10\n",
            "Episode 29: reward: -99.398, steps: 4\n",
            "Episode 30: reward: -99.647, steps: 2\n",
            "Episode 31: reward: -99.974, steps: 1\n",
            "Episode 32: reward: -97.312, steps: 6\n",
            "Episode 33: reward: -99.647, steps: 2\n",
            "Episode 34: reward: -95.860, steps: 9\n",
            "Episode 35: reward: -97.462, steps: 6\n",
            "Episode 36: reward: -97.356, steps: 7\n",
            "Episode 37: reward: -99.647, steps: 2\n",
            "Episode 38: reward: -99.974, steps: 1\n",
            "Episode 39: reward: -96.238, steps: 10\n",
            "Episode 40: reward: -96.540, steps: 9\n",
            "Episode 41: reward: -98.824, steps: 5\n",
            "Episode 42: reward: -96.500, steps: 9\n",
            "Episode 43: reward: -96.690, steps: 9\n",
            "Episode 44: reward: -99.398, steps: 4\n",
            "Episode 45: reward: -99.295, steps: 3\n",
            "Episode 46: reward: -97.841, steps: 5\n",
            "Episode 47: reward: -99.295, steps: 3\n",
            "Episode 48: reward: -97.791, steps: 5\n",
            "Episode 49: reward: -95.858, steps: 10\n",
            "Episode 50: reward: -95.908, steps: 10\n",
            "Episode 51: reward: -96.640, steps: 9\n",
            "Episode 52: reward: -96.840, steps: 9\n",
            "Episode 53: reward: -99.278, steps: 4\n",
            "Episode 54: reward: -97.941, steps: 5\n",
            "Episode 55: reward: -96.690, steps: 9\n",
            "Episode 56: reward: -99.398, steps: 4\n",
            "Episode 57: reward: -96.540, steps: 9\n",
            "Episode 58: reward: -95.650, steps: 9\n",
            "Episode 59: reward: -96.747, steps: 7\n",
            "Episode 60: reward: -96.920, steps: 9\n",
            "Episode 61: reward: -97.670, steps: 9\n",
            "Episode 62: reward: -96.590, steps: 9\n",
            "Episode 63: reward: -96.640, steps: 9\n",
            "Episode 64: reward: -96.640, steps: 9\n",
            "Episode 65: reward: -96.800, steps: 9\n",
            "Episode 66: reward: -97.362, steps: 6\n",
            "Episode 67: reward: -96.970, steps: 9\n",
            "Episode 68: reward: -96.740, steps: 9\n",
            "Episode 69: reward: -96.038, steps: 10\n",
            "Episode 70: reward: -96.188, steps: 10\n",
            "Episode 71: reward: -96.740, steps: 9\n",
            "Episode 72: reward: -97.312, steps: 6\n",
            "Episode 73: reward: -97.412, steps: 6\n",
            "Episode 74: reward: -96.590, steps: 9\n",
            "Episode 75: reward: -99.278, steps: 4\n",
            "Episode 76: reward: -97.020, steps: 9\n",
            "Episode 77: reward: -96.570, steps: 9\n",
            "Episode 78: reward: -99.278, steps: 4\n",
            "Episode 79: reward: -96.750, steps: 9\n",
            "Episode 80: reward: -99.278, steps: 4\n",
            "Episode 81: reward: -99.295, steps: 3\n",
            "Episode 82: reward: -96.288, steps: 10\n",
            "Episode 83: reward: -99.398, steps: 4\n",
            "Episode 84: reward: -97.791, steps: 5\n",
            "Episode 85: reward: -96.890, steps: 9\n",
            "Episode 86: reward: -98.258, steps: 4\n",
            "Episode 87: reward: -95.520, steps: 11\n",
            "Episode 88: reward: -98.882, steps: 6\n",
            "Episode 89: reward: -97.841, steps: 5\n",
            "Episode 90: reward: -98.546, steps: 7\n",
            "Episode 91: reward: -96.138, steps: 10\n",
            "Episode 92: reward: -99.974, steps: 1\n",
            "Episode 93: reward: -99.647, steps: 2\n",
            "Episode 94: reward: -98.458, steps: 4\n",
            "Episode 95: reward: -99.974, steps: 1\n",
            "Episode 96: reward: -99.008, steps: 5\n",
            "Episode 97: reward: -99.008, steps: 5\n",
            "Episode 98: reward: -98.824, steps: 5\n",
            "Episode 99: reward: -98.378, steps: 4\n",
            "Episode 100: reward: -96.750, steps: 9\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 30 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005, 'adam_epsilon': 0.1, 'batch_size': 32, 'target_model_update': 0.001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_29\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_29 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 187s 19ms/step - reward: -19.8841\n",
            "2026 episodes - episode_reward: -98.145 [-100.142, -94.350] - loss: 187.909 - mae: 60.319 - mean_q: -39.192\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 199s 20ms/step - reward: -18.7227\n",
            "1908 episodes - episode_reward: -98.127 [-100.142, -93.106] - loss: 22.594 - mae: 88.897 - mean_q: -83.455\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 196s 20ms/step - reward: -16.3206\n",
            "done, took 583.365 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.662, steps: 6\n",
            "Episode 2: reward: -96.520, steps: 9\n",
            "Episode 3: reward: -98.791, steps: 5\n",
            "Episode 4: reward: -98.191, steps: 5\n",
            "Episode 5: reward: -96.290, steps: 9\n",
            "Episode 6: reward: -97.256, steps: 7\n",
            "Episode 7: reward: -96.420, steps: 9\n",
            "Episode 8: reward: -97.506, steps: 7\n",
            "Episode 9: reward: -99.974, steps: 1\n",
            "Episode 10: reward: -97.562, steps: 6\n",
            "Episode 11: reward: -96.540, steps: 9\n",
            "Episode 12: reward: -98.012, steps: 6\n",
            "Episode 13: reward: -96.320, steps: 9\n",
            "Episode 14: reward: -99.974, steps: 1\n",
            "Episode 15: reward: -97.762, steps: 6\n",
            "Episode 16: reward: -97.206, steps: 7\n",
            "Episode 17: reward: -97.612, steps: 6\n",
            "Episode 18: reward: -97.662, steps: 6\n",
            "Episode 19: reward: -98.091, steps: 5\n",
            "Episode 20: reward: -96.620, steps: 9\n",
            "Episode 21: reward: -97.812, steps: 6\n",
            "Episode 22: reward: -97.862, steps: 6\n",
            "Episode 23: reward: -96.135, steps: 10\n",
            "Episode 24: reward: -99.974, steps: 1\n",
            "Episode 25: reward: -96.090, steps: 9\n",
            "Episode 26: reward: -96.440, steps: 9\n",
            "Episode 27: reward: -97.512, steps: 6\n",
            "Episode 28: reward: -97.006, steps: 7\n",
            "Episode 29: reward: -96.570, steps: 9\n",
            "Episode 30: reward: -98.791, steps: 5\n",
            "Episode 31: reward: -97.862, steps: 6\n",
            "Episode 32: reward: -96.870, steps: 9\n",
            "Episode 33: reward: -97.562, steps: 6\n",
            "Episode 34: reward: -97.891, steps: 5\n",
            "Episode 35: reward: -97.312, steps: 6\n",
            "Episode 36: reward: -96.470, steps: 9\n",
            "Episode 37: reward: -97.941, steps: 5\n",
            "Episode 38: reward: -96.370, steps: 9\n",
            "Episode 39: reward: -96.770, steps: 9\n",
            "Episode 40: reward: -98.791, steps: 5\n",
            "Episode 41: reward: -95.628, steps: 10\n",
            "Episode 42: reward: -97.106, steps: 7\n",
            "Episode 43: reward: -96.390, steps: 9\n",
            "Episode 44: reward: -97.991, steps: 5\n",
            "Episode 45: reward: -98.062, steps: 6\n",
            "Episode 46: reward: -97.456, steps: 7\n",
            "Episode 47: reward: -97.662, steps: 6\n",
            "Episode 48: reward: -97.662, steps: 6\n",
            "Episode 49: reward: -97.562, steps: 6\n",
            "Episode 50: reward: -97.912, steps: 6\n",
            "Episode 51: reward: -97.612, steps: 6\n",
            "Episode 52: reward: -97.562, steps: 6\n",
            "Episode 53: reward: -98.791, steps: 5\n",
            "Episode 54: reward: -97.891, steps: 5\n",
            "Episode 55: reward: -96.590, steps: 9\n",
            "Episode 56: reward: -97.006, steps: 7\n",
            "Episode 57: reward: -98.041, steps: 5\n",
            "Episode 58: reward: -96.770, steps: 9\n",
            "Episode 59: reward: -97.612, steps: 6\n",
            "Episode 60: reward: -97.612, steps: 6\n",
            "Episode 61: reward: -97.712, steps: 6\n",
            "Episode 62: reward: -97.562, steps: 6\n",
            "Episode 63: reward: -98.041, steps: 5\n",
            "Episode 64: reward: -97.056, steps: 7\n",
            "Episode 65: reward: -96.720, steps: 9\n",
            "Episode 66: reward: -97.206, steps: 7\n",
            "Episode 67: reward: -97.991, steps: 5\n",
            "Episode 68: reward: -96.540, steps: 9\n",
            "Episode 69: reward: -98.091, steps: 5\n",
            "Episode 70: reward: -99.974, steps: 1\n",
            "Episode 71: reward: -96.540, steps: 9\n",
            "Episode 72: reward: -98.041, steps: 5\n",
            "Episode 73: reward: -98.141, steps: 5\n",
            "Episode 74: reward: -97.912, steps: 6\n",
            "Episode 75: reward: -97.256, steps: 7\n",
            "Episode 76: reward: -96.770, steps: 9\n",
            "Episode 77: reward: -97.712, steps: 6\n",
            "Episode 78: reward: -95.205, steps: 10\n",
            "Episode 79: reward: -97.662, steps: 6\n",
            "Episode 80: reward: -97.812, steps: 6\n",
            "Episode 81: reward: -97.306, steps: 7\n",
            "Episode 82: reward: -97.512, steps: 6\n",
            "Episode 83: reward: -97.991, steps: 5\n",
            "Episode 84: reward: -97.462, steps: 6\n",
            "Episode 85: reward: -97.812, steps: 6\n",
            "Episode 86: reward: -97.941, steps: 5\n",
            "Episode 87: reward: -97.512, steps: 6\n",
            "Episode 88: reward: -98.191, steps: 5\n",
            "Episode 89: reward: -99.974, steps: 1\n",
            "Episode 90: reward: -96.440, steps: 9\n",
            "Episode 91: reward: -97.606, steps: 7\n",
            "Episode 92: reward: -97.562, steps: 6\n",
            "Episode 93: reward: -96.720, steps: 9\n",
            "Episode 94: reward: -96.240, steps: 9\n",
            "Episode 95: reward: -97.412, steps: 6\n",
            "Episode 96: reward: -99.974, steps: 1\n",
            "Episode 97: reward: -96.490, steps: 9\n",
            "Episode 98: reward: -98.991, steps: 5\n",
            "Episode 99: reward: -98.791, steps: 5\n",
            "Episode 100: reward: -97.712, steps: 6\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 31 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.001, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_30 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 16)                672       \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,437\n",
            "Trainable params: 1,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 185s 18ms/step - reward: -20.5936\n",
            "2096 episodes - episode_reward: -98.252 [-100.142, -94.075] - loss: 267.034 - mae: 56.230 - mean_q: -36.820\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 201s 20ms/step - reward: -20.4941\n",
            "2087 episodes - episode_reward: -98.199 [-100.142, -93.945] - loss: 43.836 - mae: 87.233 - mean_q: -80.831\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 201s 20ms/step - reward: -19.7283\n",
            "done, took 587.447 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.841, steps: 5\n",
            "Episode 2: reward: -98.362, steps: 6\n",
            "Episode 3: reward: -97.741, steps: 5\n",
            "Episode 4: reward: -98.395, steps: 6\n",
            "Episode 5: reward: -97.612, steps: 6\n",
            "Episode 6: reward: -99.295, steps: 3\n",
            "Episode 7: reward: -97.512, steps: 6\n",
            "Episode 8: reward: -94.665, steps: 12\n",
            "Episode 9: reward: -97.791, steps: 5\n",
            "Episode 10: reward: -97.791, steps: 5\n",
            "Episode 11: reward: -97.462, steps: 6\n",
            "Episode 12: reward: -97.812, steps: 6\n",
            "Episode 13: reward: -97.591, steps: 5\n",
            "Episode 14: reward: -96.432, steps: 9\n",
            "Episode 15: reward: -96.441, steps: 8\n",
            "Episode 16: reward: -99.647, steps: 2\n",
            "Episode 17: reward: -98.716, steps: 3\n",
            "Episode 18: reward: -96.207, steps: 9\n",
            "Episode 19: reward: -98.578, steps: 6\n",
            "Episode 20: reward: -95.618, steps: 10\n",
            "Episode 21: reward: -97.512, steps: 6\n",
            "Episode 22: reward: -95.880, steps: 9\n",
            "Episode 23: reward: -99.647, steps: 2\n",
            "Episode 24: reward: -96.641, steps: 9\n",
            "Episode 25: reward: -99.295, steps: 3\n",
            "Episode 26: reward: -99.647, steps: 2\n",
            "Episode 27: reward: -98.578, steps: 6\n",
            "Episode 28: reward: -97.641, steps: 5\n",
            "Episode 29: reward: -96.915, steps: 8\n",
            "Episode 30: reward: -99.647, steps: 2\n",
            "Episode 31: reward: -99.763, steps: 3\n",
            "Episode 32: reward: -98.395, steps: 6\n",
            "Episode 33: reward: -98.243, steps: 4\n",
            "Episode 34: reward: -98.395, steps: 6\n",
            "Episode 35: reward: -97.412, steps: 6\n",
            "Episode 36: reward: -99.295, steps: 3\n",
            "Episode 37: reward: -97.741, steps: 5\n",
            "Episode 38: reward: -98.578, steps: 6\n",
            "Episode 39: reward: -99.763, steps: 3\n",
            "Episode 40: reward: -96.308, steps: 9\n",
            "Episode 41: reward: -99.647, steps: 2\n",
            "Episode 42: reward: -97.562, steps: 6\n",
            "Episode 43: reward: -95.994, steps: 9\n",
            "Episode 44: reward: -98.056, steps: 4\n",
            "Episode 45: reward: -99.647, steps: 2\n",
            "Episode 46: reward: -98.578, steps: 6\n",
            "Episode 47: reward: -99.647, steps: 2\n",
            "Episode 48: reward: -97.841, steps: 5\n",
            "Episode 49: reward: -96.208, steps: 9\n",
            "Episode 50: reward: -97.562, steps: 6\n",
            "Episode 51: reward: -99.647, steps: 2\n",
            "Episode 52: reward: -96.621, steps: 8\n",
            "Episode 53: reward: -99.763, steps: 3\n",
            "Episode 54: reward: -97.791, steps: 5\n",
            "Episode 55: reward: -99.647, steps: 2\n",
            "Episode 56: reward: -96.118, steps: 10\n",
            "Episode 57: reward: -97.741, steps: 5\n",
            "Episode 58: reward: -97.841, steps: 5\n",
            "Episode 59: reward: -99.647, steps: 2\n",
            "Episode 60: reward: -99.763, steps: 3\n",
            "Episode 61: reward: -97.841, steps: 5\n",
            "Episode 62: reward: -97.812, steps: 6\n",
            "Episode 63: reward: -98.578, steps: 6\n",
            "Episode 64: reward: -96.641, steps: 8\n",
            "Episode 65: reward: -97.741, steps: 5\n",
            "Episode 66: reward: -98.395, steps: 6\n",
            "Episode 67: reward: -97.362, steps: 6\n",
            "Episode 68: reward: -97.641, steps: 5\n",
            "Episode 69: reward: -99.647, steps: 2\n",
            "Episode 70: reward: -98.578, steps: 6\n",
            "Episode 71: reward: -99.647, steps: 2\n",
            "Episode 72: reward: -96.521, steps: 8\n",
            "Episode 73: reward: -97.741, steps: 5\n",
            "Episode 74: reward: -99.295, steps: 3\n",
            "Episode 75: reward: -97.641, steps: 5\n",
            "Episode 76: reward: -97.841, steps: 5\n",
            "Episode 77: reward: -97.741, steps: 5\n",
            "Episode 78: reward: -97.462, steps: 6\n",
            "Episode 79: reward: -99.385, steps: 4\n",
            "Episode 80: reward: -97.712, steps: 6\n",
            "Episode 81: reward: -99.756, steps: 3\n",
            "Episode 82: reward: -99.647, steps: 2\n",
            "Episode 83: reward: -97.712, steps: 6\n",
            "Episode 84: reward: -96.374, steps: 9\n",
            "Episode 85: reward: -96.133, steps: 9\n",
            "Episode 86: reward: -96.541, steps: 9\n",
            "Episode 87: reward: -97.512, steps: 6\n",
            "Episode 88: reward: -97.841, steps: 5\n",
            "Episode 89: reward: -96.988, steps: 8\n",
            "Episode 90: reward: -98.562, steps: 6\n",
            "Episode 91: reward: -97.712, steps: 6\n",
            "Episode 92: reward: -97.862, steps: 6\n",
            "Episode 93: reward: -98.578, steps: 6\n",
            "Episode 94: reward: -99.647, steps: 2\n",
            "Episode 95: reward: -96.564, steps: 9\n",
            "Episode 96: reward: -96.441, steps: 9\n",
            "Episode 97: reward: -96.741, steps: 9\n",
            "Episode 98: reward: -97.791, steps: 5\n",
            "Episode 99: reward: -99.647, steps: 2\n",
            "Episode 100: reward: -99.647, steps: 2\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 32 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_31 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 16)                672       \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,709\n",
            "Trainable params: 1,709\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 197s 20ms/step - reward: -22.3271\n",
            "2267 episodes - episode_reward: -98.488 [-100.142, -91.833] - loss: 87.095 - mae: 80.655 - mean_q: -72.675\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 198s 20ms/step - reward: -14.7405\n",
            "1514 episodes - episode_reward: -97.360 [-100.135, -93.063] - loss: 7.079 - mae: 90.821 - mean_q: -87.929\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 197s 20ms/step - reward: -14.0431\n",
            "done, took 591.616 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -95.511, steps: 10\n",
            "Episode 2: reward: -97.512, steps: 6\n",
            "Episode 3: reward: -98.458, steps: 4\n",
            "Episode 4: reward: -97.712, steps: 6\n",
            "Episode 5: reward: -97.712, steps: 6\n",
            "Episode 6: reward: -95.881, steps: 10\n",
            "Episode 7: reward: -98.618, steps: 4\n",
            "Episode 8: reward: -96.325, steps: 10\n",
            "Episode 9: reward: -95.951, steps: 10\n",
            "Episode 10: reward: -96.071, steps: 10\n",
            "Episode 11: reward: -97.562, steps: 6\n",
            "Episode 12: reward: -97.306, steps: 7\n",
            "Episode 13: reward: -97.512, steps: 6\n",
            "Episode 14: reward: -95.851, steps: 10\n",
            "Episode 15: reward: -95.945, steps: 10\n",
            "Episode 16: reward: -96.141, steps: 10\n",
            "Episode 17: reward: -98.458, steps: 4\n",
            "Episode 18: reward: -98.458, steps: 4\n",
            "Episode 19: reward: -97.762, steps: 6\n",
            "Episode 20: reward: -95.888, steps: 10\n",
            "Episode 21: reward: -98.538, steps: 4\n",
            "Episode 22: reward: -95.704, steps: 11\n",
            "Episode 23: reward: -97.762, steps: 6\n",
            "Episode 24: reward: -97.506, steps: 7\n",
            "Episode 25: reward: -96.071, steps: 10\n",
            "Episode 26: reward: -98.418, steps: 4\n",
            "Episode 27: reward: -97.562, steps: 6\n",
            "Episode 28: reward: -95.818, steps: 10\n",
            "Episode 29: reward: -97.512, steps: 6\n",
            "Episode 30: reward: -97.462, steps: 6\n",
            "Episode 31: reward: -97.512, steps: 6\n",
            "Episode 32: reward: -95.818, steps: 10\n",
            "Episode 33: reward: -97.862, steps: 6\n",
            "Episode 34: reward: -97.462, steps: 6\n",
            "Episode 35: reward: -97.912, steps: 6\n",
            "Episode 36: reward: -97.712, steps: 6\n",
            "Episode 37: reward: -96.020, steps: 11\n",
            "Episode 38: reward: -97.612, steps: 6\n",
            "Episode 39: reward: -97.312, steps: 6\n",
            "Episode 40: reward: -95.818, steps: 10\n",
            "Episode 41: reward: -95.945, steps: 10\n",
            "Episode 42: reward: -94.958, steps: 10\n",
            "Episode 43: reward: -98.338, steps: 4\n",
            "Episode 44: reward: -97.812, steps: 6\n",
            "Episode 45: reward: -97.262, steps: 6\n",
            "Episode 46: reward: -97.412, steps: 6\n",
            "Episode 47: reward: -95.725, steps: 10\n",
            "Episode 48: reward: -98.458, steps: 4\n",
            "Episode 49: reward: -96.451, steps: 10\n",
            "Episode 50: reward: -97.712, steps: 6\n",
            "Episode 51: reward: -97.256, steps: 7\n",
            "Episode 52: reward: -97.512, steps: 6\n",
            "Episode 53: reward: -97.662, steps: 6\n",
            "Episode 54: reward: -97.662, steps: 6\n",
            "Episode 55: reward: -97.462, steps: 6\n",
            "Episode 56: reward: -96.078, steps: 10\n",
            "Episode 57: reward: -97.262, steps: 6\n",
            "Episode 58: reward: -97.662, steps: 6\n",
            "Episode 59: reward: -96.135, steps: 10\n",
            "Episode 60: reward: -97.912, steps: 6\n",
            "Episode 61: reward: -95.691, steps: 10\n",
            "Episode 62: reward: -97.356, steps: 7\n",
            "Episode 63: reward: -97.712, steps: 6\n",
            "Episode 64: reward: -96.261, steps: 10\n",
            "Episode 65: reward: -97.662, steps: 6\n",
            "Episode 66: reward: -95.508, steps: 10\n",
            "Episode 67: reward: -97.312, steps: 6\n",
            "Episode 68: reward: -98.498, steps: 4\n",
            "Episode 69: reward: -95.851, steps: 10\n",
            "Episode 70: reward: -95.945, steps: 10\n",
            "Episode 71: reward: -96.135, steps: 10\n",
            "Episode 72: reward: -97.612, steps: 6\n",
            "Episode 73: reward: -95.881, steps: 10\n",
            "Episode 74: reward: -95.915, steps: 10\n",
            "Episode 75: reward: -96.135, steps: 10\n",
            "Episode 76: reward: -97.612, steps: 6\n",
            "Episode 77: reward: -96.388, steps: 10\n",
            "Episode 78: reward: -97.462, steps: 6\n",
            "Episode 79: reward: -97.162, steps: 6\n",
            "Episode 80: reward: -98.178, steps: 4\n",
            "Episode 81: reward: -97.612, steps: 6\n",
            "Episode 82: reward: -97.562, steps: 6\n",
            "Episode 83: reward: -95.945, steps: 10\n",
            "Episode 84: reward: -97.612, steps: 6\n",
            "Episode 85: reward: -95.881, steps: 10\n",
            "Episode 86: reward: -95.725, steps: 10\n",
            "Episode 87: reward: -95.501, steps: 10\n",
            "Episode 88: reward: -97.406, steps: 7\n",
            "Episode 89: reward: -97.812, steps: 6\n",
            "Episode 90: reward: -95.978, steps: 10\n",
            "Episode 91: reward: -97.562, steps: 6\n",
            "Episode 92: reward: -95.818, steps: 10\n",
            "Episode 93: reward: -98.538, steps: 4\n",
            "Episode 94: reward: -97.362, steps: 6\n",
            "Episode 95: reward: -95.691, steps: 10\n",
            "Episode 96: reward: -98.418, steps: 4\n",
            "Episode 97: reward: -97.662, steps: 6\n",
            "Episode 98: reward: -98.178, steps: 4\n",
            "Episode 99: reward: -95.945, steps: 10\n",
            "Episode 100: reward: -96.071, steps: 10\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 33 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005, 'adam_epsilon': 1e-05, 'batch_size': 32, 'target_model_update': 0.005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_32 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 16)                672       \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,437\n",
            "Trainable params: 1,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 202s 20ms/step - reward: -25.0675\n",
            "2539 episodes - episode_reward: -98.731 [-100.142, -95.158] - loss: 209.010 - mae: 74.609 - mean_q: -68.396\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 215s 22ms/step - reward: -20.7957\n",
            "2117 episodes - episode_reward: -98.232 [-100.142, -95.280] - loss: 42.990 - mae: 91.989 - mean_q: -88.254\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 215s 22ms/step - reward: -20.8721\n",
            "done, took 633.466 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -99.295, steps: 3\n",
            "Episode 2: reward: -97.891, steps: 5\n",
            "Episode 3: reward: -97.841, steps: 5\n",
            "Episode 4: reward: -97.791, steps: 5\n",
            "Episode 5: reward: -97.741, steps: 5\n",
            "Episode 6: reward: -97.462, steps: 6\n",
            "Episode 7: reward: -97.941, steps: 5\n",
            "Episode 8: reward: -97.712, steps: 6\n",
            "Episode 9: reward: -96.447, steps: 11\n",
            "Episode 10: reward: -97.991, steps: 5\n",
            "Episode 11: reward: -97.591, steps: 5\n",
            "Episode 12: reward: -98.123, steps: 7\n",
            "Episode 13: reward: -98.156, steps: 7\n",
            "Episode 14: reward: -97.891, steps: 5\n",
            "Episode 15: reward: -97.691, steps: 5\n",
            "Episode 16: reward: -97.741, steps: 5\n",
            "Episode 17: reward: -97.362, steps: 6\n",
            "Episode 18: reward: -97.353, steps: 9\n",
            "Episode 19: reward: -97.941, steps: 5\n",
            "Episode 20: reward: -99.295, steps: 3\n",
            "Episode 21: reward: -98.006, steps: 7\n",
            "Episode 22: reward: -99.251, steps: 4\n",
            "Episode 23: reward: -98.123, steps: 7\n",
            "Episode 24: reward: -97.941, steps: 5\n",
            "Episode 25: reward: -97.941, steps: 5\n",
            "Episode 26: reward: -99.295, steps: 3\n",
            "Episode 27: reward: -97.941, steps: 5\n",
            "Episode 28: reward: -99.295, steps: 3\n",
            "Episode 29: reward: -97.991, steps: 5\n",
            "Episode 30: reward: -96.482, steps: 9\n",
            "Episode 31: reward: -97.941, steps: 5\n",
            "Episode 32: reward: -99.295, steps: 3\n",
            "Episode 33: reward: -99.295, steps: 3\n",
            "Episode 34: reward: -96.234, steps: 10\n",
            "Episode 35: reward: -99.295, steps: 3\n",
            "Episode 36: reward: -99.295, steps: 3\n",
            "Episode 37: reward: -97.791, steps: 5\n",
            "Episode 38: reward: -99.295, steps: 3\n",
            "Episode 39: reward: -99.385, steps: 4\n",
            "Episode 40: reward: -98.612, steps: 6\n",
            "Episode 41: reward: -97.841, steps: 5\n",
            "Episode 42: reward: -99.295, steps: 3\n",
            "Episode 43: reward: -97.791, steps: 5\n",
            "Episode 44: reward: -97.941, steps: 5\n",
            "Episode 45: reward: -97.741, steps: 5\n",
            "Episode 46: reward: -99.385, steps: 4\n",
            "Episode 47: reward: -97.891, steps: 5\n",
            "Episode 48: reward: -97.512, steps: 6\n",
            "Episode 49: reward: -99.295, steps: 3\n",
            "Episode 50: reward: -97.712, steps: 6\n",
            "Episode 51: reward: -99.295, steps: 3\n",
            "Episode 52: reward: -99.295, steps: 3\n",
            "Episode 53: reward: -98.123, steps: 7\n",
            "Episode 54: reward: -97.727, steps: 9\n",
            "Episode 55: reward: -97.940, steps: 7\n",
            "Episode 56: reward: -99.295, steps: 3\n",
            "Episode 57: reward: -97.346, steps: 7\n",
            "Episode 58: reward: -97.991, steps: 5\n",
            "Episode 59: reward: -97.301, steps: 8\n",
            "Episode 60: reward: -97.841, steps: 5\n",
            "Episode 61: reward: -97.641, steps: 5\n",
            "Episode 62: reward: -99.295, steps: 3\n",
            "Episode 63: reward: -99.278, steps: 4\n",
            "Episode 64: reward: -99.295, steps: 3\n",
            "Episode 65: reward: -97.362, steps: 6\n",
            "Episode 66: reward: -96.212, steps: 10\n",
            "Episode 67: reward: -97.005, steps: 10\n",
            "Episode 68: reward: -97.462, steps: 6\n",
            "Episode 69: reward: -97.841, steps: 5\n",
            "Episode 70: reward: -99.295, steps: 3\n",
            "Episode 71: reward: -97.791, steps: 5\n",
            "Episode 72: reward: -99.398, steps: 4\n",
            "Episode 73: reward: -97.741, steps: 5\n",
            "Episode 74: reward: -97.741, steps: 5\n",
            "Episode 75: reward: -97.941, steps: 5\n",
            "Episode 76: reward: -99.295, steps: 3\n",
            "Episode 77: reward: -99.647, steps: 2\n",
            "Episode 78: reward: -98.123, steps: 7\n",
            "Episode 79: reward: -97.612, steps: 6\n",
            "Episode 80: reward: -99.295, steps: 3\n",
            "Episode 81: reward: -97.612, steps: 6\n",
            "Episode 82: reward: -97.741, steps: 5\n",
            "Episode 83: reward: -98.123, steps: 7\n",
            "Episode 84: reward: -99.295, steps: 3\n",
            "Episode 85: reward: -97.362, steps: 6\n",
            "Episode 86: reward: -99.295, steps: 3\n",
            "Episode 87: reward: -99.295, steps: 3\n",
            "Episode 88: reward: -97.941, steps: 5\n",
            "Episode 89: reward: -97.841, steps: 5\n",
            "Episode 90: reward: -99.295, steps: 3\n",
            "Episode 91: reward: -97.412, steps: 6\n",
            "Episode 92: reward: -97.941, steps: 5\n",
            "Episode 93: reward: -97.612, steps: 6\n",
            "Episode 94: reward: -97.362, steps: 6\n",
            "Episode 95: reward: -97.841, steps: 5\n",
            "Episode 96: reward: -97.940, steps: 7\n",
            "Episode 97: reward: -97.662, steps: 6\n",
            "Episode 98: reward: -97.891, steps: 5\n",
            "Episode 99: reward: -99.295, steps: 3\n",
            "Episode 100: reward: -97.841, steps: 5\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 34 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.005, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_33\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_33 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 195s 19ms/step - reward: -18.0194\n",
            "1838 episodes - episode_reward: -98.038 [-100.122, -93.809] - loss: 105.976 - mae: 63.217 - mean_q: -35.891\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 205s 20ms/step - reward: -14.5475\n",
            "1491 episodes - episode_reward: -97.570 [-100.142, -92.320] - loss: 33.266 - mae: 79.897 - mean_q: -66.915\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 202s 20ms/step - reward: -12.9423\n",
            "done, took 601.486 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.306, steps: 7\n",
            "Episode 2: reward: -95.460, steps: 11\n",
            "Episode 3: reward: -97.106, steps: 7\n",
            "Episode 4: reward: -95.547, steps: 11\n",
            "Episode 5: reward: -95.737, steps: 11\n",
            "Episode 6: reward: -95.517, steps: 11\n",
            "Episode 7: reward: -95.210, steps: 11\n",
            "Episode 8: reward: -95.074, steps: 11\n",
            "Episode 9: reward: -95.654, steps: 11\n",
            "Episode 10: reward: -97.306, steps: 7\n",
            "Episode 11: reward: -94.660, steps: 11\n",
            "Episode 12: reward: -97.206, steps: 7\n",
            "Episode 13: reward: -96.956, steps: 7\n",
            "Episode 14: reward: -95.464, steps: 11\n",
            "Episode 15: reward: -97.156, steps: 7\n",
            "Episode 16: reward: -95.327, steps: 11\n",
            "Episode 17: reward: -95.137, steps: 11\n",
            "Episode 18: reward: -96.656, steps: 7\n",
            "Episode 19: reward: -95.390, steps: 11\n",
            "Episode 20: reward: -95.264, steps: 11\n",
            "Episode 21: reward: -97.056, steps: 7\n",
            "Episode 22: reward: -95.084, steps: 11\n",
            "Episode 23: reward: -99.295, steps: 3\n",
            "Episode 24: reward: -95.610, steps: 11\n",
            "Episode 25: reward: -96.956, steps: 7\n",
            "Episode 26: reward: -95.230, steps: 11\n",
            "Episode 27: reward: -98.698, steps: 4\n",
            "Episode 28: reward: -95.610, steps: 11\n",
            "Episode 29: reward: -97.056, steps: 7\n",
            "Episode 30: reward: -97.456, steps: 7\n",
            "Episode 31: reward: -95.644, steps: 11\n",
            "Episode 32: reward: -97.056, steps: 7\n",
            "Episode 33: reward: -96.956, steps: 7\n",
            "Episode 34: reward: -95.420, steps: 11\n",
            "Episode 35: reward: -97.406, steps: 7\n",
            "Episode 36: reward: -95.357, steps: 11\n",
            "Episode 37: reward: -95.420, steps: 11\n",
            "Episode 38: reward: -95.580, steps: 11\n",
            "Episode 39: reward: -97.006, steps: 7\n",
            "Episode 40: reward: -95.137, steps: 11\n",
            "Episode 41: reward: -95.927, steps: 11\n",
            "Episode 42: reward: -96.906, steps: 7\n",
            "Episode 43: reward: -95.610, steps: 11\n",
            "Episode 44: reward: -95.037, steps: 11\n",
            "Episode 45: reward: -95.327, steps: 11\n",
            "Episode 46: reward: -96.360, steps: 9\n",
            "Episode 47: reward: -95.190, steps: 11\n",
            "Episode 48: reward: -95.200, steps: 11\n",
            "Episode 49: reward: -94.884, steps: 11\n",
            "Episode 50: reward: -95.264, steps: 11\n",
            "Episode 51: reward: -94.087, steps: 11\n",
            "Episode 52: reward: -96.590, steps: 9\n",
            "Episode 53: reward: -95.274, steps: 11\n",
            "Episode 54: reward: -95.674, steps: 11\n",
            "Episode 55: reward: -95.524, steps: 11\n",
            "Episode 56: reward: -95.800, steps: 11\n",
            "Episode 57: reward: -95.084, steps: 11\n",
            "Episode 58: reward: -95.274, steps: 11\n",
            "Episode 59: reward: -97.206, steps: 7\n",
            "Episode 60: reward: -95.547, steps: 11\n",
            "Episode 61: reward: -94.404, steps: 11\n",
            "Episode 62: reward: -96.956, steps: 7\n",
            "Episode 63: reward: -96.340, steps: 9\n",
            "Episode 64: reward: -99.074, steps: 5\n",
            "Episode 65: reward: -95.707, steps: 11\n",
            "Episode 66: reward: -97.206, steps: 7\n",
            "Episode 67: reward: -95.200, steps: 11\n",
            "Episode 68: reward: -95.610, steps: 11\n",
            "Episode 69: reward: -99.074, steps: 5\n",
            "Episode 70: reward: -97.306, steps: 7\n",
            "Episode 71: reward: -94.594, steps: 11\n",
            "Episode 72: reward: -96.856, steps: 7\n",
            "Episode 73: reward: -95.327, steps: 11\n",
            "Episode 74: reward: -98.958, steps: 5\n",
            "Episode 75: reward: -95.337, steps: 11\n",
            "Episode 76: reward: -95.390, steps: 11\n",
            "Episode 77: reward: -98.177, steps: 8\n",
            "Episode 78: reward: -95.357, steps: 11\n",
            "Episode 79: reward: -95.864, steps: 11\n",
            "Episode 80: reward: -95.144, steps: 11\n",
            "Episode 81: reward: -94.720, steps: 11\n",
            "Episode 82: reward: -95.547, steps: 11\n",
            "Episode 83: reward: -97.006, steps: 7\n",
            "Episode 84: reward: -94.884, steps: 11\n",
            "Episode 85: reward: -96.756, steps: 7\n",
            "Episode 86: reward: -95.084, steps: 11\n",
            "Episode 87: reward: -95.337, steps: 11\n",
            "Episode 88: reward: -97.040, steps: 9\n",
            "Episode 89: reward: -95.207, steps: 11\n",
            "Episode 90: reward: -95.270, steps: 11\n",
            "Episode 91: reward: -95.547, steps: 11\n",
            "Episode 92: reward: -94.684, steps: 11\n",
            "Episode 93: reward: -98.958, steps: 5\n",
            "Episode 94: reward: -95.484, steps: 11\n",
            "Episode 95: reward: -95.327, steps: 11\n",
            "Episode 96: reward: -96.956, steps: 7\n",
            "Episode 97: reward: -95.274, steps: 11\n",
            "Episode 98: reward: -95.337, steps: 11\n",
            "Episode 99: reward: -95.864, steps: 11\n",
            "Episode 100: reward: -95.327, steps: 11\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 35 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.0005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_34 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 16)                784       \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,565\n",
            "Trainable params: 3,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 200s 20ms/step - reward: -17.5310\n",
            "1790 episodes - episode_reward: -97.940 [-100.115, -93.330] - loss: 170.237 - mae: 49.902 - mean_q: -16.522\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 208s 21ms/step - reward: -12.5371\n",
            "1294 episodes - episode_reward: -96.884 [-99.974, -92.113] - loss: 58.943 - mae: 68.982 - mean_q: -51.027\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 211s 21ms/step - reward: -13.5190\n",
            "done, took 618.970 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.306, steps: 7\n",
            "Episode 2: reward: -97.906, steps: 7\n",
            "Episode 3: reward: -98.106, steps: 7\n",
            "Episode 4: reward: -98.106, steps: 7\n",
            "Episode 5: reward: -95.394, steps: 11\n",
            "Episode 6: reward: -98.106, steps: 7\n",
            "Episode 7: reward: -98.106, steps: 7\n",
            "Episode 8: reward: -99.378, steps: 4\n",
            "Episode 9: reward: -98.106, steps: 7\n",
            "Episode 10: reward: -96.547, steps: 11\n",
            "Episode 11: reward: -99.295, steps: 3\n",
            "Episode 12: reward: -96.397, steps: 11\n",
            "Episode 13: reward: -99.295, steps: 3\n",
            "Episode 14: reward: -97.456, steps: 7\n",
            "Episode 15: reward: -99.295, steps: 3\n",
            "Episode 16: reward: -95.680, steps: 11\n",
            "Episode 17: reward: -98.106, steps: 7\n",
            "Episode 18: reward: -99.295, steps: 3\n",
            "Episode 19: reward: -99.378, steps: 4\n",
            "Episode 20: reward: -97.906, steps: 7\n",
            "Episode 21: reward: -97.906, steps: 7\n",
            "Episode 22: reward: -99.295, steps: 3\n",
            "Episode 23: reward: -97.906, steps: 7\n",
            "Episode 24: reward: -99.295, steps: 3\n",
            "Episode 25: reward: -99.295, steps: 3\n",
            "Episode 26: reward: -98.106, steps: 7\n",
            "Episode 27: reward: -97.366, steps: 8\n",
            "Episode 28: reward: -99.295, steps: 3\n",
            "Episode 29: reward: -97.206, steps: 7\n",
            "Episode 30: reward: -97.206, steps: 7\n",
            "Episode 31: reward: -98.106, steps: 7\n",
            "Episode 32: reward: -97.906, steps: 7\n",
            "Episode 33: reward: -99.295, steps: 3\n",
            "Episode 34: reward: -98.106, steps: 7\n",
            "Episode 35: reward: -95.387, steps: 11\n",
            "Episode 36: reward: -99.295, steps: 3\n",
            "Episode 37: reward: -98.106, steps: 7\n",
            "Episode 38: reward: -98.106, steps: 7\n",
            "Episode 39: reward: -97.406, steps: 7\n",
            "Episode 40: reward: -99.295, steps: 3\n",
            "Episode 41: reward: -95.330, steps: 11\n",
            "Episode 42: reward: -98.106, steps: 7\n",
            "Episode 43: reward: -96.397, steps: 11\n",
            "Episode 44: reward: -97.306, steps: 7\n",
            "Episode 45: reward: -99.295, steps: 3\n",
            "Episode 46: reward: -97.906, steps: 7\n",
            "Episode 47: reward: -99.378, steps: 4\n",
            "Episode 48: reward: -97.456, steps: 7\n",
            "Episode 49: reward: -99.295, steps: 3\n",
            "Episode 50: reward: -97.906, steps: 7\n",
            "Episode 51: reward: -99.378, steps: 4\n",
            "Episode 52: reward: -99.295, steps: 3\n",
            "Episode 53: reward: -99.295, steps: 3\n",
            "Episode 54: reward: -95.647, steps: 11\n",
            "Episode 55: reward: -95.640, steps: 11\n",
            "Episode 56: reward: -99.295, steps: 3\n",
            "Episode 57: reward: -98.106, steps: 7\n",
            "Episode 58: reward: -97.206, steps: 7\n",
            "Episode 59: reward: -97.906, steps: 7\n",
            "Episode 60: reward: -99.295, steps: 3\n",
            "Episode 61: reward: -99.295, steps: 3\n",
            "Episode 62: reward: -99.295, steps: 3\n",
            "Episode 63: reward: -99.295, steps: 3\n",
            "Episode 64: reward: -97.906, steps: 7\n",
            "Episode 65: reward: -96.397, steps: 11\n",
            "Episode 66: reward: -96.397, steps: 11\n",
            "Episode 67: reward: -99.295, steps: 3\n",
            "Episode 68: reward: -99.378, steps: 4\n",
            "Episode 69: reward: -98.106, steps: 7\n",
            "Episode 70: reward: -99.295, steps: 3\n",
            "Episode 71: reward: -98.106, steps: 7\n",
            "Episode 72: reward: -98.106, steps: 7\n",
            "Episode 73: reward: -94.643, steps: 15\n",
            "Episode 74: reward: -99.378, steps: 4\n",
            "Episode 75: reward: -99.295, steps: 3\n",
            "Episode 76: reward: -99.295, steps: 3\n",
            "Episode 77: reward: -99.295, steps: 3\n",
            "Episode 78: reward: -99.295, steps: 3\n",
            "Episode 79: reward: -99.295, steps: 3\n",
            "Episode 80: reward: -98.106, steps: 7\n",
            "Episode 81: reward: -99.378, steps: 4\n",
            "Episode 82: reward: -99.295, steps: 3\n",
            "Episode 83: reward: -99.295, steps: 3\n",
            "Episode 84: reward: -98.106, steps: 8\n",
            "Episode 85: reward: -99.295, steps: 3\n",
            "Episode 86: reward: -96.284, steps: 11\n",
            "Episode 87: reward: -99.295, steps: 3\n",
            "Episode 88: reward: -96.284, steps: 11\n",
            "Episode 89: reward: -98.106, steps: 7\n",
            "Episode 90: reward: -98.106, steps: 7\n",
            "Episode 91: reward: -99.295, steps: 3\n",
            "Episode 92: reward: -99.295, steps: 3\n",
            "Episode 93: reward: -99.295, steps: 3\n",
            "Episode 94: reward: -97.206, steps: 7\n",
            "Episode 95: reward: -97.906, steps: 7\n",
            "Episode 96: reward: -99.378, steps: 4\n",
            "Episode 97: reward: -99.295, steps: 3\n",
            "Episode 98: reward: -99.295, steps: 3\n",
            "Episode 99: reward: -99.378, steps: 4\n",
            "Episode 100: reward: -97.056, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 36 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.001, 'adam_epsilon': 0.1, 'batch_size': 32, 'target_model_update': 0.0005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_35 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_90 (Dense)            (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,885\n",
            "Trainable params: 3,885\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 199s 20ms/step - reward: -17.5851\n",
            "1796 episodes - episode_reward: -97.914 [-100.109, -93.907] - loss: 271.574 - mae: 50.106 - mean_q: -19.024\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 209s 21ms/step - reward: -13.9237\n",
            "1432 episodes - episode_reward: -97.232 [-99.974, -92.483] - loss: 64.979 - mae: 72.251 - mean_q: -56.220\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 210s 21ms/step - reward: -13.8020\n",
            "done, took 617.899 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -95.197, steps: 11\n",
            "Episode 2: reward: -94.344, steps: 11\n",
            "Episode 3: reward: -96.670, steps: 9\n",
            "Episode 4: reward: -97.406, steps: 7\n",
            "Episode 5: reward: -97.106, steps: 7\n",
            "Episode 6: reward: -97.494, steps: 7\n",
            "Episode 7: reward: -97.256, steps: 7\n",
            "Episode 8: reward: -98.991, steps: 5\n",
            "Episode 9: reward: -97.306, steps: 7\n",
            "Episode 10: reward: -95.624, steps: 11\n",
            "Episode 11: reward: -95.140, steps: 11\n",
            "Episode 12: reward: -95.134, steps: 11\n",
            "Episode 13: reward: -99.378, steps: 4\n",
            "Episode 14: reward: -95.360, steps: 11\n",
            "Episode 15: reward: -98.791, steps: 5\n",
            "Episode 16: reward: -99.974, steps: 1\n",
            "Episode 17: reward: -97.891, steps: 5\n",
            "Episode 18: reward: -95.767, steps: 11\n",
            "Episode 19: reward: -97.006, steps: 7\n",
            "Episode 20: reward: -96.258, steps: 10\n",
            "Episode 21: reward: -95.550, steps: 11\n",
            "Episode 22: reward: -96.148, steps: 10\n",
            "Episode 23: reward: -97.206, steps: 7\n",
            "Episode 24: reward: -97.156, steps: 7\n",
            "Episode 25: reward: -94.950, steps: 11\n",
            "Episode 26: reward: -97.006, steps: 7\n",
            "Episode 27: reward: -97.406, steps: 7\n",
            "Episode 28: reward: -94.977, steps: 11\n",
            "Episode 29: reward: -93.143, steps: 15\n",
            "Episode 30: reward: -94.470, steps: 11\n",
            "Episode 31: reward: -95.750, steps: 11\n",
            "Episode 32: reward: -99.647, steps: 2\n",
            "Episode 33: reward: -97.006, steps: 7\n",
            "Episode 34: reward: -95.370, steps: 11\n",
            "Episode 35: reward: -96.108, steps: 10\n",
            "Episode 36: reward: -99.647, steps: 2\n",
            "Episode 37: reward: -97.306, steps: 7\n",
            "Episode 38: reward: -97.256, steps: 7\n",
            "Episode 39: reward: -98.791, steps: 5\n",
            "Episode 40: reward: -97.056, steps: 7\n",
            "Episode 41: reward: -95.360, steps: 11\n",
            "Episode 42: reward: -95.204, steps: 11\n",
            "Episode 43: reward: -96.308, steps: 10\n",
            "Episode 44: reward: -98.791, steps: 5\n",
            "Episode 45: reward: -95.234, steps: 11\n",
            "Episode 46: reward: -99.378, steps: 4\n",
            "Episode 47: reward: -96.956, steps: 7\n",
            "Episode 48: reward: -96.870, steps: 9\n",
            "Episode 49: reward: -97.406, steps: 7\n",
            "Episode 50: reward: -96.620, steps: 9\n",
            "Episode 51: reward: -97.156, steps: 7\n",
            "Episode 52: reward: -97.256, steps: 7\n",
            "Episode 53: reward: -96.846, steps: 8\n",
            "Episode 54: reward: -95.497, steps: 11\n",
            "Episode 55: reward: -95.998, steps: 10\n",
            "Episode 56: reward: -95.170, steps: 11\n",
            "Episode 57: reward: -97.090, steps: 9\n",
            "Episode 58: reward: -99.647, steps: 2\n",
            "Episode 59: reward: -97.206, steps: 7\n",
            "Episode 60: reward: -96.058, steps: 10\n",
            "Episode 61: reward: -95.260, steps: 11\n",
            "Episode 62: reward: -95.134, steps: 11\n",
            "Episode 63: reward: -96.856, steps: 7\n",
            "Episode 64: reward: -95.450, steps: 11\n",
            "Episode 65: reward: -97.056, steps: 7\n",
            "Episode 66: reward: -95.434, steps: 11\n",
            "Episode 67: reward: -97.056, steps: 7\n",
            "Episode 68: reward: -97.409, steps: 6\n",
            "Episode 69: reward: -95.070, steps: 11\n",
            "Episode 70: reward: -98.791, steps: 5\n",
            "Episode 71: reward: -97.056, steps: 7\n",
            "Episode 72: reward: -97.156, steps: 7\n",
            "Episode 73: reward: -97.056, steps: 7\n",
            "Episode 74: reward: -99.238, steps: 4\n",
            "Episode 75: reward: -95.614, steps: 11\n",
            "Episode 76: reward: -97.206, steps: 7\n",
            "Episode 77: reward: -95.517, steps: 11\n",
            "Episode 78: reward: -95.234, steps: 11\n",
            "Episode 79: reward: -96.956, steps: 7\n",
            "Episode 80: reward: -96.408, steps: 10\n",
            "Episode 81: reward: -98.991, steps: 5\n",
            "Episode 82: reward: -97.456, steps: 7\n",
            "Episode 83: reward: -98.991, steps: 5\n",
            "Episode 84: reward: -95.614, steps: 11\n",
            "Episode 85: reward: -97.006, steps: 7\n",
            "Episode 86: reward: -99.647, steps: 2\n",
            "Episode 87: reward: -94.787, steps: 11\n",
            "Episode 88: reward: -98.791, steps: 5\n",
            "Episode 89: reward: -95.307, steps: 11\n",
            "Episode 90: reward: -96.570, steps: 9\n",
            "Episode 91: reward: -98.791, steps: 5\n",
            "Episode 92: reward: -97.356, steps: 7\n",
            "Episode 93: reward: -95.197, steps: 11\n",
            "Episode 94: reward: -97.156, steps: 7\n",
            "Episode 95: reward: -99.974, steps: 1\n",
            "Episode 96: reward: -95.926, steps: 10\n",
            "Episode 97: reward: -95.260, steps: 11\n",
            "Episode 98: reward: -95.767, steps: 11\n",
            "Episode 99: reward: -95.307, steps: 11\n",
            "Episode 100: reward: -95.560, steps: 11\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 37 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.1, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_36 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,829\n",
            "Trainable params: 2,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 202s 20ms/step - reward: -19.5076\n",
            "1987 episodes - episode_reward: -98.177 [-100.142, -91.852] - loss: 128.702 - mae: 63.319 - mean_q: -39.332\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 214s 21ms/step - reward: -16.2788\n",
            "1663 episodes - episode_reward: -97.888 [-100.142, -93.130] - loss: 39.707 - mae: 81.234 - mean_q: -69.956\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 214s 21ms/step - reward: -15.3909\n",
            "done, took 629.566 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.762, steps: 6\n",
            "Episode 2: reward: -97.662, steps: 6\n",
            "Episode 3: reward: -96.820, steps: 9\n",
            "Episode 4: reward: -97.862, steps: 6\n",
            "Episode 5: reward: -95.608, steps: 10\n",
            "Episode 6: reward: -97.612, steps: 6\n",
            "Episode 7: reward: -97.762, steps: 6\n",
            "Episode 8: reward: -95.945, steps: 10\n",
            "Episode 9: reward: -97.462, steps: 6\n",
            "Episode 10: reward: -96.008, steps: 10\n",
            "Episode 11: reward: -96.620, steps: 9\n",
            "Episode 12: reward: -97.206, steps: 7\n",
            "Episode 13: reward: -96.071, steps: 10\n",
            "Episode 14: reward: -98.658, steps: 4\n",
            "Episode 15: reward: -98.418, steps: 4\n",
            "Episode 16: reward: -96.620, steps: 9\n",
            "Episode 17: reward: -95.628, steps: 10\n",
            "Episode 18: reward: -97.962, steps: 6\n",
            "Episode 19: reward: -99.295, steps: 3\n",
            "Episode 20: reward: -97.612, steps: 6\n",
            "Episode 21: reward: -97.262, steps: 6\n",
            "Episode 22: reward: -99.974, steps: 1\n",
            "Episode 23: reward: -96.920, steps: 9\n",
            "Episode 24: reward: -96.078, steps: 10\n",
            "Episode 25: reward: -96.390, steps: 9\n",
            "Episode 26: reward: -95.675, steps: 10\n",
            "Episode 27: reward: -96.260, steps: 9\n",
            "Episode 28: reward: -97.512, steps: 6\n",
            "Episode 29: reward: -95.755, steps: 10\n",
            "Episode 30: reward: -95.760, steps: 9\n",
            "Episode 31: reward: -99.974, steps: 1\n",
            "Episode 32: reward: -95.028, steps: 10\n",
            "Episode 33: reward: -97.462, steps: 6\n",
            "Episode 34: reward: -96.628, steps: 10\n",
            "Episode 35: reward: -95.448, steps: 10\n",
            "Episode 36: reward: -99.278, steps: 4\n",
            "Episode 37: reward: -96.071, steps: 10\n",
            "Episode 38: reward: -99.305, steps: 4\n",
            "Episode 39: reward: -98.378, steps: 4\n",
            "Episode 40: reward: -98.538, steps: 4\n",
            "Episode 41: reward: -97.020, steps: 9\n",
            "Episode 42: reward: -97.306, steps: 7\n",
            "Episode 43: reward: -99.398, steps: 4\n",
            "Episode 44: reward: -95.768, steps: 10\n",
            "Episode 45: reward: -97.562, steps: 6\n",
            "Episode 46: reward: -99.278, steps: 4\n",
            "Episode 47: reward: -96.720, steps: 9\n",
            "Episode 48: reward: -96.160, steps: 9\n",
            "Episode 49: reward: -95.015, steps: 10\n",
            "Episode 50: reward: -97.412, steps: 6\n",
            "Episode 51: reward: -97.662, steps: 6\n",
            "Episode 52: reward: -97.020, steps: 9\n",
            "Episode 53: reward: -99.295, steps: 3\n",
            "Episode 54: reward: -97.812, steps: 6\n",
            "Episode 55: reward: -95.755, steps: 10\n",
            "Episode 56: reward: -98.458, steps: 4\n",
            "Episode 57: reward: -96.261, steps: 10\n",
            "Episode 58: reward: -97.412, steps: 6\n",
            "Episode 59: reward: -96.920, steps: 9\n",
            "Episode 60: reward: -98.598, steps: 4\n",
            "Episode 61: reward: -97.412, steps: 6\n",
            "Episode 62: reward: -99.295, steps: 3\n",
            "Episode 63: reward: -98.418, steps: 4\n",
            "Episode 64: reward: -97.412, steps: 6\n",
            "Episode 65: reward: -95.928, steps: 10\n",
            "Episode 66: reward: -97.662, steps: 6\n",
            "Episode 67: reward: -97.262, steps: 6\n",
            "Episode 68: reward: -96.920, steps: 9\n",
            "Episode 69: reward: -98.418, steps: 4\n",
            "Episode 70: reward: -99.974, steps: 1\n",
            "Episode 71: reward: -96.670, steps: 9\n",
            "Episode 72: reward: -97.512, steps: 6\n",
            "Episode 73: reward: -96.208, steps: 10\n",
            "Episode 74: reward: -97.062, steps: 6\n",
            "Episode 75: reward: -97.662, steps: 6\n",
            "Episode 76: reward: -96.590, steps: 9\n",
            "Episode 77: reward: -99.647, steps: 2\n",
            "Episode 78: reward: -97.256, steps: 7\n",
            "Episode 79: reward: -95.825, steps: 10\n",
            "Episode 80: reward: -96.071, steps: 10\n",
            "Episode 81: reward: -95.641, steps: 10\n",
            "Episode 82: reward: -95.881, steps: 10\n",
            "Episode 83: reward: -95.788, steps: 10\n",
            "Episode 84: reward: -95.028, steps: 10\n",
            "Episode 85: reward: -96.590, steps: 9\n",
            "Episode 86: reward: -97.762, steps: 6\n",
            "Episode 87: reward: -97.712, steps: 6\n",
            "Episode 88: reward: -98.338, steps: 4\n",
            "Episode 89: reward: -95.960, steps: 9\n",
            "Episode 90: reward: -99.295, steps: 3\n",
            "Episode 91: reward: -95.945, steps: 10\n",
            "Episode 92: reward: -97.512, steps: 6\n",
            "Episode 93: reward: -95.405, steps: 10\n",
            "Episode 94: reward: -97.612, steps: 6\n",
            "Episode 95: reward: -97.812, steps: 6\n",
            "Episode 96: reward: -95.571, steps: 10\n",
            "Episode 97: reward: -97.812, steps: 6\n",
            "Episode 98: reward: -97.362, steps: 6\n",
            "Episode 99: reward: -99.974, steps: 1\n",
            "Episode 100: reward: -95.565, steps: 10\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 38 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_37 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 210s 21ms/step - reward: -19.9263\n",
            "2029 episodes - episode_reward: -98.208 [-100.142, -93.948] - loss: 186.194 - mae: 65.578 - mean_q: -50.756\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 225s 23ms/step - reward: -18.7356\n",
            "1910 episodes - episode_reward: -98.092 [-100.115, -92.747] - loss: 32.485 - mae: 90.469 - mean_q: -84.495\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 225s 23ms/step - reward: -18.5807\n",
            "done, took 660.837 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.712, steps: 6\n",
            "Episode 2: reward: -97.891, steps: 5\n",
            "Episode 3: reward: -95.791, steps: 10\n",
            "Episode 4: reward: -95.480, steps: 11\n",
            "Episode 5: reward: -97.362, steps: 6\n",
            "Episode 6: reward: -99.291, steps: 4\n",
            "Episode 7: reward: -99.295, steps: 3\n",
            "Episode 8: reward: -97.412, steps: 6\n",
            "Episode 9: reward: -99.405, steps: 4\n",
            "Episode 10: reward: -98.698, steps: 4\n",
            "Episode 11: reward: -99.295, steps: 3\n",
            "Episode 12: reward: -97.562, steps: 6\n",
            "Episode 13: reward: -95.928, steps: 10\n",
            "Episode 14: reward: -97.562, steps: 6\n",
            "Episode 15: reward: -99.295, steps: 3\n",
            "Episode 16: reward: -99.295, steps: 3\n",
            "Episode 17: reward: -97.841, steps: 5\n",
            "Episode 18: reward: -99.295, steps: 3\n",
            "Episode 19: reward: -99.305, steps: 4\n",
            "Episode 20: reward: -99.295, steps: 3\n",
            "Episode 21: reward: -96.231, steps: 10\n",
            "Episode 22: reward: -99.405, steps: 4\n",
            "Episode 23: reward: -96.143, steps: 9\n",
            "Episode 24: reward: -95.102, steps: 13\n",
            "Episode 25: reward: -97.462, steps: 6\n",
            "Episode 26: reward: -97.979, steps: 5\n",
            "Episode 27: reward: -99.305, steps: 4\n",
            "Episode 28: reward: -99.411, steps: 4\n",
            "Episode 29: reward: -99.411, steps: 4\n",
            "Episode 30: reward: -97.891, steps: 5\n",
            "Episode 31: reward: -97.462, steps: 6\n",
            "Episode 32: reward: -98.165, steps: 5\n",
            "Episode 33: reward: -95.928, steps: 10\n",
            "Episode 34: reward: -97.841, steps: 5\n",
            "Episode 35: reward: -97.612, steps: 6\n",
            "Episode 36: reward: -97.612, steps: 6\n",
            "Episode 37: reward: -99.405, steps: 4\n",
            "Episode 38: reward: -97.512, steps: 6\n",
            "Episode 39: reward: -97.762, steps: 6\n",
            "Episode 40: reward: -99.405, steps: 4\n",
            "Episode 41: reward: -96.047, steps: 9\n",
            "Episode 42: reward: -96.248, steps: 10\n",
            "Episode 43: reward: -98.106, steps: 7\n",
            "Episode 44: reward: -97.562, steps: 6\n",
            "Episode 45: reward: -97.691, steps: 5\n",
            "Episode 46: reward: -97.741, steps: 5\n",
            "Episode 47: reward: -97.841, steps: 5\n",
            "Episode 48: reward: -97.791, steps: 5\n",
            "Episode 49: reward: -99.429, steps: 2\n",
            "Episode 50: reward: -96.681, steps: 9\n",
            "Episode 51: reward: -97.841, steps: 5\n",
            "Episode 52: reward: -98.106, steps: 7\n",
            "Episode 53: reward: -97.791, steps: 5\n",
            "Episode 54: reward: -99.295, steps: 3\n",
            "Episode 55: reward: -97.441, steps: 5\n",
            "Episode 56: reward: -99.974, steps: 1\n",
            "Episode 57: reward: -97.356, steps: 7\n",
            "Episode 58: reward: -99.305, steps: 4\n",
            "Episode 59: reward: -97.762, steps: 6\n",
            "Episode 60: reward: -95.904, steps: 9\n",
            "Episode 61: reward: -97.512, steps: 6\n",
            "Episode 62: reward: -97.891, steps: 5\n",
            "Episode 63: reward: -96.430, steps: 11\n",
            "Episode 64: reward: -97.712, steps: 6\n",
            "Episode 65: reward: -99.295, steps: 3\n",
            "Episode 66: reward: -99.295, steps: 3\n",
            "Episode 67: reward: -99.295, steps: 3\n",
            "Episode 68: reward: -95.991, steps: 10\n",
            "Episode 69: reward: -94.838, steps: 10\n",
            "Episode 70: reward: -99.295, steps: 3\n",
            "Episode 71: reward: -97.791, steps: 5\n",
            "Episode 72: reward: -99.295, steps: 3\n",
            "Episode 73: reward: -98.282, steps: 7\n",
            "Episode 74: reward: -96.945, steps: 8\n",
            "Episode 75: reward: -97.906, steps: 7\n",
            "Episode 76: reward: -95.890, steps: 9\n",
            "Episode 77: reward: -98.958, steps: 5\n",
            "Episode 78: reward: -97.143, steps: 7\n",
            "Episode 79: reward: -99.295, steps: 3\n",
            "Episode 80: reward: -97.891, steps: 5\n",
            "Episode 81: reward: -97.612, steps: 6\n",
            "Episode 82: reward: -99.295, steps: 3\n",
            "Episode 83: reward: -97.791, steps: 5\n",
            "Episode 84: reward: -97.620, steps: 7\n",
            "Episode 85: reward: -97.612, steps: 6\n",
            "Episode 86: reward: -97.712, steps: 6\n",
            "Episode 87: reward: -97.612, steps: 6\n",
            "Episode 88: reward: -98.106, steps: 7\n",
            "Episode 89: reward: -97.791, steps: 5\n",
            "Episode 90: reward: -96.311, steps: 10\n",
            "Episode 91: reward: -97.316, steps: 7\n",
            "Episode 92: reward: -99.295, steps: 3\n",
            "Episode 93: reward: -99.295, steps: 3\n",
            "Episode 94: reward: -97.462, steps: 6\n",
            "Episode 95: reward: -98.958, steps: 5\n",
            "Episode 96: reward: -98.818, steps: 4\n",
            "Episode 97: reward: -96.353, steps: 9\n",
            "Episode 98: reward: -96.168, steps: 10\n",
            "Episode 99: reward: -97.979, steps: 5\n",
            "Episode 100: reward: -97.741, steps: 5\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 39 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_38 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 219s 22ms/step - reward: -24.2140\n",
            "2451 episodes - episode_reward: -98.793 [-100.129, -92.688] - loss: 202.467 - mae: 71.180 - mean_q: -53.886\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 224s 22ms/step - reward: -18.4037\n",
            "1875 episodes - episode_reward: -98.153 [-100.142, -93.113] - loss: 22.982 - mae: 89.601 - mean_q: -83.899\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -15.5473\n",
            "done, took 665.214 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.891, steps: 5\n",
            "Episode 2: reward: -95.563, steps: 9\n",
            "Episode 3: reward: -97.812, steps: 6\n",
            "Episode 4: reward: -96.590, steps: 9\n",
            "Episode 5: reward: -96.590, steps: 9\n",
            "Episode 6: reward: -97.182, steps: 6\n",
            "Episode 7: reward: -97.741, steps: 5\n",
            "Episode 8: reward: -97.791, steps: 5\n",
            "Episode 9: reward: -96.700, steps: 9\n",
            "Episode 10: reward: -98.041, steps: 5\n",
            "Episode 11: reward: -97.841, steps: 5\n",
            "Episode 12: reward: -97.902, steps: 6\n",
            "Episode 13: reward: -98.115, steps: 6\n",
            "Episode 14: reward: -96.367, steps: 9\n",
            "Episode 15: reward: -96.333, steps: 9\n",
            "Episode 16: reward: -98.141, steps: 5\n",
            "Episode 17: reward: -97.841, steps: 5\n",
            "Episode 18: reward: -96.523, steps: 9\n",
            "Episode 19: reward: -97.891, steps: 5\n",
            "Episode 20: reward: -96.193, steps: 9\n",
            "Episode 21: reward: -96.780, steps: 9\n",
            "Episode 22: reward: -96.587, steps: 9\n",
            "Episode 23: reward: -96.573, steps: 9\n",
            "Episode 24: reward: -98.041, steps: 5\n",
            "Episode 25: reward: -96.573, steps: 9\n",
            "Episode 26: reward: -95.943, steps: 9\n",
            "Episode 27: reward: -96.207, steps: 9\n",
            "Episode 28: reward: -96.716, steps: 7\n",
            "Episode 29: reward: -96.401, steps: 10\n",
            "Episode 30: reward: -98.191, steps: 5\n",
            "Episode 31: reward: -96.590, steps: 9\n",
            "Episode 32: reward: -97.791, steps: 5\n",
            "Episode 33: reward: -98.191, steps: 5\n",
            "Episode 34: reward: -97.991, steps: 5\n",
            "Episode 35: reward: -97.288, steps: 6\n",
            "Episode 36: reward: -96.103, steps: 9\n",
            "Episode 37: reward: -96.333, steps: 9\n",
            "Episode 38: reward: -97.640, steps: 9\n",
            "Episode 39: reward: -96.510, steps: 9\n",
            "Episode 40: reward: -96.360, steps: 9\n",
            "Episode 41: reward: -96.270, steps: 9\n",
            "Episode 42: reward: -98.041, steps: 5\n",
            "Episode 43: reward: -96.320, steps: 9\n",
            "Episode 44: reward: -96.067, steps: 9\n",
            "Episode 45: reward: -98.041, steps: 5\n",
            "Episode 46: reward: -96.510, steps: 9\n",
            "Episode 47: reward: -96.650, steps: 9\n",
            "Episode 48: reward: -95.560, steps: 9\n",
            "Episode 49: reward: -96.350, steps: 9\n",
            "Episode 50: reward: -97.941, steps: 5\n",
            "Episode 51: reward: -97.941, steps: 5\n",
            "Episode 52: reward: -98.091, steps: 5\n",
            "Episode 53: reward: -98.141, steps: 5\n",
            "Episode 54: reward: -98.173, steps: 7\n",
            "Episode 55: reward: -96.333, steps: 9\n",
            "Episode 56: reward: -97.841, steps: 5\n",
            "Episode 57: reward: -95.331, steps: 10\n",
            "Episode 58: reward: -97.031, steps: 7\n",
            "Episode 59: reward: -97.691, steps: 5\n",
            "Episode 60: reward: -96.510, steps: 9\n",
            "Episode 61: reward: -98.191, steps: 5\n",
            "Episode 62: reward: -98.141, steps: 5\n",
            "Episode 63: reward: -98.091, steps: 5\n",
            "Episode 64: reward: -96.650, steps: 9\n",
            "Episode 65: reward: -97.991, steps: 5\n",
            "Episode 66: reward: -96.587, steps: 9\n",
            "Episode 67: reward: -97.941, steps: 5\n",
            "Episode 68: reward: -96.383, steps: 9\n",
            "Episode 69: reward: -96.447, steps: 9\n",
            "Episode 70: reward: -97.688, steps: 6\n",
            "Episode 71: reward: -96.113, steps: 9\n",
            "Episode 72: reward: -99.974, steps: 1\n",
            "Episode 73: reward: -97.941, steps: 5\n",
            "Episode 74: reward: -96.457, steps: 9\n",
            "Episode 75: reward: -96.327, steps: 9\n",
            "Episode 76: reward: -96.527, steps: 9\n",
            "Episode 77: reward: -96.573, steps: 9\n",
            "Episode 78: reward: -96.460, steps: 9\n",
            "Episode 79: reward: -95.673, steps: 9\n",
            "Episode 80: reward: -98.499, steps: 5\n",
            "Episode 81: reward: -97.941, steps: 5\n",
            "Episode 82: reward: -96.067, steps: 9\n",
            "Episode 83: reward: -98.141, steps: 5\n",
            "Episode 84: reward: -96.330, steps: 9\n",
            "Episode 85: reward: -94.355, steps: 13\n",
            "Episode 86: reward: -96.001, steps: 8\n",
            "Episode 87: reward: -96.207, steps: 9\n",
            "Episode 88: reward: -97.791, steps: 5\n",
            "Episode 89: reward: -96.303, steps: 9\n",
            "Episode 90: reward: -98.141, steps: 5\n",
            "Episode 91: reward: -96.460, steps: 9\n",
            "Episode 92: reward: -99.647, steps: 2\n",
            "Episode 93: reward: -96.527, steps: 9\n",
            "Episode 94: reward: -97.891, steps: 5\n",
            "Episode 95: reward: -97.713, steps: 7\n",
            "Episode 96: reward: -97.955, steps: 6\n",
            "Episode 97: reward: -97.791, steps: 5\n",
            "Episode 98: reward: -97.991, steps: 5\n",
            "Episode 99: reward: -96.314, steps: 10\n",
            "Episode 100: reward: -96.257, steps: 9\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 40 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.005, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.01, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_39 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 16)                672       \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,437\n",
            "Trainable params: 1,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -20.2955\n",
            "2064 episodes - episode_reward: -98.331 [-100.142, -91.741] - loss: 65.451 - mae: 81.753 - mean_q: -70.291\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -14.4795\n",
            "1486 episodes - episode_reward: -97.441 [-100.142, -92.153] - loss: 14.694 - mae: 87.802 - mean_q: -82.090\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -13.5759\n",
            "done, took 659.301 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.662, steps: 6\n",
            "Episode 2: reward: -95.985, steps: 10\n",
            "Episode 3: reward: -97.612, steps: 6\n",
            "Episode 4: reward: -96.588, steps: 10\n",
            "Episode 5: reward: -95.381, steps: 10\n",
            "Episode 6: reward: -96.698, steps: 9\n",
            "Episode 7: reward: -97.662, steps: 6\n",
            "Episode 8: reward: -97.512, steps: 6\n",
            "Episode 9: reward: -97.662, steps: 6\n",
            "Episode 10: reward: -95.578, steps: 10\n",
            "Episode 11: reward: -97.462, steps: 6\n",
            "Episode 12: reward: -97.512, steps: 6\n",
            "Episode 13: reward: -97.912, steps: 6\n",
            "Episode 14: reward: -97.812, steps: 6\n",
            "Episode 15: reward: -97.762, steps: 6\n",
            "Episode 16: reward: -95.278, steps: 10\n",
            "Episode 17: reward: -97.612, steps: 6\n",
            "Episode 18: reward: -95.708, steps: 10\n",
            "Episode 19: reward: -95.961, steps: 10\n",
            "Episode 20: reward: -97.412, steps: 6\n",
            "Episode 21: reward: -96.308, steps: 10\n",
            "Episode 22: reward: -96.111, steps: 10\n",
            "Episode 23: reward: -96.151, steps: 10\n",
            "Episode 24: reward: -95.961, steps: 10\n",
            "Episode 25: reward: -94.938, steps: 10\n",
            "Episode 26: reward: -96.055, steps: 10\n",
            "Episode 27: reward: -96.498, steps: 10\n",
            "Episode 28: reward: -97.462, steps: 6\n",
            "Episode 29: reward: -96.088, steps: 10\n",
            "Episode 30: reward: -96.108, steps: 10\n",
            "Episode 31: reward: -96.341, steps: 10\n",
            "Episode 32: reward: -96.025, steps: 10\n",
            "Episode 33: reward: -97.300, steps: 7\n",
            "Episode 34: reward: -97.662, steps: 6\n",
            "Episode 35: reward: -95.835, steps: 10\n",
            "Episode 36: reward: -95.411, steps: 10\n",
            "Episode 37: reward: -96.798, steps: 9\n",
            "Episode 38: reward: -95.598, steps: 10\n",
            "Episode 39: reward: -96.740, steps: 9\n",
            "Episode 40: reward: -96.118, steps: 10\n",
            "Episode 41: reward: -95.731, steps: 10\n",
            "Episode 42: reward: -95.668, steps: 10\n",
            "Episode 43: reward: -95.958, steps: 10\n",
            "Episode 44: reward: -95.161, steps: 10\n",
            "Episode 45: reward: -96.175, steps: 10\n",
            "Episode 46: reward: -95.985, steps: 10\n",
            "Episode 47: reward: -95.728, steps: 10\n",
            "Episode 48: reward: -97.462, steps: 6\n",
            "Episode 49: reward: -97.712, steps: 6\n",
            "Episode 50: reward: -97.462, steps: 6\n",
            "Episode 51: reward: -97.612, steps: 6\n",
            "Episode 52: reward: -95.411, steps: 10\n",
            "Episode 53: reward: -95.921, steps: 10\n",
            "Episode 54: reward: -97.562, steps: 6\n",
            "Episode 55: reward: -95.991, steps: 10\n",
            "Episode 56: reward: -95.928, steps: 10\n",
            "Episode 57: reward: -93.877, steps: 14\n",
            "Episode 58: reward: -96.151, steps: 10\n",
            "Episode 59: reward: -95.895, steps: 10\n",
            "Episode 60: reward: -96.021, steps: 10\n",
            "Episode 61: reward: -97.862, steps: 6\n",
            "Episode 62: reward: -97.762, steps: 6\n",
            "Episode 63: reward: -94.029, steps: 13\n",
            "Episode 64: reward: -95.675, steps: 10\n",
            "Episode 65: reward: -96.201, steps: 10\n",
            "Episode 66: reward: -95.611, steps: 10\n",
            "Episode 67: reward: -97.662, steps: 6\n",
            "Episode 68: reward: -97.891, steps: 5\n",
            "Episode 69: reward: -95.708, steps: 10\n",
            "Episode 70: reward: -97.562, steps: 6\n",
            "Episode 71: reward: -96.798, steps: 9\n",
            "Episode 72: reward: -97.212, steps: 6\n",
            "Episode 73: reward: -96.215, steps: 10\n",
            "Episode 74: reward: -95.795, steps: 10\n",
            "Episode 75: reward: -93.163, steps: 14\n",
            "Episode 76: reward: -97.512, steps: 6\n",
            "Episode 77: reward: -96.118, steps: 10\n",
            "Episode 78: reward: -95.725, steps: 10\n",
            "Episode 79: reward: -96.181, steps: 10\n",
            "Episode 80: reward: -96.401, steps: 8\n",
            "Episode 81: reward: -95.538, steps: 10\n",
            "Episode 82: reward: -96.215, steps: 10\n",
            "Episode 83: reward: -97.612, steps: 6\n",
            "Episode 84: reward: -97.862, steps: 6\n",
            "Episode 85: reward: -98.791, steps: 5\n",
            "Episode 86: reward: -95.728, steps: 10\n",
            "Episode 87: reward: -97.662, steps: 6\n",
            "Episode 88: reward: -95.865, steps: 10\n",
            "Episode 89: reward: -96.201, steps: 8\n",
            "Episode 90: reward: -95.415, steps: 10\n",
            "Episode 91: reward: -96.341, steps: 10\n",
            "Episode 92: reward: -95.948, steps: 10\n",
            "Episode 93: reward: -97.662, steps: 6\n",
            "Episode 94: reward: -96.798, steps: 9\n",
            "Episode 95: reward: -95.475, steps: 10\n",
            "Episode 96: reward: -96.245, steps: 10\n",
            "Episode 97: reward: -97.712, steps: 6\n",
            "Episode 98: reward: -96.490, steps: 9\n",
            "Episode 99: reward: -97.762, steps: 6\n",
            "Episode 100: reward: -95.918, steps: 10\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 41 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_40\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_40 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_100 (Dense)           (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_101 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,069\n",
            "Trainable params: 5,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 216s 21ms/step - reward: -19.1721\n",
            "1952 episodes - episode_reward: -98.219 [-100.135, -93.186] - loss: 168.754 - mae: 60.225 - mean_q: -32.376\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 232s 23ms/step - reward: -18.9667\n",
            "1927 episodes - episode_reward: -98.426 [-100.142, -92.493] - loss: 48.048 - mae: 81.420 - mean_q: -71.344\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -17.8404\n",
            "done, took 678.081 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -98.578, steps: 6\n",
            "Episode 2: reward: -99.295, steps: 3\n",
            "Episode 3: reward: -99.647, steps: 2\n",
            "Episode 4: reward: -98.395, steps: 6\n",
            "Episode 5: reward: -98.123, steps: 7\n",
            "Episode 6: reward: -99.295, steps: 3\n",
            "Episode 7: reward: -98.578, steps: 6\n",
            "Episode 8: reward: -98.395, steps: 6\n",
            "Episode 9: reward: -98.578, steps: 6\n",
            "Episode 10: reward: -98.073, steps: 7\n",
            "Episode 11: reward: -95.818, steps: 10\n",
            "Episode 12: reward: -99.647, steps: 2\n",
            "Episode 13: reward: -98.395, steps: 6\n",
            "Episode 14: reward: -98.395, steps: 6\n",
            "Episode 15: reward: -99.647, steps: 2\n",
            "Episode 16: reward: -99.647, steps: 2\n",
            "Episode 17: reward: -98.578, steps: 6\n",
            "Episode 18: reward: -98.073, steps: 7\n",
            "Episode 19: reward: -99.295, steps: 3\n",
            "Episode 20: reward: -97.712, steps: 6\n",
            "Episode 21: reward: -96.820, steps: 9\n",
            "Episode 22: reward: -98.123, steps: 7\n",
            "Episode 23: reward: -98.395, steps: 6\n",
            "Episode 24: reward: -99.647, steps: 2\n",
            "Episode 25: reward: -99.008, steps: 5\n",
            "Episode 26: reward: -99.647, steps: 2\n",
            "Episode 27: reward: -96.018, steps: 10\n",
            "Episode 28: reward: -97.712, steps: 6\n",
            "Episode 29: reward: -98.578, steps: 6\n",
            "Episode 30: reward: -98.791, steps: 5\n",
            "Episode 31: reward: -98.578, steps: 6\n",
            "Episode 32: reward: -98.578, steps: 6\n",
            "Episode 33: reward: -97.357, steps: 9\n",
            "Episode 34: reward: -98.190, steps: 7\n",
            "Episode 35: reward: -98.190, steps: 7\n",
            "Episode 36: reward: -98.190, steps: 7\n",
            "Episode 37: reward: -99.647, steps: 2\n",
            "Episode 38: reward: -99.647, steps: 2\n",
            "Episode 39: reward: -98.190, steps: 7\n",
            "Episode 40: reward: -98.123, steps: 7\n",
            "Episode 41: reward: -97.456, steps: 7\n",
            "Episode 42: reward: -99.295, steps: 3\n",
            "Episode 43: reward: -99.647, steps: 2\n",
            "Episode 44: reward: -96.068, steps: 10\n",
            "Episode 45: reward: -97.712, steps: 6\n",
            "Episode 46: reward: -99.647, steps: 2\n",
            "Episode 47: reward: -98.073, steps: 7\n",
            "Episode 48: reward: -98.395, steps: 6\n",
            "Episode 49: reward: -98.578, steps: 6\n",
            "Episode 50: reward: -98.141, steps: 5\n",
            "Episode 51: reward: -98.073, steps: 7\n",
            "Episode 52: reward: -98.190, steps: 7\n",
            "Episode 53: reward: -95.450, steps: 9\n",
            "Episode 54: reward: -98.578, steps: 6\n",
            "Episode 55: reward: -98.073, steps: 7\n",
            "Episode 56: reward: -98.190, steps: 7\n",
            "Episode 57: reward: -99.647, steps: 2\n",
            "Episode 58: reward: -98.578, steps: 6\n",
            "Episode 59: reward: -98.362, steps: 6\n",
            "Episode 60: reward: -98.578, steps: 6\n",
            "Episode 61: reward: -98.395, steps: 6\n",
            "Episode 62: reward: -96.440, steps: 9\n",
            "Episode 63: reward: -99.647, steps: 2\n",
            "Episode 64: reward: -99.647, steps: 2\n",
            "Episode 65: reward: -98.578, steps: 6\n",
            "Episode 66: reward: -99.647, steps: 2\n",
            "Episode 67: reward: -96.118, steps: 10\n",
            "Episode 68: reward: -98.578, steps: 6\n",
            "Episode 69: reward: -98.578, steps: 6\n",
            "Episode 70: reward: -98.190, steps: 7\n",
            "Episode 71: reward: -95.460, steps: 11\n",
            "Episode 72: reward: -98.190, steps: 7\n",
            "Episode 73: reward: -99.008, steps: 5\n",
            "Episode 74: reward: -99.647, steps: 2\n",
            "Episode 75: reward: -99.295, steps: 3\n",
            "Episode 76: reward: -99.647, steps: 2\n",
            "Episode 77: reward: -99.974, steps: 1\n",
            "Episode 78: reward: -98.578, steps: 6\n",
            "Episode 79: reward: -99.974, steps: 1\n",
            "Episode 80: reward: -97.456, steps: 7\n",
            "Episode 81: reward: -98.395, steps: 6\n",
            "Episode 82: reward: -99.974, steps: 1\n",
            "Episode 83: reward: -98.190, steps: 7\n",
            "Episode 84: reward: -98.578, steps: 6\n",
            "Episode 85: reward: -98.073, steps: 7\n",
            "Episode 86: reward: -99.647, steps: 2\n",
            "Episode 87: reward: -97.456, steps: 7\n",
            "Episode 88: reward: -98.578, steps: 6\n",
            "Episode 89: reward: -99.647, steps: 2\n",
            "Episode 90: reward: -96.018, steps: 10\n",
            "Episode 91: reward: -98.578, steps: 6\n",
            "Episode 92: reward: -99.647, steps: 2\n",
            "Episode 93: reward: -99.295, steps: 3\n",
            "Episode 94: reward: -96.268, steps: 10\n",
            "Episode 95: reward: -98.598, steps: 4\n",
            "Episode 96: reward: -98.395, steps: 6\n",
            "Episode 97: reward: -99.647, steps: 2\n",
            "Episode 98: reward: -98.395, steps: 6\n",
            "Episode 99: reward: -96.068, steps: 10\n",
            "Episode 100: reward: -98.190, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 42 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.001, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 32, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_41 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_102 (Dense)           (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_103 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " dense_104 (Dense)           (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,637\n",
            "Trainable params: 2,637\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -18.4120\n",
            "1879 episodes - episode_reward: -97.988 [-100.135, -94.170] - loss: 208.741 - mae: 62.903 - mean_q: -44.363\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 233s 23ms/step - reward: -16.9908\n",
            "1737 episodes - episode_reward: -97.817 [-100.122, -94.330] - loss: 14.912 - mae: 88.823 - mean_q: -84.124\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 228s 23ms/step - reward: -14.3058\n",
            "done, took 677.830 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.056, steps: 7\n",
            "Episode 2: reward: -97.506, steps: 7\n",
            "Episode 3: reward: -95.394, steps: 11\n",
            "Episode 4: reward: -97.306, steps: 7\n",
            "Episode 5: reward: -96.919, steps: 8\n",
            "Episode 6: reward: -97.166, steps: 8\n",
            "Episode 7: reward: -97.456, steps: 7\n",
            "Episode 8: reward: -97.326, steps: 8\n",
            "Episode 9: reward: -97.206, steps: 7\n",
            "Episode 10: reward: -97.306, steps: 7\n",
            "Episode 11: reward: -95.204, steps: 11\n",
            "Episode 12: reward: -97.156, steps: 7\n",
            "Episode 13: reward: -97.056, steps: 7\n",
            "Episode 14: reward: -97.106, steps: 7\n",
            "Episode 15: reward: -95.577, steps: 11\n",
            "Episode 16: reward: -96.906, steps: 7\n",
            "Episode 17: reward: -97.256, steps: 7\n",
            "Episode 18: reward: -96.906, steps: 7\n",
            "Episode 19: reward: -97.156, steps: 7\n",
            "Episode 20: reward: -97.106, steps: 7\n",
            "Episode 21: reward: -97.006, steps: 8\n",
            "Episode 22: reward: -97.106, steps: 7\n",
            "Episode 23: reward: -97.156, steps: 7\n",
            "Episode 24: reward: -95.134, steps: 11\n",
            "Episode 25: reward: -95.394, steps: 11\n",
            "Episode 26: reward: -95.077, steps: 11\n",
            "Episode 27: reward: -95.450, steps: 11\n",
            "Episode 28: reward: -97.646, steps: 8\n",
            "Episode 29: reward: -97.356, steps: 7\n",
            "Episode 30: reward: -97.106, steps: 7\n",
            "Episode 31: reward: -96.956, steps: 7\n",
            "Episode 32: reward: -97.356, steps: 7\n",
            "Episode 33: reward: -97.056, steps: 7\n",
            "Episode 34: reward: -97.206, steps: 7\n",
            "Episode 35: reward: -96.956, steps: 7\n",
            "Episode 36: reward: -97.256, steps: 7\n",
            "Episode 37: reward: -95.450, steps: 11\n",
            "Episode 38: reward: -97.206, steps: 7\n",
            "Episode 39: reward: -97.056, steps: 7\n",
            "Episode 40: reward: -97.433, steps: 8\n",
            "Episode 41: reward: -97.306, steps: 7\n",
            "Episode 42: reward: -97.306, steps: 7\n",
            "Episode 43: reward: -97.006, steps: 7\n",
            "Episode 44: reward: -95.577, steps: 11\n",
            "Episode 45: reward: -97.006, steps: 7\n",
            "Episode 46: reward: -95.514, steps: 11\n",
            "Episode 47: reward: -97.106, steps: 7\n",
            "Episode 48: reward: -96.856, steps: 7\n",
            "Episode 49: reward: -96.956, steps: 7\n",
            "Episode 50: reward: -97.256, steps: 7\n",
            "Episode 51: reward: -96.906, steps: 7\n",
            "Episode 52: reward: -97.156, steps: 7\n",
            "Episode 53: reward: -97.206, steps: 7\n",
            "Episode 54: reward: -97.306, steps: 7\n",
            "Episode 55: reward: -97.106, steps: 7\n",
            "Episode 56: reward: -97.206, steps: 7\n",
            "Episode 57: reward: -95.520, steps: 11\n",
            "Episode 58: reward: -97.356, steps: 7\n",
            "Episode 59: reward: -97.006, steps: 7\n",
            "Episode 60: reward: -97.006, steps: 7\n",
            "Episode 61: reward: -95.077, steps: 11\n",
            "Episode 62: reward: -97.056, steps: 7\n",
            "Episode 63: reward: -97.406, steps: 7\n",
            "Episode 64: reward: -96.473, steps: 8\n",
            "Episode 65: reward: -97.406, steps: 7\n",
            "Episode 66: reward: -96.906, steps: 7\n",
            "Episode 67: reward: -95.704, steps: 11\n",
            "Episode 68: reward: -96.793, steps: 8\n",
            "Episode 69: reward: -97.056, steps: 7\n",
            "Episode 70: reward: -97.606, steps: 7\n",
            "Episode 71: reward: -93.999, steps: 12\n",
            "Episode 72: reward: -96.499, steps: 8\n",
            "Episode 73: reward: -94.887, steps: 11\n",
            "Episode 74: reward: -95.830, steps: 11\n",
            "Episode 75: reward: -97.456, steps: 7\n",
            "Episode 76: reward: -97.256, steps: 7\n",
            "Episode 77: reward: -96.956, steps: 7\n",
            "Episode 78: reward: -97.206, steps: 7\n",
            "Episode 79: reward: -95.204, steps: 11\n",
            "Episode 80: reward: -97.356, steps: 7\n",
            "Episode 81: reward: -97.306, steps: 7\n",
            "Episode 82: reward: -96.906, steps: 7\n",
            "Episode 83: reward: -97.206, steps: 7\n",
            "Episode 84: reward: -97.206, steps: 7\n",
            "Episode 85: reward: -96.906, steps: 7\n",
            "Episode 86: reward: -97.006, steps: 7\n",
            "Episode 87: reward: -96.906, steps: 7\n",
            "Episode 88: reward: -97.056, steps: 7\n",
            "Episode 89: reward: -95.394, steps: 11\n",
            "Episode 90: reward: -95.197, steps: 11\n",
            "Episode 91: reward: -97.219, steps: 8\n",
            "Episode 92: reward: -97.056, steps: 7\n",
            "Episode 93: reward: -97.106, steps: 7\n",
            "Episode 94: reward: -97.306, steps: 7\n",
            "Episode 95: reward: -97.056, steps: 7\n",
            "Episode 96: reward: -96.579, steps: 8\n",
            "Episode 97: reward: -97.256, steps: 7\n",
            "Episode 98: reward: -97.106, steps: 7\n",
            "Episode 99: reward: -97.306, steps: 7\n",
            "Episode 100: reward: -97.556, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 43 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_42 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_105 (Dense)           (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_106 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,829\n",
            "Trainable params: 2,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -18.4208\n",
            "1878 episodes - episode_reward: -98.089 [-100.135, -93.373] - loss: 184.432 - mae: 65.818 - mean_q: -47.701\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -15.6323\n",
            "1600 episodes - episode_reward: -97.703 [-100.142, -92.843] - loss: 30.367 - mae: 85.902 - mean_q: -78.200\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 226s 23ms/step - reward: -13.4153\n",
            "done, took 673.870 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.201, steps: 10\n",
            "Episode 2: reward: -96.371, steps: 10\n",
            "Episode 3: reward: -95.601, steps: 10\n",
            "Episode 4: reward: -95.928, steps: 10\n",
            "Episode 5: reward: -97.712, steps: 6\n",
            "Episode 6: reward: -98.658, steps: 4\n",
            "Episode 7: reward: -95.791, steps: 10\n",
            "Episode 8: reward: -97.712, steps: 6\n",
            "Episode 9: reward: -95.735, steps: 10\n",
            "Episode 10: reward: -95.695, steps: 10\n",
            "Episode 11: reward: -95.981, steps: 10\n",
            "Episode 12: reward: -95.918, steps: 10\n",
            "Episode 13: reward: -95.385, steps: 10\n",
            "Episode 14: reward: -95.538, steps: 10\n",
            "Episode 15: reward: -96.181, steps: 10\n",
            "Episode 16: reward: -95.865, steps: 10\n",
            "Episode 17: reward: -99.974, steps: 1\n",
            "Episode 18: reward: -96.168, steps: 10\n",
            "Episode 19: reward: -97.562, steps: 6\n",
            "Episode 20: reward: -95.341, steps: 10\n",
            "Episode 21: reward: -96.168, steps: 10\n",
            "Episode 22: reward: -95.915, steps: 10\n",
            "Episode 23: reward: -95.738, steps: 10\n",
            "Episode 24: reward: -97.712, steps: 6\n",
            "Episode 25: reward: -95.728, steps: 10\n",
            "Episode 26: reward: -96.105, steps: 10\n",
            "Episode 27: reward: -95.865, steps: 10\n",
            "Episode 28: reward: -93.903, steps: 14\n",
            "Episode 29: reward: -95.981, steps: 10\n",
            "Episode 30: reward: -95.299, steps: 12\n",
            "Episode 31: reward: -96.055, steps: 10\n",
            "Episode 32: reward: -96.245, steps: 10\n",
            "Episode 33: reward: -95.418, steps: 10\n",
            "Episode 34: reward: -97.462, steps: 6\n",
            "Episode 35: reward: -99.974, steps: 1\n",
            "Episode 36: reward: -95.991, steps: 10\n",
            "Episode 37: reward: -97.762, steps: 6\n",
            "Episode 38: reward: -95.788, steps: 10\n",
            "Episode 39: reward: -97.662, steps: 6\n",
            "Episode 40: reward: -94.596, steps: 13\n",
            "Episode 41: reward: -96.442, steps: 8\n",
            "Episode 42: reward: -95.115, steps: 10\n",
            "Episode 43: reward: -97.762, steps: 6\n",
            "Episode 44: reward: -95.665, steps: 10\n",
            "Episode 45: reward: -95.578, steps: 10\n",
            "Episode 46: reward: -96.775, steps: 8\n",
            "Episode 47: reward: -96.371, steps: 10\n",
            "Episode 48: reward: -97.912, steps: 6\n",
            "Episode 49: reward: -94.521, steps: 10\n",
            "Episode 50: reward: -97.662, steps: 6\n",
            "Episode 51: reward: -96.171, steps: 10\n",
            "Episode 52: reward: -95.675, steps: 10\n",
            "Episode 53: reward: -96.338, steps: 10\n",
            "Episode 54: reward: -96.590, steps: 9\n",
            "Episode 55: reward: -97.462, steps: 6\n",
            "Episode 56: reward: -96.021, steps: 10\n",
            "Episode 57: reward: -97.412, steps: 6\n",
            "Episode 58: reward: -95.598, steps: 10\n",
            "Episode 59: reward: -95.155, steps: 10\n",
            "Episode 60: reward: -97.762, steps: 6\n",
            "Episode 61: reward: -95.475, steps: 10\n",
            "Episode 62: reward: -97.712, steps: 6\n",
            "Episode 63: reward: -97.712, steps: 6\n",
            "Episode 64: reward: -95.995, steps: 10\n",
            "Episode 65: reward: -97.054, steps: 8\n",
            "Episode 66: reward: -96.055, steps: 10\n",
            "Episode 67: reward: -96.041, steps: 10\n",
            "Episode 68: reward: -95.981, steps: 10\n",
            "Episode 69: reward: -95.801, steps: 10\n",
            "Episode 70: reward: -95.925, steps: 10\n",
            "Episode 71: reward: -97.454, steps: 8\n",
            "Episode 72: reward: -94.479, steps: 12\n",
            "Episode 73: reward: -97.762, steps: 6\n",
            "Episode 74: reward: -97.562, steps: 6\n",
            "Episode 75: reward: -95.411, steps: 10\n",
            "Episode 76: reward: -96.498, steps: 10\n",
            "Episode 77: reward: -95.309, steps: 12\n",
            "Episode 78: reward: -95.601, steps: 10\n",
            "Episode 79: reward: -96.485, steps: 10\n",
            "Episode 80: reward: -97.304, steps: 8\n",
            "Episode 81: reward: -98.698, steps: 4\n",
            "Episode 82: reward: -96.101, steps: 8\n",
            "Episode 83: reward: -97.304, steps: 8\n",
            "Episode 84: reward: -96.761, steps: 9\n",
            "Episode 85: reward: -97.077, steps: 8\n",
            "Episode 86: reward: -95.978, steps: 10\n",
            "Episode 87: reward: -97.127, steps: 8\n",
            "Episode 88: reward: -97.300, steps: 7\n",
            "Episode 89: reward: -97.812, steps: 6\n",
            "Episode 90: reward: -96.271, steps: 10\n",
            "Episode 91: reward: -95.738, steps: 10\n",
            "Episode 92: reward: -97.612, steps: 6\n",
            "Episode 93: reward: -95.411, steps: 10\n",
            "Episode 94: reward: -95.345, steps: 10\n",
            "Episode 95: reward: -97.662, steps: 6\n",
            "Episode 96: reward: -98.958, steps: 5\n",
            "Episode 97: reward: -97.862, steps: 6\n",
            "Episode 98: reward: -95.319, steps: 12\n",
            "Episode 99: reward: -97.662, steps: 6\n",
            "Episode 100: reward: -96.328, steps: 10\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 44 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.005, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.01, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_43 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_107 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_108 (Dense)           (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_109 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,069\n",
            "Trainable params: 5,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -20.8107\n",
            "2115 episodes - episode_reward: -98.396 [-100.142, -92.937] - loss: 74.680 - mae: 81.528 - mean_q: -70.254\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -16.4588\n",
            "1682 episodes - episode_reward: -97.852 [-100.142, -92.593] - loss: 14.838 - mae: 87.943 - mean_q: -82.355\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -14.7215\n",
            "done, took 682.500 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -94.464, steps: 11\n",
            "Episode 2: reward: -97.106, steps: 7\n",
            "Episode 3: reward: -97.456, steps: 7\n",
            "Episode 4: reward: -95.388, steps: 10\n",
            "Episode 5: reward: -97.662, steps: 6\n",
            "Episode 6: reward: -97.306, steps: 7\n",
            "Episode 7: reward: -95.860, steps: 9\n",
            "Episode 8: reward: -95.387, steps: 11\n",
            "Episode 9: reward: -97.462, steps: 6\n",
            "Episode 10: reward: -97.712, steps: 6\n",
            "Episode 11: reward: -95.798, steps: 10\n",
            "Episode 12: reward: -95.735, steps: 10\n",
            "Episode 13: reward: -95.078, steps: 10\n",
            "Episode 14: reward: -96.070, steps: 9\n",
            "Episode 15: reward: -97.712, steps: 6\n",
            "Episode 16: reward: -95.584, steps: 11\n",
            "Episode 17: reward: -95.450, steps: 11\n",
            "Episode 18: reward: -97.662, steps: 6\n",
            "Episode 19: reward: -97.362, steps: 6\n",
            "Episode 20: reward: -95.818, steps: 10\n",
            "Episode 21: reward: -95.691, steps: 10\n",
            "Episode 22: reward: -95.577, steps: 11\n",
            "Episode 23: reward: -95.908, steps: 10\n",
            "Episode 24: reward: -95.798, steps: 10\n",
            "Episode 25: reward: -97.662, steps: 6\n",
            "Episode 26: reward: -95.658, steps: 10\n",
            "Episode 27: reward: -97.312, steps: 6\n",
            "Episode 28: reward: -95.761, steps: 10\n",
            "Episode 29: reward: -96.068, steps: 10\n",
            "Episode 30: reward: -97.612, steps: 6\n",
            "Episode 31: reward: -96.568, steps: 10\n",
            "Episode 32: reward: -97.412, steps: 6\n",
            "Episode 33: reward: -95.868, steps: 10\n",
            "Episode 34: reward: -97.006, steps: 7\n",
            "Episode 35: reward: -95.640, steps: 11\n",
            "Episode 36: reward: -95.945, steps: 10\n",
            "Episode 37: reward: -95.968, steps: 10\n",
            "Episode 38: reward: -99.647, steps: 2\n",
            "Episode 39: reward: -96.490, steps: 9\n",
            "Episode 40: reward: -94.394, steps: 11\n",
            "Episode 41: reward: -97.312, steps: 6\n",
            "Episode 42: reward: -97.156, steps: 7\n",
            "Episode 43: reward: -95.640, steps: 11\n",
            "Episode 44: reward: -97.206, steps: 7\n",
            "Episode 45: reward: -97.006, steps: 7\n",
            "Episode 46: reward: -95.885, steps: 10\n",
            "Episode 47: reward: -95.980, steps: 9\n",
            "Episode 48: reward: -95.881, steps: 10\n",
            "Episode 49: reward: -95.808, steps: 10\n",
            "Episode 50: reward: -95.450, steps: 11\n",
            "Episode 51: reward: -97.362, steps: 6\n",
            "Episode 52: reward: -97.056, steps: 7\n",
            "Episode 53: reward: -95.048, steps: 10\n",
            "Episode 54: reward: -99.647, steps: 2\n",
            "Episode 55: reward: -96.208, steps: 10\n",
            "Episode 56: reward: -96.640, steps: 9\n",
            "Episode 57: reward: -97.612, steps: 6\n",
            "Episode 58: reward: -95.818, steps: 10\n",
            "Episode 59: reward: -95.948, steps: 10\n",
            "Episode 60: reward: -97.106, steps: 7\n",
            "Episode 61: reward: -97.612, steps: 6\n",
            "Episode 62: reward: -97.662, steps: 6\n",
            "Episode 63: reward: -95.767, steps: 11\n",
            "Episode 64: reward: -97.306, steps: 7\n",
            "Episode 65: reward: -97.262, steps: 6\n",
            "Episode 66: reward: -97.412, steps: 6\n",
            "Episode 67: reward: -97.412, steps: 6\n",
            "Episode 68: reward: -97.106, steps: 7\n",
            "Episode 69: reward: -96.318, steps: 10\n",
            "Episode 70: reward: -95.908, steps: 10\n",
            "Episode 71: reward: -97.212, steps: 6\n",
            "Episode 72: reward: -97.412, steps: 6\n",
            "Episode 73: reward: -95.704, steps: 11\n",
            "Episode 74: reward: -97.462, steps: 6\n",
            "Episode 75: reward: -97.762, steps: 6\n",
            "Episode 76: reward: -98.362, steps: 6\n",
            "Episode 77: reward: -97.106, steps: 7\n",
            "Episode 78: reward: -97.256, steps: 7\n",
            "Episode 79: reward: -94.520, steps: 11\n",
            "Episode 80: reward: -97.456, steps: 7\n",
            "Episode 81: reward: -95.577, steps: 11\n",
            "Episode 82: reward: -97.006, steps: 7\n",
            "Episode 83: reward: -99.305, steps: 4\n",
            "Episode 84: reward: -98.580, steps: 7\n",
            "Episode 85: reward: -99.647, steps: 2\n",
            "Episode 86: reward: -97.356, steps: 7\n",
            "Episode 87: reward: -97.462, steps: 6\n",
            "Episode 88: reward: -97.356, steps: 7\n",
            "Episode 89: reward: -97.462, steps: 6\n",
            "Episode 90: reward: -97.612, steps: 6\n",
            "Episode 91: reward: -97.306, steps: 7\n",
            "Episode 92: reward: -98.362, steps: 6\n",
            "Episode 93: reward: -97.612, steps: 6\n",
            "Episode 94: reward: -97.362, steps: 6\n",
            "Episode 95: reward: -96.158, steps: 10\n",
            "Episode 96: reward: -99.647, steps: 2\n",
            "Episode 97: reward: -97.808, steps: 8\n",
            "Episode 98: reward: -95.767, steps: 11\n",
            "Episode 99: reward: -97.562, steps: 6\n",
            "Episode 100: reward: -95.560, steps: 11\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 45 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.0001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_44 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_110 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_111 (Dense)           (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_112 (Dense)           (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -17.3716\n",
            "1774 episodes - episode_reward: -97.924 [-100.142, -93.233] - loss: 171.548 - mae: 43.206 - mean_q: 1.372\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 229s 23ms/step - reward: -13.1895\n",
            "1359 episodes - episode_reward: -97.053 [-99.974, -91.433] - loss: 193.211 - mae: 44.452 - mean_q: -4.461\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 229s 23ms/step - reward: -12.3358\n",
            "done, took 678.514 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -95.074, steps: 11\n",
            "Episode 2: reward: -94.917, steps: 11\n",
            "Episode 3: reward: -97.006, steps: 7\n",
            "Episode 4: reward: -95.374, steps: 11\n",
            "Episode 5: reward: -95.070, steps: 11\n",
            "Episode 6: reward: -95.010, steps: 11\n",
            "Episode 7: reward: -95.434, steps: 11\n",
            "Episode 8: reward: -97.306, steps: 7\n",
            "Episode 9: reward: -95.497, steps: 11\n",
            "Episode 10: reward: -95.650, steps: 11\n",
            "Episode 11: reward: -95.370, steps: 11\n",
            "Episode 12: reward: -97.006, steps: 7\n",
            "Episode 13: reward: -94.534, steps: 11\n",
            "Episode 14: reward: -96.973, steps: 8\n",
            "Episode 15: reward: -95.400, steps: 11\n",
            "Episode 16: reward: -96.956, steps: 7\n",
            "Episode 17: reward: -97.006, steps: 7\n",
            "Episode 18: reward: -95.500, steps: 11\n",
            "Episode 19: reward: -96.806, steps: 7\n",
            "Episode 20: reward: -95.517, steps: 11\n",
            "Episode 21: reward: -96.956, steps: 7\n",
            "Episode 22: reward: -95.390, steps: 11\n",
            "Episode 23: reward: -97.106, steps: 7\n",
            "Episode 24: reward: -93.513, steps: 15\n",
            "Episode 25: reward: -97.056, steps: 7\n",
            "Episode 26: reward: -94.544, steps: 11\n",
            "Episode 27: reward: -95.434, steps: 11\n",
            "Episode 28: reward: -95.400, steps: 11\n",
            "Episode 29: reward: -97.106, steps: 7\n",
            "Episode 30: reward: -95.624, steps: 11\n",
            "Episode 31: reward: -95.210, steps: 11\n",
            "Episode 32: reward: -97.056, steps: 7\n",
            "Episode 33: reward: -97.006, steps: 7\n",
            "Episode 34: reward: -97.156, steps: 7\n",
            "Episode 35: reward: -95.687, steps: 11\n",
            "Episode 36: reward: -97.206, steps: 7\n",
            "Episode 37: reward: -95.707, steps: 11\n",
            "Episode 38: reward: -97.406, steps: 7\n",
            "Episode 39: reward: -95.940, steps: 11\n",
            "Episode 40: reward: -95.137, steps: 11\n",
            "Episode 41: reward: -95.464, steps: 11\n",
            "Episode 42: reward: -95.274, steps: 11\n",
            "Episode 43: reward: -94.914, steps: 11\n",
            "Episode 44: reward: -95.107, steps: 11\n",
            "Episode 45: reward: -97.006, steps: 7\n",
            "Episode 46: reward: -96.856, steps: 7\n",
            "Episode 47: reward: -92.713, steps: 15\n",
            "Episode 48: reward: -97.256, steps: 7\n",
            "Episode 49: reward: -95.497, steps: 11\n",
            "Episode 50: reward: -95.210, steps: 11\n",
            "Episode 51: reward: -95.494, steps: 11\n",
            "Episode 52: reward: -96.956, steps: 7\n",
            "Episode 53: reward: -95.864, steps: 11\n",
            "Episode 54: reward: -97.306, steps: 7\n",
            "Episode 55: reward: -95.244, steps: 11\n",
            "Episode 56: reward: -95.400, steps: 11\n",
            "Episode 57: reward: -94.660, steps: 11\n",
            "Episode 58: reward: -97.006, steps: 7\n",
            "Episode 59: reward: -97.106, steps: 7\n",
            "Episode 60: reward: -96.956, steps: 7\n",
            "Episode 61: reward: -97.056, steps: 7\n",
            "Episode 62: reward: -97.306, steps: 7\n",
            "Episode 63: reward: -97.306, steps: 7\n",
            "Episode 64: reward: -95.560, steps: 11\n",
            "Episode 65: reward: -97.156, steps: 7\n",
            "Episode 66: reward: -95.687, steps: 11\n",
            "Episode 67: reward: -95.587, steps: 11\n",
            "Episode 68: reward: -97.306, steps: 7\n",
            "Episode 69: reward: -94.734, steps: 11\n",
            "Episode 70: reward: -95.437, steps: 11\n",
            "Episode 71: reward: -97.356, steps: 7\n",
            "Episode 72: reward: -97.206, steps: 7\n",
            "Episode 73: reward: -94.290, steps: 11\n",
            "Episode 74: reward: -95.370, steps: 11\n",
            "Episode 75: reward: -97.306, steps: 7\n",
            "Episode 76: reward: -95.360, steps: 11\n",
            "Episode 77: reward: -95.210, steps: 11\n",
            "Episode 78: reward: -97.106, steps: 7\n",
            "Episode 79: reward: -97.006, steps: 7\n",
            "Episode 80: reward: -95.464, steps: 11\n",
            "Episode 81: reward: -97.306, steps: 7\n",
            "Episode 82: reward: -96.856, steps: 7\n",
            "Episode 83: reward: -96.806, steps: 7\n",
            "Episode 84: reward: -96.956, steps: 7\n",
            "Episode 85: reward: -97.256, steps: 7\n",
            "Episode 86: reward: -96.806, steps: 7\n",
            "Episode 87: reward: -95.327, steps: 11\n",
            "Episode 88: reward: -95.184, steps: 11\n",
            "Episode 89: reward: -97.206, steps: 7\n",
            "Episode 90: reward: -95.800, steps: 11\n",
            "Episode 91: reward: -96.956, steps: 7\n",
            "Episode 92: reward: -95.307, steps: 11\n",
            "Episode 93: reward: -96.856, steps: 7\n",
            "Episode 94: reward: -95.770, steps: 11\n",
            "Episode 95: reward: -95.484, steps: 11\n",
            "Episode 96: reward: -95.674, steps: 11\n",
            "Episode 97: reward: -95.624, steps: 11\n",
            "Episode 98: reward: -97.256, steps: 7\n",
            "Episode 99: reward: -95.590, steps: 11\n",
            "Episode 100: reward: -96.956, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 46 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.005, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.00146, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_45 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_113 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_114 (Dense)           (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_115 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,069\n",
            "Trainable params: 5,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 227s 22ms/step - reward: -19.5528\n",
            "1990 episodes - episode_reward: -98.257 [-100.135, -93.178] - loss: 115.098 - mae: 65.182 - mean_q: -43.005\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 238s 24ms/step - reward: -15.8750\n",
            "1622 episodes - episode_reward: -97.871 [-100.142, -92.710] - loss: 35.422 - mae: 82.102 - mean_q: -72.394\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 239s 24ms/step - reward: -14.5917\n",
            "done, took 704.053 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -98.387, steps: 7\n",
            "Episode 2: reward: -95.457, steps: 11\n",
            "Episode 3: reward: -97.056, steps: 7\n",
            "Episode 4: reward: -97.156, steps: 7\n",
            "Episode 5: reward: -97.006, steps: 7\n",
            "Episode 6: reward: -95.340, steps: 11\n",
            "Episode 7: reward: -97.056, steps: 7\n",
            "Episode 8: reward: -97.056, steps: 7\n",
            "Episode 9: reward: -98.190, steps: 7\n",
            "Episode 10: reward: -99.385, steps: 4\n",
            "Episode 11: reward: -97.056, steps: 7\n",
            "Episode 12: reward: -99.385, steps: 4\n",
            "Episode 13: reward: -97.606, steps: 7\n",
            "Episode 14: reward: -98.578, steps: 4\n",
            "Episode 15: reward: -97.056, steps: 7\n",
            "Episode 16: reward: -95.640, steps: 11\n",
            "Episode 17: reward: -98.791, steps: 5\n",
            "Episode 18: reward: -97.056, steps: 7\n",
            "Episode 19: reward: -96.088, steps: 10\n",
            "Episode 20: reward: -97.306, steps: 7\n",
            "Episode 21: reward: -97.106, steps: 7\n",
            "Episode 22: reward: -96.897, steps: 9\n",
            "Episode 23: reward: -97.006, steps: 7\n",
            "Episode 24: reward: -96.906, steps: 7\n",
            "Episode 25: reward: -95.134, steps: 11\n",
            "Episode 26: reward: -96.188, steps: 10\n",
            "Episode 27: reward: -95.204, steps: 11\n",
            "Episode 28: reward: -96.906, steps: 7\n",
            "Episode 29: reward: -97.106, steps: 7\n",
            "Episode 30: reward: -97.493, steps: 8\n",
            "Episode 31: reward: -97.156, steps: 7\n",
            "Episode 32: reward: -96.953, steps: 8\n",
            "Episode 33: reward: -97.206, steps: 7\n",
            "Episode 34: reward: -99.295, steps: 3\n",
            "Episode 35: reward: -95.450, steps: 11\n",
            "Episode 36: reward: -97.940, steps: 7\n",
            "Episode 37: reward: -97.456, steps: 7\n",
            "Episode 38: reward: -95.394, steps: 11\n",
            "Episode 39: reward: -99.385, steps: 4\n",
            "Episode 40: reward: -95.450, steps: 11\n",
            "Episode 41: reward: -95.457, steps: 11\n",
            "Episode 42: reward: -96.540, steps: 9\n",
            "Episode 43: reward: -95.938, steps: 10\n",
            "Episode 44: reward: -99.295, steps: 3\n",
            "Episode 45: reward: -97.156, steps: 7\n",
            "Episode 46: reward: -97.156, steps: 7\n",
            "Episode 47: reward: -97.006, steps: 7\n",
            "Episode 48: reward: -98.362, steps: 6\n",
            "Episode 49: reward: -97.006, steps: 7\n",
            "Episode 50: reward: -98.958, steps: 5\n",
            "Episode 51: reward: -95.554, steps: 11\n",
            "Episode 52: reward: -99.251, steps: 4\n",
            "Episode 53: reward: -96.956, steps: 7\n",
            "Episode 54: reward: -96.906, steps: 7\n",
            "Episode 55: reward: -95.460, steps: 11\n",
            "Episode 56: reward: -98.538, steps: 4\n",
            "Episode 57: reward: -97.056, steps: 7\n",
            "Episode 58: reward: -98.190, steps: 7\n",
            "Episode 59: reward: -98.362, steps: 6\n",
            "Episode 60: reward: -96.956, steps: 7\n",
            "Episode 61: reward: -96.690, steps: 9\n",
            "Episode 62: reward: -95.340, steps: 11\n",
            "Episode 63: reward: -99.251, steps: 4\n",
            "Episode 64: reward: -99.295, steps: 3\n",
            "Episode 65: reward: -95.197, steps: 11\n",
            "Episode 66: reward: -99.385, steps: 4\n",
            "Episode 67: reward: -96.790, steps: 9\n",
            "Episode 68: reward: -97.356, steps: 7\n",
            "Episode 69: reward: -95.164, steps: 11\n",
            "Episode 70: reward: -96.906, steps: 7\n",
            "Episode 71: reward: -97.657, steps: 9\n",
            "Episode 72: reward: -93.687, steps: 14\n",
            "Episode 73: reward: -98.562, steps: 6\n",
            "Episode 74: reward: -95.704, steps: 11\n",
            "Episode 75: reward: -97.006, steps: 7\n",
            "Episode 76: reward: -95.457, steps: 11\n",
            "Episode 77: reward: -95.324, steps: 11\n",
            "Episode 78: reward: -97.106, steps: 7\n",
            "Episode 79: reward: -98.958, steps: 5\n",
            "Episode 80: reward: -97.056, steps: 7\n",
            "Episode 81: reward: -97.662, steps: 6\n",
            "Episode 82: reward: -98.362, steps: 6\n",
            "Episode 83: reward: -96.906, steps: 7\n",
            "Episode 84: reward: -94.584, steps: 11\n",
            "Episode 85: reward: -95.387, steps: 11\n",
            "Episode 86: reward: -95.024, steps: 11\n",
            "Episode 87: reward: -95.070, steps: 11\n",
            "Episode 88: reward: -95.514, steps: 11\n",
            "Episode 89: reward: -96.953, steps: 7\n",
            "Episode 90: reward: -97.156, steps: 7\n",
            "Episode 91: reward: -99.385, steps: 4\n",
            "Episode 92: reward: -98.958, steps: 5\n",
            "Episode 93: reward: -97.008, steps: 10\n",
            "Episode 94: reward: -95.014, steps: 11\n",
            "Episode 95: reward: -99.385, steps: 4\n",
            "Episode 96: reward: -99.385, steps: 4\n",
            "Episode 97: reward: -96.906, steps: 7\n",
            "Episode 98: reward: -96.238, steps: 10\n",
            "Episode 99: reward: -97.256, steps: 7\n",
            "Episode 100: reward: -97.106, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 47 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.005, 'adam_epsilon': 1e-05, 'batch_size': 32, 'target_model_update': 0.0005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_46 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_116 (Dense)           (None, 16)                672       \n",
            "                                                                 \n",
            " dense_117 (Dense)           (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,437\n",
            "Trainable params: 1,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 226s 22ms/step - reward: -19.5095\n",
            "1988 episodes - episode_reward: -98.136 [-99.974, -94.013] - loss: 257.291 - mae: 50.595 - mean_q: -21.503\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 234s 23ms/step - reward: -15.3770\n",
            "1576 episodes - episode_reward: -97.570 [-99.974, -91.730] - loss: 84.327 - mae: 75.429 - mean_q: -61.818\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 239s 24ms/step - reward: -14.6403\n",
            "done, took 699.033 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.712, steps: 6\n",
            "Episode 2: reward: -95.155, steps: 10\n",
            "Episode 3: reward: -96.181, steps: 10\n",
            "Episode 4: reward: -97.912, steps: 6\n",
            "Episode 5: reward: -97.712, steps: 6\n",
            "Episode 6: reward: -97.812, steps: 6\n",
            "Episode 7: reward: -95.348, steps: 10\n",
            "Episode 8: reward: -97.462, steps: 6\n",
            "Episode 9: reward: -97.512, steps: 6\n",
            "Episode 10: reward: -97.812, steps: 6\n",
            "Episode 11: reward: -97.612, steps: 6\n",
            "Episode 12: reward: -95.791, steps: 10\n",
            "Episode 13: reward: -97.212, steps: 6\n",
            "Episode 14: reward: -95.755, steps: 10\n",
            "Episode 15: reward: -96.118, steps: 10\n",
            "Episode 16: reward: -97.762, steps: 6\n",
            "Episode 17: reward: -95.675, steps: 10\n",
            "Episode 18: reward: -97.712, steps: 6\n",
            "Episode 19: reward: -97.662, steps: 6\n",
            "Episode 20: reward: -97.412, steps: 6\n",
            "Episode 21: reward: -97.562, steps: 6\n",
            "Episode 22: reward: -97.362, steps: 6\n",
            "Episode 23: reward: -95.855, steps: 10\n",
            "Episode 24: reward: -97.462, steps: 6\n",
            "Episode 25: reward: -97.612, steps: 6\n",
            "Episode 26: reward: -97.912, steps: 6\n",
            "Episode 27: reward: -95.475, steps: 10\n",
            "Episode 28: reward: -95.855, steps: 10\n",
            "Episode 29: reward: -95.918, steps: 10\n",
            "Episode 30: reward: -95.801, steps: 10\n",
            "Episode 31: reward: -97.462, steps: 6\n",
            "Episode 32: reward: -97.762, steps: 6\n",
            "Episode 33: reward: -97.612, steps: 6\n",
            "Episode 34: reward: -96.108, steps: 10\n",
            "Episode 35: reward: -97.612, steps: 6\n",
            "Episode 36: reward: -97.812, steps: 6\n",
            "Episode 37: reward: -96.118, steps: 10\n",
            "Episode 38: reward: -97.812, steps: 6\n",
            "Episode 39: reward: -95.791, steps: 10\n",
            "Episode 40: reward: -95.928, steps: 10\n",
            "Episode 41: reward: -97.512, steps: 6\n",
            "Episode 42: reward: -95.728, steps: 10\n",
            "Episode 43: reward: -95.928, steps: 10\n",
            "Episode 44: reward: -95.791, steps: 10\n",
            "Episode 45: reward: -96.055, steps: 10\n",
            "Episode 46: reward: -95.675, steps: 10\n",
            "Episode 47: reward: -97.412, steps: 6\n",
            "Episode 48: reward: -97.512, steps: 6\n",
            "Episode 49: reward: -96.118, steps: 10\n",
            "Episode 50: reward: -97.362, steps: 6\n",
            "Episode 51: reward: -95.611, steps: 10\n",
            "Episode 52: reward: -97.462, steps: 6\n",
            "Episode 53: reward: -96.055, steps: 10\n",
            "Episode 54: reward: -96.245, steps: 10\n",
            "Episode 55: reward: -95.865, steps: 10\n",
            "Episode 56: reward: -95.675, steps: 10\n",
            "Episode 57: reward: -94.188, steps: 10\n",
            "Episode 58: reward: -95.865, steps: 10\n",
            "Episode 59: reward: -95.611, steps: 10\n",
            "Episode 60: reward: -97.462, steps: 6\n",
            "Episode 61: reward: -97.412, steps: 6\n",
            "Episode 62: reward: -94.251, steps: 10\n",
            "Episode 63: reward: -95.991, steps: 10\n",
            "Episode 64: reward: -97.612, steps: 6\n",
            "Episode 65: reward: -95.728, steps: 10\n",
            "Episode 66: reward: -95.705, steps: 10\n",
            "Episode 67: reward: -97.312, steps: 6\n",
            "Episode 68: reward: -96.108, steps: 10\n",
            "Episode 69: reward: -97.712, steps: 6\n",
            "Episode 70: reward: -95.918, steps: 10\n",
            "Episode 71: reward: -97.262, steps: 6\n",
            "Episode 72: reward: -95.738, steps: 10\n",
            "Episode 73: reward: -97.762, steps: 6\n",
            "Episode 74: reward: -96.055, steps: 10\n",
            "Episode 75: reward: -97.612, steps: 6\n",
            "Episode 76: reward: -95.855, steps: 10\n",
            "Episode 77: reward: -95.948, steps: 10\n",
            "Episode 78: reward: -97.562, steps: 6\n",
            "Episode 79: reward: -95.345, steps: 10\n",
            "Episode 80: reward: -97.412, steps: 6\n",
            "Episode 81: reward: -95.538, steps: 10\n",
            "Episode 82: reward: -97.312, steps: 6\n",
            "Episode 83: reward: -95.801, steps: 10\n",
            "Episode 84: reward: -95.548, steps: 10\n",
            "Episode 85: reward: -97.812, steps: 6\n",
            "Episode 86: reward: -95.580, steps: 11\n",
            "Episode 87: reward: -95.738, steps: 10\n",
            "Episode 88: reward: -97.562, steps: 6\n",
            "Episode 89: reward: -97.812, steps: 6\n",
            "Episode 90: reward: -97.262, steps: 6\n",
            "Episode 91: reward: -96.181, steps: 10\n",
            "Episode 92: reward: -96.118, steps: 10\n",
            "Episode 93: reward: -95.538, steps: 10\n",
            "Episode 94: reward: -95.791, steps: 10\n",
            "Episode 95: reward: -97.462, steps: 6\n",
            "Episode 96: reward: -95.675, steps: 10\n",
            "Episode 97: reward: -97.662, steps: 6\n",
            "Episode 98: reward: -95.151, steps: 10\n",
            "Episode 99: reward: -97.662, steps: 6\n",
            "Episode 100: reward: -96.045, steps: 10\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 48 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.001, 'adam_epsilon': 0.1, 'batch_size': 32, 'target_model_update': 0.01, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 16, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_47 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_118 (Dense)           (None, 16)                672       \n",
            "                                                                 \n",
            " dense_119 (Dense)           (None, 16)                272       \n",
            "                                                                 \n",
            " dense_120 (Dense)           (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,709\n",
            "Trainable params: 1,709\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 237s 23ms/step - reward: -22.2889\n",
            "2263 episodes - episode_reward: -98.494 [-100.142, -94.775] - loss: 89.029 - mae: 83.648 - mean_q: -78.335\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 244s 24ms/step - reward: -18.4204\n",
            "1880 episodes - episode_reward: -97.981 [-99.974, -94.011] - loss: 3.860 - mae: 92.714 - mean_q: -91.285\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 243s 24ms/step - reward: -17.2447\n",
            "done, took 723.660 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -98.191, steps: 5\n",
            "Episode 2: reward: -97.991, steps: 5\n",
            "Episode 3: reward: -98.191, steps: 5\n",
            "Episode 4: reward: -97.641, steps: 5\n",
            "Episode 5: reward: -97.791, steps: 5\n",
            "Episode 6: reward: -97.891, steps: 5\n",
            "Episode 7: reward: -96.857, steps: 9\n",
            "Episode 8: reward: -97.691, steps: 5\n",
            "Episode 9: reward: -97.941, steps: 5\n",
            "Episode 10: reward: -97.741, steps: 5\n",
            "Episode 11: reward: -98.441, steps: 5\n",
            "Episode 12: reward: -98.041, steps: 5\n",
            "Episode 13: reward: -97.991, steps: 5\n",
            "Episode 14: reward: -97.941, steps: 5\n",
            "Episode 15: reward: -98.191, steps: 5\n",
            "Episode 16: reward: -98.391, steps: 5\n",
            "Episode 17: reward: -98.241, steps: 5\n",
            "Episode 18: reward: -98.241, steps: 5\n",
            "Episode 19: reward: -98.041, steps: 5\n",
            "Episode 20: reward: -98.041, steps: 5\n",
            "Episode 21: reward: -98.191, steps: 5\n",
            "Episode 22: reward: -98.091, steps: 5\n",
            "Episode 23: reward: -96.420, steps: 9\n",
            "Episode 24: reward: -98.141, steps: 5\n",
            "Episode 25: reward: -96.357, steps: 9\n",
            "Episode 26: reward: -98.241, steps: 5\n",
            "Episode 27: reward: -97.841, steps: 5\n",
            "Episode 28: reward: -97.941, steps: 5\n",
            "Episode 29: reward: -97.891, steps: 5\n",
            "Episode 30: reward: -97.902, steps: 5\n",
            "Episode 31: reward: -98.041, steps: 5\n",
            "Episode 32: reward: -98.091, steps: 5\n",
            "Episode 33: reward: -96.610, steps: 9\n",
            "Episode 34: reward: -98.191, steps: 5\n",
            "Episode 35: reward: -98.291, steps: 5\n",
            "Episode 36: reward: -97.795, steps: 6\n",
            "Episode 37: reward: -98.191, steps: 5\n",
            "Episode 38: reward: -98.191, steps: 5\n",
            "Episode 39: reward: -98.291, steps: 5\n",
            "Episode 40: reward: -98.041, steps: 5\n",
            "Episode 41: reward: -98.091, steps: 5\n",
            "Episode 42: reward: -97.515, steps: 6\n",
            "Episode 43: reward: -97.991, steps: 5\n",
            "Episode 44: reward: -97.891, steps: 5\n",
            "Episode 45: reward: -98.141, steps: 5\n",
            "Episode 46: reward: -97.941, steps: 5\n",
            "Episode 47: reward: -98.141, steps: 5\n",
            "Episode 48: reward: -96.800, steps: 9\n",
            "Episode 49: reward: -97.741, steps: 5\n",
            "Episode 50: reward: -98.091, steps: 5\n",
            "Episode 51: reward: -97.795, steps: 6\n",
            "Episode 52: reward: -96.001, steps: 10\n",
            "Episode 53: reward: -97.891, steps: 5\n",
            "Episode 54: reward: -98.241, steps: 5\n",
            "Episode 55: reward: -97.891, steps: 5\n",
            "Episode 56: reward: -98.191, steps: 5\n",
            "Episode 57: reward: -97.841, steps: 5\n",
            "Episode 58: reward: -96.948, steps: 6\n",
            "Episode 59: reward: -97.891, steps: 5\n",
            "Episode 60: reward: -97.991, steps: 5\n",
            "Episode 61: reward: -98.272, steps: 5\n",
            "Episode 62: reward: -97.791, steps: 5\n",
            "Episode 63: reward: -98.091, steps: 5\n",
            "Episode 64: reward: -96.483, steps: 9\n",
            "Episode 65: reward: -98.141, steps: 5\n",
            "Episode 66: reward: -96.540, steps: 9\n",
            "Episode 67: reward: -97.991, steps: 5\n",
            "Episode 68: reward: -98.591, steps: 5\n",
            "Episode 69: reward: -98.041, steps: 5\n",
            "Episode 70: reward: -97.622, steps: 6\n",
            "Episode 71: reward: -98.191, steps: 5\n",
            "Episode 72: reward: -97.237, steps: 9\n",
            "Episode 73: reward: -97.641, steps: 5\n",
            "Episode 74: reward: -97.173, steps: 9\n",
            "Episode 75: reward: -98.341, steps: 5\n",
            "Episode 76: reward: -98.041, steps: 5\n",
            "Episode 77: reward: -97.991, steps: 5\n",
            "Episode 78: reward: -98.191, steps: 5\n",
            "Episode 79: reward: -97.791, steps: 5\n",
            "Episode 80: reward: -97.795, steps: 6\n",
            "Episode 81: reward: -97.641, steps: 5\n",
            "Episode 82: reward: -97.568, steps: 6\n",
            "Episode 83: reward: -96.081, steps: 10\n",
            "Episode 84: reward: -98.141, steps: 5\n",
            "Episode 85: reward: -97.941, steps: 5\n",
            "Episode 86: reward: -98.091, steps: 5\n",
            "Episode 87: reward: -97.891, steps: 5\n",
            "Episode 88: reward: -97.641, steps: 5\n",
            "Episode 89: reward: -97.891, steps: 5\n",
            "Episode 90: reward: -97.891, steps: 5\n",
            "Episode 91: reward: -98.041, steps: 5\n",
            "Episode 92: reward: -97.791, steps: 5\n",
            "Episode 93: reward: -98.141, steps: 5\n",
            "Episode 94: reward: -98.041, steps: 5\n",
            "Episode 95: reward: -96.730, steps: 9\n",
            "Episode 96: reward: -98.191, steps: 5\n",
            "Episode 97: reward: -97.691, steps: 5\n",
            "Episode 98: reward: -96.293, steps: 9\n",
            "Episode 99: reward: -98.041, steps: 5\n",
            "Episode 100: reward: -98.091, steps: 5\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 49 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.0001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_48\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_48 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_121 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_122 (Dense)           (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 229s 23ms/step - reward: -20.1551\n",
            "2050 episodes - episode_reward: -98.319 [-100.142, -93.517] - loss: 378.955 - mae: 41.360 - mean_q: 2.967\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 240s 24ms/step - reward: -14.7360\n",
            "1512 episodes - episode_reward: -97.458 [-100.109, -91.597] - loss: 300.583 - mae: 47.177 - mean_q: -4.088\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 237s 24ms/step - reward: -14.1009\n",
            "done, took 706.471 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.179, steps: 9\n",
            "Episode 2: reward: -98.103, steps: 5\n",
            "Episode 3: reward: -96.595, steps: 9\n",
            "Episode 4: reward: -95.807, steps: 11\n",
            "Episode 5: reward: -95.594, steps: 11\n",
            "Episode 6: reward: -96.765, steps: 9\n",
            "Episode 7: reward: -96.790, steps: 9\n",
            "Episode 8: reward: -96.540, steps: 9\n",
            "Episode 9: reward: -96.108, steps: 10\n",
            "Episode 10: reward: -96.369, steps: 9\n",
            "Episode 11: reward: -97.156, steps: 7\n",
            "Episode 12: reward: -97.056, steps: 7\n",
            "Episode 13: reward: -96.010, steps: 9\n",
            "Episode 14: reward: -96.425, steps: 9\n",
            "Episode 15: reward: -98.190, steps: 5\n",
            "Episode 16: reward: -96.405, steps: 9\n",
            "Episode 17: reward: -96.575, steps: 9\n",
            "Episode 18: reward: -97.503, steps: 6\n",
            "Episode 19: reward: -96.539, steps: 9\n",
            "Episode 20: reward: -98.060, steps: 5\n",
            "Episode 21: reward: -97.306, steps: 7\n",
            "Episode 22: reward: -97.306, steps: 7\n",
            "Episode 23: reward: -97.930, steps: 5\n",
            "Episode 24: reward: -95.860, steps: 9\n",
            "Episode 25: reward: -98.146, steps: 5\n",
            "Episode 26: reward: -96.806, steps: 7\n",
            "Episode 27: reward: -97.306, steps: 7\n",
            "Episode 28: reward: -97.106, steps: 7\n",
            "Episode 29: reward: -96.440, steps: 9\n",
            "Episode 30: reward: -97.206, steps: 7\n",
            "Episode 31: reward: -96.462, steps: 9\n",
            "Episode 32: reward: -96.462, steps: 9\n",
            "Episode 33: reward: -97.506, steps: 7\n",
            "Episode 34: reward: -96.438, steps: 10\n",
            "Episode 35: reward: -96.519, steps: 9\n",
            "Episode 36: reward: -96.255, steps: 9\n",
            "Episode 37: reward: -96.540, steps: 9\n",
            "Episode 38: reward: -97.056, steps: 7\n",
            "Episode 39: reward: -97.356, steps: 7\n",
            "Episode 40: reward: -95.902, steps: 10\n",
            "Episode 41: reward: -96.462, steps: 9\n",
            "Episode 42: reward: -97.306, steps: 7\n",
            "Episode 43: reward: -96.709, steps: 9\n",
            "Episode 44: reward: -98.348, steps: 6\n",
            "Episode 45: reward: -97.306, steps: 7\n",
            "Episode 46: reward: -97.386, steps: 8\n",
            "Episode 47: reward: -96.292, steps: 9\n",
            "Episode 48: reward: -97.207, steps: 9\n",
            "Episode 49: reward: -95.423, steps: 13\n",
            "Episode 50: reward: -96.539, steps: 9\n",
            "Episode 51: reward: -98.060, steps: 5\n",
            "Episode 52: reward: -94.670, steps: 11\n",
            "Episode 53: reward: -97.306, steps: 7\n",
            "Episode 54: reward: -98.991, steps: 5\n",
            "Episode 55: reward: -97.843, steps: 5\n",
            "Episode 56: reward: -96.405, steps: 9\n",
            "Episode 57: reward: -98.016, steps: 5\n",
            "Episode 58: reward: -98.791, steps: 5\n",
            "Episode 59: reward: -98.103, steps: 5\n",
            "Episode 60: reward: -96.055, steps: 9\n",
            "Episode 61: reward: -96.652, steps: 9\n",
            "Episode 62: reward: -96.425, steps: 9\n",
            "Episode 63: reward: -95.910, steps: 11\n",
            "Episode 64: reward: -98.103, steps: 5\n",
            "Episode 65: reward: -97.156, steps: 7\n",
            "Episode 66: reward: -97.373, steps: 9\n",
            "Episode 67: reward: -98.091, steps: 5\n",
            "Episode 68: reward: -98.362, steps: 6\n",
            "Episode 69: reward: -97.973, steps: 5\n",
            "Episode 70: reward: -95.910, steps: 11\n",
            "Episode 71: reward: -98.091, steps: 5\n",
            "Episode 72: reward: -97.713, steps: 5\n",
            "Episode 73: reward: -96.956, steps: 7\n",
            "Episode 74: reward: -98.791, steps: 5\n",
            "Episode 75: reward: -97.973, steps: 5\n",
            "Episode 76: reward: -96.382, steps: 10\n",
            "Episode 77: reward: -98.016, steps: 5\n",
            "Episode 78: reward: -97.256, steps: 7\n",
            "Episode 79: reward: -98.016, steps: 5\n",
            "Episode 80: reward: -95.490, steps: 11\n",
            "Episode 81: reward: -97.006, steps: 7\n",
            "Episode 82: reward: -98.060, steps: 5\n",
            "Episode 83: reward: -96.652, steps: 9\n",
            "Episode 84: reward: -99.411, steps: 4\n",
            "Episode 85: reward: -98.103, steps: 5\n",
            "Episode 86: reward: -96.482, steps: 9\n",
            "Episode 87: reward: -97.886, steps: 5\n",
            "Episode 88: reward: -96.765, steps: 9\n",
            "Episode 89: reward: -96.110, steps: 9\n",
            "Episode 90: reward: -98.233, steps: 5\n",
            "Episode 91: reward: -98.060, steps: 5\n",
            "Episode 92: reward: -97.040, steps: 9\n",
            "Episode 93: reward: -95.822, steps: 10\n",
            "Episode 94: reward: -97.256, steps: 7\n",
            "Episode 95: reward: -95.720, steps: 11\n",
            "Episode 96: reward: -97.876, steps: 6\n",
            "Episode 97: reward: -96.425, steps: 9\n",
            "Episode 98: reward: -96.856, steps: 7\n",
            "Episode 99: reward: -96.270, steps: 9\n",
            "Episode 100: reward: -96.870, steps: 9\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 50 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.01, 'batch_size': 32, 'target_model_update': 0.005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_49\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_49 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_123 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_124 (Dense)           (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_125 (Dense)           (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 235s 23ms/step - reward: -18.9815\n",
            "1933 episodes - episode_reward: -98.198 [-100.129, -92.273] - loss: 68.241 - mae: 71.805 - mean_q: -49.610\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 245s 25ms/step - reward: -16.1528\n",
            "1651 episodes - episode_reward: -97.838 [-100.142, -89.042] - loss: 17.523 - mae: 86.587 - mean_q: -79.193\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 250s 25ms/step - reward: -17.3751\n",
            "done, took 729.526 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.256, steps: 7\n",
            "Episode 2: reward: -97.206, steps: 7\n",
            "Episode 3: reward: -97.106, steps: 7\n",
            "Episode 4: reward: -97.156, steps: 7\n",
            "Episode 5: reward: -97.206, steps: 7\n",
            "Episode 6: reward: -97.356, steps: 7\n",
            "Episode 7: reward: -96.856, steps: 7\n",
            "Episode 8: reward: -97.006, steps: 7\n",
            "Episode 9: reward: -97.206, steps: 7\n",
            "Episode 10: reward: -97.206, steps: 7\n",
            "Episode 11: reward: -97.056, steps: 7\n",
            "Episode 12: reward: -97.006, steps: 7\n",
            "Episode 13: reward: -97.106, steps: 7\n",
            "Episode 14: reward: -97.156, steps: 7\n",
            "Episode 15: reward: -97.156, steps: 7\n",
            "Episode 16: reward: -96.756, steps: 7\n",
            "Episode 17: reward: -97.106, steps: 7\n",
            "Episode 18: reward: -97.256, steps: 7\n",
            "Episode 19: reward: -97.256, steps: 7\n",
            "Episode 20: reward: -97.156, steps: 7\n",
            "Episode 21: reward: -96.856, steps: 7\n",
            "Episode 22: reward: -96.856, steps: 7\n",
            "Episode 23: reward: -97.056, steps: 7\n",
            "Episode 24: reward: -96.806, steps: 7\n",
            "Episode 25: reward: -97.206, steps: 7\n",
            "Episode 26: reward: -97.256, steps: 7\n",
            "Episode 27: reward: -97.256, steps: 7\n",
            "Episode 28: reward: -97.056, steps: 7\n",
            "Episode 29: reward: -96.756, steps: 7\n",
            "Episode 30: reward: -97.206, steps: 7\n",
            "Episode 31: reward: -97.156, steps: 7\n",
            "Episode 32: reward: -96.906, steps: 7\n",
            "Episode 33: reward: -97.306, steps: 7\n",
            "Episode 34: reward: -97.156, steps: 7\n",
            "Episode 35: reward: -97.356, steps: 7\n",
            "Episode 36: reward: -97.056, steps: 7\n",
            "Episode 37: reward: -97.156, steps: 7\n",
            "Episode 38: reward: -97.006, steps: 7\n",
            "Episode 39: reward: -97.106, steps: 7\n",
            "Episode 40: reward: -96.906, steps: 7\n",
            "Episode 41: reward: -97.206, steps: 7\n",
            "Episode 42: reward: -97.306, steps: 7\n",
            "Episode 43: reward: -97.256, steps: 7\n",
            "Episode 44: reward: -97.256, steps: 7\n",
            "Episode 45: reward: -97.206, steps: 7\n",
            "Episode 46: reward: -97.206, steps: 7\n",
            "Episode 47: reward: -97.156, steps: 7\n",
            "Episode 48: reward: -97.106, steps: 7\n",
            "Episode 49: reward: -96.756, steps: 7\n",
            "Episode 50: reward: -96.706, steps: 7\n",
            "Episode 51: reward: -97.206, steps: 7\n",
            "Episode 52: reward: -97.256, steps: 7\n",
            "Episode 53: reward: -97.356, steps: 7\n",
            "Episode 54: reward: -97.356, steps: 7\n",
            "Episode 55: reward: -96.906, steps: 7\n",
            "Episode 56: reward: -97.156, steps: 7\n",
            "Episode 57: reward: -97.056, steps: 7\n",
            "Episode 58: reward: -97.506, steps: 7\n",
            "Episode 59: reward: -97.156, steps: 7\n",
            "Episode 60: reward: -97.006, steps: 7\n",
            "Episode 61: reward: -96.956, steps: 7\n",
            "Episode 62: reward: -97.506, steps: 7\n",
            "Episode 63: reward: -97.256, steps: 7\n",
            "Episode 64: reward: -97.356, steps: 7\n",
            "Episode 65: reward: -96.906, steps: 7\n",
            "Episode 66: reward: -97.206, steps: 7\n",
            "Episode 67: reward: -97.156, steps: 7\n",
            "Episode 68: reward: -96.856, steps: 7\n",
            "Episode 69: reward: -97.206, steps: 7\n",
            "Episode 70: reward: -96.956, steps: 7\n",
            "Episode 71: reward: -97.356, steps: 7\n",
            "Episode 72: reward: -96.806, steps: 7\n",
            "Episode 73: reward: -97.106, steps: 7\n",
            "Episode 74: reward: -97.106, steps: 7\n",
            "Episode 75: reward: -96.806, steps: 7\n",
            "Episode 76: reward: -96.956, steps: 7\n",
            "Episode 77: reward: -97.106, steps: 7\n",
            "Episode 78: reward: -97.456, steps: 7\n",
            "Episode 79: reward: -96.856, steps: 7\n",
            "Episode 80: reward: -97.306, steps: 7\n",
            "Episode 81: reward: -96.906, steps: 7\n",
            "Episode 82: reward: -97.456, steps: 7\n",
            "Episode 83: reward: -97.006, steps: 7\n",
            "Episode 84: reward: -97.056, steps: 7\n",
            "Episode 85: reward: -96.906, steps: 7\n",
            "Episode 86: reward: -97.256, steps: 7\n",
            "Episode 87: reward: -97.256, steps: 7\n",
            "Episode 88: reward: -97.006, steps: 7\n",
            "Episode 89: reward: -97.156, steps: 7\n",
            "Episode 90: reward: -97.206, steps: 7\n",
            "Episode 91: reward: -97.056, steps: 7\n",
            "Episode 92: reward: -97.006, steps: 7\n",
            "Episode 93: reward: -97.156, steps: 7\n",
            "Episode 94: reward: -97.206, steps: 7\n",
            "Episode 95: reward: -97.106, steps: 7\n",
            "Episode 96: reward: -97.656, steps: 7\n",
            "Episode 97: reward: -97.206, steps: 7\n",
            "Episode 98: reward: -97.406, steps: 7\n",
            "Episode 99: reward: -97.156, steps: 7\n",
            "Episode 100: reward: -97.006, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 51 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.01, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 48}\n",
            "\n",
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_50 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_126 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_127 (Dense)           (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 241s 24ms/step - reward: -22.0940\n",
            "2245 episodes - episode_reward: -98.415 [-100.135, -93.182] - loss: 160.073 - mae: 77.490 - mean_q: -68.535\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 252s 25ms/step - reward: -20.2882\n",
            "2065 episodes - episode_reward: -98.248 [-100.142, -93.743] - loss: 23.661 - mae: 92.044 - mean_q: -87.429\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 247s 25ms/step - reward: -17.4884\n",
            "done, took 740.049 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.967, steps: 7\n",
            "Episode 2: reward: -98.538, steps: 4\n",
            "Episode 3: reward: -97.891, steps: 5\n",
            "Episode 4: reward: -96.270, steps: 9\n",
            "Episode 5: reward: -98.041, steps: 5\n",
            "Episode 6: reward: -99.391, steps: 4\n",
            "Episode 7: reward: -97.991, steps: 5\n",
            "Episode 8: reward: -97.791, steps: 5\n",
            "Episode 9: reward: -95.943, steps: 9\n",
            "Episode 10: reward: -95.858, steps: 10\n",
            "Episode 11: reward: -97.841, steps: 5\n",
            "Episode 12: reward: -98.458, steps: 4\n",
            "Episode 13: reward: -97.312, steps: 6\n",
            "Episode 14: reward: -96.260, steps: 9\n",
            "Episode 15: reward: -97.941, steps: 5\n",
            "Episode 16: reward: -97.962, steps: 6\n",
            "Episode 17: reward: -97.862, steps: 6\n",
            "Episode 18: reward: -99.251, steps: 4\n",
            "Episode 19: reward: -97.912, steps: 6\n",
            "Episode 20: reward: -97.891, steps: 5\n",
            "Episode 21: reward: -95.928, steps: 10\n",
            "Episode 22: reward: -98.538, steps: 4\n",
            "Episode 23: reward: -97.691, steps: 5\n",
            "Episode 24: reward: -97.941, steps: 5\n",
            "Episode 25: reward: -98.658, steps: 4\n",
            "Episode 26: reward: -96.270, steps: 9\n",
            "Episode 27: reward: -95.985, steps: 10\n",
            "Episode 28: reward: -97.941, steps: 5\n",
            "Episode 29: reward: -96.557, steps: 9\n",
            "Episode 30: reward: -96.523, steps: 9\n",
            "Episode 31: reward: -97.312, steps: 6\n",
            "Episode 32: reward: -95.881, steps: 10\n",
            "Episode 33: reward: -96.067, steps: 9\n",
            "Episode 34: reward: -95.348, steps: 10\n",
            "Episode 35: reward: -97.662, steps: 6\n",
            "Episode 36: reward: -97.691, steps: 5\n",
            "Episode 37: reward: -96.540, steps: 9\n",
            "Episode 38: reward: -96.430, steps: 9\n",
            "Episode 39: reward: -94.733, steps: 9\n",
            "Episode 40: reward: -99.295, steps: 3\n",
            "Episode 41: reward: -97.306, steps: 7\n",
            "Episode 42: reward: -97.891, steps: 5\n",
            "Episode 43: reward: -99.305, steps: 4\n",
            "Episode 44: reward: -99.305, steps: 4\n",
            "Episode 45: reward: -98.538, steps: 4\n",
            "Episode 46: reward: -97.791, steps: 5\n",
            "Episode 47: reward: -97.841, steps: 5\n",
            "Episode 48: reward: -96.460, steps: 9\n",
            "Episode 49: reward: -97.641, steps: 5\n",
            "Episode 50: reward: -98.378, steps: 4\n",
            "Episode 51: reward: -96.298, steps: 10\n",
            "Episode 52: reward: -98.658, steps: 4\n",
            "Episode 53: reward: -96.070, steps: 9\n",
            "Episode 54: reward: -97.812, steps: 6\n",
            "Episode 55: reward: -96.230, steps: 9\n",
            "Episode 56: reward: -98.418, steps: 4\n",
            "Episode 57: reward: -97.433, steps: 7\n",
            "Episode 58: reward: -97.691, steps: 5\n",
            "Episode 59: reward: -98.578, steps: 4\n",
            "Episode 60: reward: -96.207, steps: 9\n",
            "Episode 61: reward: -96.088, steps: 10\n",
            "Episode 62: reward: -97.562, steps: 6\n",
            "Episode 63: reward: -96.260, steps: 9\n",
            "Episode 64: reward: -96.270, steps: 9\n",
            "Episode 65: reward: -97.941, steps: 5\n",
            "Episode 66: reward: -99.251, steps: 4\n",
            "Episode 67: reward: -99.385, steps: 4\n",
            "Episode 68: reward: -98.091, steps: 5\n",
            "Episode 69: reward: -96.197, steps: 9\n",
            "Episode 70: reward: -99.251, steps: 4\n",
            "Episode 71: reward: -97.412, steps: 6\n",
            "Episode 72: reward: -96.390, steps: 9\n",
            "Episode 73: reward: -97.051, steps: 8\n",
            "Episode 74: reward: -95.038, steps: 10\n",
            "Episode 75: reward: -97.862, steps: 6\n",
            "Episode 76: reward: -98.658, steps: 4\n",
            "Episode 77: reward: -96.557, steps: 9\n",
            "Episode 78: reward: -98.378, steps: 4\n",
            "Episode 79: reward: -96.397, steps: 9\n",
            "Episode 80: reward: -96.207, steps: 9\n",
            "Episode 81: reward: -98.658, steps: 4\n",
            "Episode 82: reward: -97.562, steps: 6\n",
            "Episode 83: reward: -95.307, steps: 9\n",
            "Episode 84: reward: -97.841, steps: 5\n",
            "Episode 85: reward: -96.070, steps: 9\n",
            "Episode 86: reward: -98.498, steps: 4\n",
            "Episode 87: reward: -98.698, steps: 4\n",
            "Episode 88: reward: -96.460, steps: 9\n",
            "Episode 89: reward: -96.207, steps: 9\n",
            "Episode 90: reward: -96.453, steps: 9\n",
            "Episode 91: reward: -96.143, steps: 9\n",
            "Episode 92: reward: -97.791, steps: 5\n",
            "Episode 93: reward: -98.123, steps: 7\n",
            "Episode 94: reward: -96.327, steps: 9\n",
            "Episode 95: reward: -97.206, steps: 7\n",
            "Episode 96: reward: -98.698, steps: 4\n",
            "Episode 97: reward: -94.518, steps: 10\n",
            "Episode 98: reward: -97.841, steps: 5\n",
            "Episode 99: reward: -98.041, steps: 5\n",
            "Episode 100: reward: -99.385, steps: 4\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 52 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.0001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_51 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_128 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_129 (Dense)           (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 233s 23ms/step - reward: -17.1256\n",
            "1750 episodes - episode_reward: -97.860 [-100.129, -92.265] - loss: 275.606 - mae: 41.978 - mean_q: -3.848\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 237s 24ms/step - reward: -13.3963\n",
            "1380 episodes - episode_reward: -97.076 [-100.115, -91.114] - loss: 252.379 - mae: 44.415 - mean_q: -13.442\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 242s 24ms/step - reward: -12.8201\n",
            "done, took 712.420 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.055, steps: 10\n",
            "Episode 2: reward: -96.055, steps: 10\n",
            "Episode 3: reward: -97.512, steps: 6\n",
            "Episode 4: reward: -97.312, steps: 6\n",
            "Episode 5: reward: -97.712, steps: 6\n",
            "Episode 6: reward: -94.965, steps: 10\n",
            "Episode 7: reward: -96.055, steps: 10\n",
            "Episode 8: reward: -97.612, steps: 6\n",
            "Episode 9: reward: -95.741, steps: 10\n",
            "Episode 10: reward: -96.245, steps: 10\n",
            "Episode 11: reward: -97.762, steps: 6\n",
            "Episode 12: reward: -96.211, steps: 10\n",
            "Episode 13: reward: -95.665, steps: 10\n",
            "Episode 14: reward: -95.641, steps: 10\n",
            "Episode 15: reward: -95.768, steps: 10\n",
            "Episode 16: reward: -96.108, steps: 10\n",
            "Episode 17: reward: -97.812, steps: 6\n",
            "Episode 18: reward: -97.562, steps: 6\n",
            "Episode 19: reward: -95.641, steps: 10\n",
            "Episode 20: reward: -97.562, steps: 6\n",
            "Episode 21: reward: -97.662, steps: 6\n",
            "Episode 22: reward: -97.612, steps: 6\n",
            "Episode 23: reward: -95.641, steps: 10\n",
            "Episode 24: reward: -97.762, steps: 6\n",
            "Episode 25: reward: -97.662, steps: 6\n",
            "Episode 26: reward: -95.991, steps: 10\n",
            "Episode 27: reward: -97.762, steps: 6\n",
            "Episode 28: reward: -95.805, steps: 10\n",
            "Episode 29: reward: -97.612, steps: 6\n",
            "Episode 30: reward: -97.512, steps: 6\n",
            "Episode 31: reward: -95.918, steps: 10\n",
            "Episode 32: reward: -95.855, steps: 10\n",
            "Episode 33: reward: -95.978, steps: 10\n",
            "Episode 34: reward: -95.991, steps: 10\n",
            "Episode 35: reward: -97.562, steps: 6\n",
            "Episode 36: reward: -95.821, steps: 10\n",
            "Episode 37: reward: -96.425, steps: 10\n",
            "Episode 38: reward: -96.118, steps: 10\n",
            "Episode 39: reward: -96.245, steps: 10\n",
            "Episode 40: reward: -96.055, steps: 10\n",
            "Episode 41: reward: -96.561, steps: 10\n",
            "Episode 42: reward: -95.991, steps: 10\n",
            "Episode 43: reward: -97.362, steps: 6\n",
            "Episode 44: reward: -96.108, steps: 10\n",
            "Episode 45: reward: -96.181, steps: 10\n",
            "Episode 46: reward: -95.895, steps: 10\n",
            "Episode 47: reward: -97.512, steps: 6\n",
            "Episode 48: reward: -95.855, steps: 10\n",
            "Episode 49: reward: -96.171, steps: 10\n",
            "Episode 50: reward: -97.662, steps: 6\n",
            "Episode 51: reward: -95.918, steps: 10\n",
            "Episode 52: reward: -95.885, steps: 10\n",
            "Episode 53: reward: -96.245, steps: 10\n",
            "Episode 54: reward: -97.862, steps: 6\n",
            "Episode 55: reward: -96.308, steps: 10\n",
            "Episode 56: reward: -96.118, steps: 10\n",
            "Episode 57: reward: -95.855, steps: 10\n",
            "Episode 58: reward: -96.275, steps: 10\n",
            "Episode 59: reward: -96.308, steps: 10\n",
            "Episode 60: reward: -95.549, steps: 11\n",
            "Episode 61: reward: -97.662, steps: 6\n",
            "Episode 62: reward: -97.862, steps: 6\n",
            "Episode 63: reward: -96.045, steps: 10\n",
            "Episode 64: reward: -94.828, steps: 10\n",
            "Episode 65: reward: -97.462, steps: 6\n",
            "Episode 66: reward: -97.312, steps: 6\n",
            "Episode 67: reward: -95.178, steps: 10\n",
            "Episode 68: reward: -97.712, steps: 6\n",
            "Episode 69: reward: -97.712, steps: 6\n",
            "Episode 70: reward: -96.435, steps: 10\n",
            "Episode 71: reward: -96.561, steps: 10\n",
            "Episode 72: reward: -96.235, steps: 10\n",
            "Episode 73: reward: -97.562, steps: 6\n",
            "Episode 74: reward: -95.918, steps: 10\n",
            "Episode 75: reward: -96.171, steps: 10\n",
            "Episode 76: reward: -97.512, steps: 6\n",
            "Episode 77: reward: -96.245, steps: 10\n",
            "Episode 78: reward: -96.181, steps: 10\n",
            "Episode 79: reward: -95.155, steps: 10\n",
            "Episode 80: reward: -97.462, steps: 6\n",
            "Episode 81: reward: -95.475, steps: 10\n",
            "Episode 82: reward: -96.118, steps: 10\n",
            "Episode 83: reward: -97.912, steps: 6\n",
            "Episode 84: reward: -95.801, steps: 10\n",
            "Episode 85: reward: -95.345, steps: 10\n",
            "Episode 86: reward: -95.928, steps: 10\n",
            "Episode 87: reward: -94.901, steps: 10\n",
            "Episode 88: reward: -96.308, steps: 10\n",
            "Episode 89: reward: -95.918, steps: 10\n",
            "Episode 90: reward: -96.011, steps: 10\n",
            "Episode 91: reward: -97.662, steps: 6\n",
            "Episode 92: reward: -95.738, steps: 10\n",
            "Episode 93: reward: -97.612, steps: 6\n",
            "Episode 94: reward: -95.671, steps: 10\n",
            "Episode 95: reward: -97.562, steps: 6\n",
            "Episode 96: reward: -96.171, steps: 10\n",
            "Episode 97: reward: -95.991, steps: 10\n",
            "Episode 98: reward: -97.512, steps: 6\n",
            "Episode 99: reward: -97.612, steps: 6\n",
            "Episode 100: reward: -97.662, steps: 6\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 53 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.01, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_52 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_130 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_131 (Dense)           (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_132 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,069\n",
            "Trainable params: 5,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 242s 24ms/step - reward: -20.4188\n",
            "2076 episodes - episode_reward: -98.358 [-100.142, -92.016] - loss: 44.855 - mae: 85.309 - mean_q: -77.814\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 249s 25ms/step - reward: -14.9895\n",
            "1537 episodes - episode_reward: -97.523 [-100.142, -93.090] - loss: 9.030 - mae: 89.381 - mean_q: -84.961\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 247s 25ms/step - reward: -13.8154\n",
            "done, took 738.231 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.512, steps: 6\n",
            "Episode 2: reward: -97.462, steps: 6\n",
            "Episode 3: reward: -96.025, steps: 10\n",
            "Episode 4: reward: -97.612, steps: 6\n",
            "Episode 5: reward: -95.701, steps: 10\n",
            "Episode 6: reward: -95.444, steps: 9\n",
            "Episode 7: reward: -96.509, steps: 9\n",
            "Episode 8: reward: -95.771, steps: 10\n",
            "Episode 9: reward: -99.378, steps: 4\n",
            "Episode 10: reward: -93.447, steps: 14\n",
            "Episode 11: reward: -99.647, steps: 2\n",
            "Episode 12: reward: -97.262, steps: 6\n",
            "Episode 13: reward: -94.093, steps: 14\n",
            "Episode 14: reward: -97.612, steps: 6\n",
            "Episode 15: reward: -97.562, steps: 6\n",
            "Episode 16: reward: -97.462, steps: 6\n",
            "Episode 17: reward: -97.923, steps: 6\n",
            "Episode 18: reward: -98.676, steps: 3\n",
            "Episode 19: reward: -96.358, steps: 10\n",
            "Episode 20: reward: -97.713, steps: 7\n",
            "Episode 21: reward: -93.723, steps: 14\n",
            "Episode 22: reward: -95.851, steps: 10\n",
            "Episode 23: reward: -96.364, steps: 9\n",
            "Episode 24: reward: -95.788, steps: 10\n",
            "Episode 25: reward: -98.916, steps: 3\n",
            "Episode 26: reward: -95.791, steps: 10\n",
            "Episode 27: reward: -95.178, steps: 10\n",
            "Episode 28: reward: -98.596, steps: 3\n",
            "Episode 29: reward: -96.920, steps: 7\n",
            "Episode 30: reward: -95.915, steps: 10\n",
            "Episode 31: reward: -94.567, steps: 14\n",
            "Episode 32: reward: -96.121, steps: 9\n",
            "Episode 33: reward: -97.562, steps: 6\n",
            "Episode 34: reward: -95.978, steps: 10\n",
            "Episode 35: reward: -93.643, steps: 14\n",
            "Episode 36: reward: -97.362, steps: 6\n",
            "Episode 37: reward: -96.271, steps: 10\n",
            "Episode 38: reward: -97.412, steps: 6\n",
            "Episode 39: reward: -94.907, steps: 12\n",
            "Episode 40: reward: -95.801, steps: 10\n",
            "Episode 41: reward: -97.712, steps: 6\n",
            "Episode 42: reward: -95.898, steps: 10\n",
            "Episode 43: reward: -98.362, steps: 6\n",
            "Episode 44: reward: -97.412, steps: 6\n",
            "Episode 45: reward: -97.512, steps: 6\n",
            "Episode 46: reward: -97.412, steps: 6\n",
            "Episode 47: reward: -93.307, steps: 14\n",
            "Episode 48: reward: -97.273, steps: 7\n",
            "Episode 49: reward: -97.412, steps: 6\n",
            "Episode 50: reward: -97.767, steps: 7\n",
            "Episode 51: reward: -97.712, steps: 6\n",
            "Episode 52: reward: -97.605, steps: 5\n",
            "Episode 53: reward: -97.873, steps: 7\n",
            "Episode 54: reward: -94.103, steps: 14\n",
            "Episode 55: reward: -97.612, steps: 6\n",
            "Episode 56: reward: -97.713, steps: 7\n",
            "Episode 57: reward: -98.362, steps: 6\n",
            "Episode 58: reward: -97.607, steps: 7\n",
            "Episode 59: reward: -95.511, steps: 10\n",
            "Episode 60: reward: -94.152, steps: 13\n",
            "Episode 61: reward: -95.898, steps: 10\n",
            "Episode 62: reward: -99.647, steps: 2\n",
            "Episode 63: reward: -97.112, steps: 6\n",
            "Episode 64: reward: -97.713, steps: 7\n",
            "Episode 65: reward: -97.712, steps: 6\n",
            "Episode 66: reward: -96.364, steps: 9\n",
            "Episode 67: reward: -96.594, steps: 9\n",
            "Episode 68: reward: -97.262, steps: 6\n",
            "Episode 69: reward: -97.180, steps: 7\n",
            "Episode 70: reward: -96.304, steps: 9\n",
            "Episode 71: reward: -96.653, steps: 7\n",
            "Episode 72: reward: -97.927, steps: 7\n",
            "Episode 73: reward: -95.538, steps: 10\n",
            "Episode 74: reward: -97.462, steps: 6\n",
            "Episode 75: reward: -97.673, steps: 6\n",
            "Episode 76: reward: -97.812, steps: 6\n",
            "Episode 77: reward: -95.855, steps: 10\n",
            "Episode 78: reward: -94.965, steps: 10\n",
            "Episode 79: reward: -96.278, steps: 10\n",
            "Episode 80: reward: -95.955, steps: 10\n",
            "Episode 81: reward: -94.861, steps: 10\n",
            "Episode 82: reward: -96.274, steps: 9\n",
            "Episode 83: reward: -97.662, steps: 6\n",
            "Episode 84: reward: -97.712, steps: 6\n",
            "Episode 85: reward: -97.612, steps: 6\n",
            "Episode 86: reward: -97.923, steps: 6\n",
            "Episode 87: reward: -97.512, steps: 6\n",
            "Episode 88: reward: -95.178, steps: 10\n",
            "Episode 89: reward: -93.567, steps: 14\n",
            "Episode 90: reward: -97.712, steps: 6\n",
            "Episode 91: reward: -96.105, steps: 10\n",
            "Episode 92: reward: -97.287, steps: 7\n",
            "Episode 93: reward: -97.839, steps: 5\n",
            "Episode 94: reward: -97.712, steps: 6\n",
            "Episode 95: reward: -96.029, steps: 9\n",
            "Episode 96: reward: -94.828, steps: 10\n",
            "Episode 97: reward: -99.647, steps: 2\n",
            "Episode 98: reward: -96.171, steps: 10\n",
            "Episode 99: reward: -97.812, steps: 6\n",
            "Episode 100: reward: -97.762, steps: 6\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 54 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.001, 'adam_epsilon': 0.1, 'batch_size': 32, 'target_model_update': 0.001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_53 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_133 (Dense)           (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_134 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,829\n",
            "Trainable params: 2,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 245s 24ms/step - reward: -20.9043\n",
            "2127 episodes - episode_reward: -98.281 [-100.142, -93.136] - loss: 206.762 - mae: 59.852 - mean_q: -40.217\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 257s 26ms/step - reward: -20.9464\n",
            "2130 episodes - episode_reward: -98.341 [-100.142, -94.168] - loss: 31.369 - mae: 89.664 - mean_q: -82.675\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 256s 26ms/step - reward: -18.7070\n",
            "done, took 759.241 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.941, steps: 5\n",
            "Episode 2: reward: -98.022, steps: 6\n",
            "Episode 3: reward: -97.891, steps: 5\n",
            "Episode 4: reward: -97.991, steps: 5\n",
            "Episode 5: reward: -97.256, steps: 7\n",
            "Episode 6: reward: -97.040, steps: 9\n",
            "Episode 7: reward: -98.698, steps: 4\n",
            "Episode 8: reward: -97.891, steps: 5\n",
            "Episode 9: reward: -96.512, steps: 8\n",
            "Episode 10: reward: -97.443, steps: 7\n",
            "Episode 11: reward: -99.647, steps: 2\n",
            "Episode 12: reward: -96.428, steps: 9\n",
            "Episode 13: reward: -96.844, steps: 8\n",
            "Episode 14: reward: -97.741, steps: 5\n",
            "Episode 15: reward: -99.647, steps: 2\n",
            "Episode 16: reward: -98.289, steps: 4\n",
            "Episode 17: reward: -97.067, steps: 8\n",
            "Episode 18: reward: -98.041, steps: 5\n",
            "Episode 19: reward: -96.519, steps: 10\n",
            "Episode 20: reward: -99.647, steps: 2\n",
            "Episode 21: reward: -97.662, steps: 6\n",
            "Episode 22: reward: -98.628, steps: 6\n",
            "Episode 23: reward: -96.844, steps: 8\n",
            "Episode 24: reward: -97.891, steps: 5\n",
            "Episode 25: reward: -98.236, steps: 5\n",
            "Episode 26: reward: -97.062, steps: 8\n",
            "Episode 27: reward: -96.675, steps: 8\n",
            "Episode 28: reward: -97.412, steps: 6\n",
            "Episode 29: reward: -96.231, steps: 8\n",
            "Episode 30: reward: -96.300, steps: 9\n",
            "Episode 31: reward: -97.208, steps: 8\n",
            "Episode 32: reward: -96.671, steps: 9\n",
            "Episode 33: reward: -99.647, steps: 2\n",
            "Episode 34: reward: -96.177, steps: 9\n",
            "Episode 35: reward: -97.891, steps: 5\n",
            "Episode 36: reward: -97.991, steps: 5\n",
            "Episode 37: reward: -99.305, steps: 4\n",
            "Episode 38: reward: -96.249, steps: 10\n",
            "Episode 39: reward: -96.258, steps: 10\n",
            "Episode 40: reward: -97.991, steps: 5\n",
            "Episode 41: reward: -97.891, steps: 5\n",
            "Episode 42: reward: -99.647, steps: 2\n",
            "Episode 43: reward: -98.383, steps: 4\n",
            "Episode 44: reward: -97.406, steps: 7\n",
            "Episode 45: reward: -98.562, steps: 6\n",
            "Episode 46: reward: -97.791, steps: 5\n",
            "Episode 47: reward: -97.841, steps: 5\n",
            "Episode 48: reward: -97.991, steps: 5\n",
            "Episode 49: reward: -98.429, steps: 4\n",
            "Episode 50: reward: -96.724, steps: 9\n",
            "Episode 51: reward: -97.106, steps: 7\n",
            "Episode 52: reward: -96.962, steps: 8\n",
            "Episode 53: reward: -99.305, steps: 4\n",
            "Episode 54: reward: -96.258, steps: 10\n",
            "Episode 55: reward: -97.791, steps: 5\n",
            "Episode 56: reward: -97.941, steps: 5\n",
            "Episode 57: reward: -97.791, steps: 5\n",
            "Episode 58: reward: -98.041, steps: 5\n",
            "Episode 59: reward: -96.158, steps: 10\n",
            "Episode 60: reward: -97.991, steps: 5\n",
            "Episode 61: reward: -99.647, steps: 2\n",
            "Episode 62: reward: -99.024, steps: 5\n",
            "Episode 63: reward: -95.918, steps: 10\n",
            "Episode 64: reward: -97.841, steps: 5\n",
            "Episode 65: reward: -97.891, steps: 5\n",
            "Episode 66: reward: -99.647, steps: 2\n",
            "Episode 67: reward: -96.578, steps: 9\n",
            "Episode 68: reward: -99.647, steps: 2\n",
            "Episode 69: reward: -98.698, steps: 4\n",
            "Episode 70: reward: -97.941, steps: 5\n",
            "Episode 71: reward: -99.756, steps: 3\n",
            "Episode 72: reward: -96.248, steps: 10\n",
            "Episode 73: reward: -97.991, steps: 5\n",
            "Episode 74: reward: -97.791, steps: 5\n",
            "Episode 75: reward: -97.682, steps: 6\n",
            "Episode 76: reward: -97.948, steps: 6\n",
            "Episode 77: reward: -95.246, steps: 13\n",
            "Episode 78: reward: -97.941, steps: 5\n",
            "Episode 79: reward: -97.522, steps: 6\n",
            "Episode 80: reward: -96.036, steps: 10\n",
            "Episode 81: reward: -96.920, steps: 9\n",
            "Episode 82: reward: -96.291, steps: 8\n",
            "Episode 83: reward: -96.890, steps: 9\n",
            "Episode 84: reward: -96.774, steps: 8\n",
            "Episode 85: reward: -97.991, steps: 5\n",
            "Episode 86: reward: -97.741, steps: 5\n",
            "Episode 87: reward: -97.941, steps: 5\n",
            "Episode 88: reward: -97.941, steps: 5\n",
            "Episode 89: reward: -96.844, steps: 8\n",
            "Episode 90: reward: -95.868, steps: 10\n",
            "Episode 91: reward: -97.791, steps: 5\n",
            "Episode 92: reward: -97.108, steps: 8\n",
            "Episode 93: reward: -98.562, steps: 6\n",
            "Episode 94: reward: -96.493, steps: 9\n",
            "Episode 95: reward: -98.698, steps: 4\n",
            "Episode 96: reward: -97.941, steps: 5\n",
            "Episode 97: reward: -98.336, steps: 4\n",
            "Episode 98: reward: -99.295, steps: 3\n",
            "Episode 99: reward: -97.691, steps: 5\n",
            "Episode 100: reward: -96.908, steps: 8\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 55 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.00146, 'adam_epsilon': 0.1, 'batch_size': 32, 'target_model_update': 0.005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_54 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_135 (Dense)           (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_136 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,829\n",
            "Trainable params: 2,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 245s 24ms/step - reward: -21.9831\n",
            "2233 episodes - episode_reward: -98.447 [-100.142, -94.277] - loss: 95.298 - mae: 81.246 - mean_q: -70.809\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 254s 25ms/step - reward: -18.0801\n",
            "1845 episodes - episode_reward: -97.996 [-100.142, -93.137] - loss: 10.294 - mae: 91.738 - mean_q: -87.994\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 249s 25ms/step - reward: -15.7163\n",
            "done, took 748.657 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -94.874, steps: 11\n",
            "Episode 2: reward: -96.871, steps: 8\n",
            "Episode 3: reward: -96.067, steps: 9\n",
            "Episode 4: reward: -98.352, steps: 5\n",
            "Episode 5: reward: -99.647, steps: 2\n",
            "Episode 6: reward: -95.988, steps: 10\n",
            "Episode 7: reward: -98.191, steps: 5\n",
            "Episode 8: reward: -97.941, steps: 5\n",
            "Episode 9: reward: -96.007, steps: 9\n",
            "Episode 10: reward: -97.691, steps: 5\n",
            "Episode 11: reward: -96.130, steps: 9\n",
            "Episode 12: reward: -97.941, steps: 5\n",
            "Episode 13: reward: -99.647, steps: 2\n",
            "Episode 14: reward: -96.367, steps: 9\n",
            "Episode 15: reward: -97.891, steps: 5\n",
            "Episode 16: reward: -97.186, steps: 7\n",
            "Episode 17: reward: -98.429, steps: 4\n",
            "Episode 18: reward: -97.941, steps: 5\n",
            "Episode 19: reward: -96.256, steps: 11\n",
            "Episode 20: reward: -96.447, steps: 9\n",
            "Episode 21: reward: -95.457, steps: 9\n",
            "Episode 22: reward: -96.128, steps: 10\n",
            "Episode 23: reward: -97.891, steps: 5\n",
            "Episode 24: reward: -96.260, steps: 9\n",
            "Episode 25: reward: -99.647, steps: 2\n",
            "Episode 26: reward: -96.541, steps: 8\n",
            "Episode 27: reward: -98.191, steps: 5\n",
            "Episode 28: reward: -94.305, steps: 13\n",
            "Episode 29: reward: -95.820, steps: 11\n",
            "Episode 30: reward: -96.670, steps: 9\n",
            "Episode 31: reward: -96.483, steps: 9\n",
            "Episode 32: reward: -97.841, steps: 5\n",
            "Episode 33: reward: -97.562, steps: 6\n",
            "Episode 34: reward: -96.751, steps: 8\n",
            "Episode 35: reward: -96.490, steps: 9\n",
            "Episode 36: reward: -96.257, steps: 9\n",
            "Episode 37: reward: -98.289, steps: 4\n",
            "Episode 38: reward: -99.647, steps: 2\n",
            "Episode 39: reward: -97.691, steps: 5\n",
            "Episode 40: reward: -99.647, steps: 2\n",
            "Episode 41: reward: -98.289, steps: 4\n",
            "Episode 42: reward: -96.193, steps: 9\n",
            "Episode 43: reward: -98.103, steps: 4\n",
            "Episode 44: reward: -96.827, steps: 9\n",
            "Episode 45: reward: -96.447, steps: 9\n",
            "Episode 46: reward: -99.398, steps: 4\n",
            "Episode 47: reward: -96.007, steps: 9\n",
            "Episode 48: reward: -97.891, steps: 5\n",
            "Episode 49: reward: -97.991, steps: 5\n",
            "Episode 50: reward: -96.383, steps: 9\n",
            "Episode 51: reward: -96.931, steps: 8\n",
            "Episode 52: reward: -96.827, steps: 9\n",
            "Episode 53: reward: -96.447, steps: 9\n",
            "Episode 54: reward: -96.700, steps: 9\n",
            "Episode 55: reward: -94.331, steps: 13\n",
            "Episode 56: reward: -99.647, steps: 2\n",
            "Episode 57: reward: -97.991, steps: 5\n",
            "Episode 58: reward: -96.747, steps: 9\n",
            "Episode 59: reward: -97.991, steps: 5\n",
            "Episode 60: reward: -96.661, steps: 8\n",
            "Episode 61: reward: -97.783, steps: 6\n",
            "Episode 62: reward: -98.141, steps: 5\n",
            "Episode 63: reward: -96.447, steps: 9\n",
            "Episode 64: reward: -98.462, steps: 6\n",
            "Episode 65: reward: -97.841, steps: 5\n",
            "Episode 66: reward: -96.487, steps: 9\n",
            "Episode 67: reward: -98.462, steps: 6\n",
            "Episode 68: reward: -96.170, steps: 9\n",
            "Episode 69: reward: -97.991, steps: 5\n",
            "Episode 70: reward: -96.510, steps: 9\n",
            "Episode 71: reward: -97.991, steps: 5\n",
            "Episode 72: reward: -95.917, steps: 9\n",
            "Episode 73: reward: -96.260, steps: 9\n",
            "Episode 74: reward: -96.133, steps: 9\n",
            "Episode 75: reward: -95.520, steps: 9\n",
            "Episode 76: reward: -96.360, steps: 9\n",
            "Episode 77: reward: -96.700, steps: 9\n",
            "Episode 78: reward: -98.091, steps: 5\n",
            "Episode 79: reward: -97.201, steps: 8\n",
            "Episode 80: reward: -98.618, steps: 4\n",
            "Episode 81: reward: -98.149, steps: 4\n",
            "Episode 82: reward: -97.741, steps: 5\n",
            "Episode 83: reward: -96.810, steps: 9\n",
            "Episode 84: reward: -99.647, steps: 2\n",
            "Episode 85: reward: -97.356, steps: 7\n",
            "Episode 86: reward: -98.041, steps: 5\n",
            "Episode 87: reward: -96.383, steps: 9\n",
            "Episode 88: reward: -99.398, steps: 4\n",
            "Episode 89: reward: -98.191, steps: 5\n",
            "Episode 90: reward: -97.589, steps: 7\n",
            "Episode 91: reward: -95.580, steps: 11\n",
            "Episode 92: reward: -96.130, steps: 9\n",
            "Episode 93: reward: -97.941, steps: 5\n",
            "Episode 94: reward: -93.805, steps: 13\n",
            "Episode 95: reward: -96.378, steps: 10\n",
            "Episode 96: reward: -97.991, steps: 5\n",
            "Episode 97: reward: -97.051, steps: 8\n",
            "Episode 98: reward: -96.931, steps: 8\n",
            "Episode 99: reward: -96.088, steps: 10\n",
            "Episode 100: reward: -96.170, steps: 9\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 56 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.005, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_55\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_55 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_137 (Dense)           (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_138 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,829\n",
            "Trainable params: 2,829\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 246s 24ms/step - reward: -20.3000\n",
            "2066 episodes - episode_reward: -98.258 [-100.129, -92.770] - loss: 126.830 - mae: 65.214 - mean_q: -41.353\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 256s 26ms/step - reward: -17.4192\n",
            "1777 episodes - episode_reward: -98.026 [-100.142, -92.367] - loss: 41.513 - mae: 82.976 - mean_q: -72.329\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 253s 25ms/step - reward: -17.2605\n",
            "done, took 755.647 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -99.295, steps: 3\n",
            "Episode 2: reward: -97.973, steps: 7\n",
            "Episode 3: reward: -96.160, steps: 9\n",
            "Episode 4: reward: -99.647, steps: 2\n",
            "Episode 5: reward: -98.140, steps: 7\n",
            "Episode 6: reward: -97.973, steps: 7\n",
            "Episode 7: reward: -97.554, steps: 8\n",
            "Episode 8: reward: -98.628, steps: 6\n",
            "Episode 9: reward: -97.906, steps: 7\n",
            "Episode 10: reward: -98.958, steps: 5\n",
            "Episode 11: reward: -98.140, steps: 7\n",
            "Episode 12: reward: -98.156, steps: 7\n",
            "Episode 13: reward: -98.362, steps: 6\n",
            "Episode 14: reward: -95.621, steps: 8\n",
            "Episode 15: reward: -97.603, steps: 9\n",
            "Episode 16: reward: -97.940, steps: 7\n",
            "Episode 17: reward: -98.958, steps: 5\n",
            "Episode 18: reward: -99.974, steps: 1\n",
            "Episode 19: reward: -96.493, steps: 9\n",
            "Episode 20: reward: -98.528, steps: 6\n",
            "Episode 21: reward: -99.295, steps: 3\n",
            "Episode 22: reward: -98.123, steps: 7\n",
            "Episode 23: reward: -99.974, steps: 1\n",
            "Episode 24: reward: -98.341, steps: 5\n",
            "Episode 25: reward: -97.306, steps: 7\n",
            "Episode 26: reward: -96.807, steps: 8\n",
            "Episode 27: reward: -96.447, steps: 8\n",
            "Episode 28: reward: -98.190, steps: 7\n",
            "Episode 29: reward: -97.154, steps: 8\n",
            "Episode 30: reward: -96.906, steps: 10\n",
            "Episode 31: reward: -97.306, steps: 7\n",
            "Episode 32: reward: -97.206, steps: 7\n",
            "Episode 33: reward: -95.796, steps: 11\n",
            "Episode 34: reward: -98.562, steps: 6\n",
            "Episode 35: reward: -99.295, steps: 3\n",
            "Episode 36: reward: -94.821, steps: 12\n",
            "Episode 37: reward: -99.295, steps: 3\n",
            "Episode 38: reward: -98.140, steps: 7\n",
            "Episode 39: reward: -97.973, steps: 7\n",
            "Episode 40: reward: -99.295, steps: 3\n",
            "Episode 41: reward: -98.140, steps: 7\n",
            "Episode 42: reward: -97.973, steps: 7\n",
            "Episode 43: reward: -98.140, steps: 7\n",
            "Episode 44: reward: -99.295, steps: 3\n",
            "Episode 45: reward: -97.637, steps: 9\n",
            "Episode 46: reward: -99.074, steps: 5\n",
            "Episode 47: reward: -97.662, steps: 6\n",
            "Episode 48: reward: -98.123, steps: 7\n",
            "Episode 49: reward: -98.140, steps: 7\n",
            "Episode 50: reward: -96.966, steps: 7\n",
            "Episode 51: reward: -98.416, steps: 5\n",
            "Episode 52: reward: -99.295, steps: 3\n",
            "Episode 53: reward: -98.958, steps: 5\n",
            "Episode 54: reward: -94.992, steps: 13\n",
            "Episode 55: reward: -97.973, steps: 7\n",
            "Episode 56: reward: -99.295, steps: 3\n",
            "Episode 57: reward: -97.973, steps: 7\n",
            "Episode 58: reward: -98.362, steps: 6\n",
            "Episode 59: reward: -98.123, steps: 7\n",
            "Episode 60: reward: -99.295, steps: 3\n",
            "Episode 61: reward: -98.562, steps: 6\n",
            "Episode 62: reward: -97.803, steps: 9\n",
            "Episode 63: reward: -98.123, steps: 7\n",
            "Episode 64: reward: -97.657, steps: 9\n",
            "Episode 65: reward: -97.940, steps: 7\n",
            "Episode 66: reward: -99.647, steps: 2\n",
            "Episode 67: reward: -96.240, steps: 9\n",
            "Episode 68: reward: -98.140, steps: 7\n",
            "Episode 69: reward: -95.974, steps: 8\n",
            "Episode 70: reward: -98.106, steps: 7\n",
            "Episode 71: reward: -97.973, steps: 7\n",
            "Episode 72: reward: -98.140, steps: 7\n",
            "Episode 73: reward: -97.941, steps: 5\n",
            "Episode 74: reward: -98.140, steps: 7\n",
            "Episode 75: reward: -99.647, steps: 2\n",
            "Episode 76: reward: -99.295, steps: 3\n",
            "Episode 77: reward: -97.306, steps: 7\n",
            "Episode 78: reward: -96.510, steps: 9\n",
            "Episode 79: reward: -96.807, steps: 8\n",
            "Episode 80: reward: -96.557, steps: 9\n",
            "Episode 81: reward: -98.569, steps: 4\n",
            "Episode 82: reward: -98.140, steps: 7\n",
            "Episode 83: reward: -95.407, steps: 9\n",
            "Episode 84: reward: -99.295, steps: 3\n",
            "Episode 85: reward: -95.606, steps: 11\n",
            "Episode 86: reward: -98.341, steps: 5\n",
            "Episode 87: reward: -97.803, steps: 9\n",
            "Episode 88: reward: -99.295, steps: 3\n",
            "Episode 89: reward: -98.362, steps: 6\n",
            "Episode 90: reward: -96.511, steps: 10\n",
            "Episode 91: reward: -98.140, steps: 7\n",
            "Episode 92: reward: -96.693, steps: 8\n",
            "Episode 93: reward: -99.295, steps: 3\n",
            "Episode 94: reward: -98.123, steps: 7\n",
            "Episode 95: reward: -99.295, steps: 3\n",
            "Episode 96: reward: -99.411, steps: 4\n",
            "Episode 97: reward: -96.867, steps: 8\n",
            "Episode 98: reward: -98.495, steps: 6\n",
            "Episode 99: reward: -97.940, steps: 7\n",
            "Episode 100: reward: -98.140, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 57 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 1e-05, 'batch_size': 32, 'target_model_update': 0.005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_56\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_56 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_139 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_140 (Dense)           (None, 16)                784       \n",
            "                                                                 \n",
            " dense_141 (Dense)           (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,565\n",
            "Trainable params: 3,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 251s 25ms/step - reward: -19.2359\n",
            "1959 episodes - episode_reward: -98.193 [-100.142, -93.478] - loss: 53.169 - mae: 82.534 - mean_q: -72.236\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 261s 26ms/step - reward: -16.8513\n",
            "1722 episodes - episode_reward: -97.858 [-100.142, -93.596] - loss: 5.262 - mae: 91.592 - mean_q: -88.438\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 260s 26ms/step - reward: -16.3383\n",
            "done, took 771.468 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.056, steps: 7\n",
            "Episode 2: reward: -96.956, steps: 7\n",
            "Episode 3: reward: -96.906, steps: 7\n",
            "Episode 4: reward: -97.006, steps: 7\n",
            "Episode 5: reward: -97.156, steps: 7\n",
            "Episode 6: reward: -97.456, steps: 7\n",
            "Episode 7: reward: -96.856, steps: 7\n",
            "Episode 8: reward: -97.406, steps: 7\n",
            "Episode 9: reward: -96.956, steps: 7\n",
            "Episode 10: reward: -96.806, steps: 7\n",
            "Episode 11: reward: -97.006, steps: 7\n",
            "Episode 12: reward: -97.006, steps: 7\n",
            "Episode 13: reward: -97.006, steps: 7\n",
            "Episode 14: reward: -97.306, steps: 7\n",
            "Episode 15: reward: -97.106, steps: 7\n",
            "Episode 16: reward: -97.056, steps: 7\n",
            "Episode 17: reward: -97.106, steps: 7\n",
            "Episode 18: reward: -97.006, steps: 7\n",
            "Episode 19: reward: -97.106, steps: 7\n",
            "Episode 20: reward: -97.456, steps: 7\n",
            "Episode 21: reward: -97.006, steps: 7\n",
            "Episode 22: reward: -97.106, steps: 7\n",
            "Episode 23: reward: -97.156, steps: 7\n",
            "Episode 24: reward: -97.056, steps: 7\n",
            "Episode 25: reward: -97.006, steps: 7\n",
            "Episode 26: reward: -97.056, steps: 7\n",
            "Episode 27: reward: -97.206, steps: 7\n",
            "Episode 28: reward: -97.006, steps: 7\n",
            "Episode 29: reward: -96.956, steps: 7\n",
            "Episode 30: reward: -97.306, steps: 7\n",
            "Episode 31: reward: -97.356, steps: 7\n",
            "Episode 32: reward: -96.856, steps: 7\n",
            "Episode 33: reward: -96.806, steps: 7\n",
            "Episode 34: reward: -97.506, steps: 7\n",
            "Episode 35: reward: -97.056, steps: 7\n",
            "Episode 36: reward: -97.206, steps: 7\n",
            "Episode 37: reward: -97.156, steps: 7\n",
            "Episode 38: reward: -96.956, steps: 7\n",
            "Episode 39: reward: -97.406, steps: 7\n",
            "Episode 40: reward: -97.056, steps: 7\n",
            "Episode 41: reward: -97.356, steps: 7\n",
            "Episode 42: reward: -97.056, steps: 7\n",
            "Episode 43: reward: -97.206, steps: 7\n",
            "Episode 44: reward: -97.056, steps: 7\n",
            "Episode 45: reward: -97.156, steps: 7\n",
            "Episode 46: reward: -97.506, steps: 7\n",
            "Episode 47: reward: -97.256, steps: 7\n",
            "Episode 48: reward: -97.056, steps: 7\n",
            "Episode 49: reward: -97.306, steps: 7\n",
            "Episode 50: reward: -97.156, steps: 7\n",
            "Episode 51: reward: -96.856, steps: 7\n",
            "Episode 52: reward: -97.256, steps: 7\n",
            "Episode 53: reward: -97.106, steps: 7\n",
            "Episode 54: reward: -97.156, steps: 7\n",
            "Episode 55: reward: -97.406, steps: 7\n",
            "Episode 56: reward: -97.206, steps: 7\n",
            "Episode 57: reward: -96.956, steps: 7\n",
            "Episode 58: reward: -96.806, steps: 7\n",
            "Episode 59: reward: -96.906, steps: 7\n",
            "Episode 60: reward: -96.806, steps: 7\n",
            "Episode 61: reward: -97.056, steps: 7\n",
            "Episode 62: reward: -97.156, steps: 7\n",
            "Episode 63: reward: -97.106, steps: 7\n",
            "Episode 64: reward: -97.406, steps: 7\n",
            "Episode 65: reward: -97.356, steps: 7\n",
            "Episode 66: reward: -97.106, steps: 7\n",
            "Episode 67: reward: -97.206, steps: 7\n",
            "Episode 68: reward: -97.106, steps: 7\n",
            "Episode 69: reward: -97.356, steps: 7\n",
            "Episode 70: reward: -97.006, steps: 7\n",
            "Episode 71: reward: -97.306, steps: 7\n",
            "Episode 72: reward: -97.006, steps: 7\n",
            "Episode 73: reward: -97.256, steps: 7\n",
            "Episode 74: reward: -97.256, steps: 7\n",
            "Episode 75: reward: -97.256, steps: 7\n",
            "Episode 76: reward: -97.156, steps: 7\n",
            "Episode 77: reward: -97.056, steps: 7\n",
            "Episode 78: reward: -97.656, steps: 7\n",
            "Episode 79: reward: -97.206, steps: 7\n",
            "Episode 80: reward: -96.906, steps: 7\n",
            "Episode 81: reward: -97.506, steps: 7\n",
            "Episode 82: reward: -97.156, steps: 7\n",
            "Episode 83: reward: -97.106, steps: 7\n",
            "Episode 84: reward: -97.006, steps: 7\n",
            "Episode 85: reward: -96.756, steps: 7\n",
            "Episode 86: reward: -97.406, steps: 7\n",
            "Episode 87: reward: -97.006, steps: 7\n",
            "Episode 88: reward: -96.906, steps: 7\n",
            "Episode 89: reward: -97.106, steps: 7\n",
            "Episode 90: reward: -96.906, steps: 7\n",
            "Episode 91: reward: -96.806, steps: 7\n",
            "Episode 92: reward: -97.406, steps: 7\n",
            "Episode 93: reward: -96.956, steps: 7\n",
            "Episode 94: reward: -97.156, steps: 7\n",
            "Episode 95: reward: -96.906, steps: 7\n",
            "Episode 96: reward: -97.406, steps: 7\n",
            "Episode 97: reward: -97.056, steps: 7\n",
            "Episode 98: reward: -97.206, steps: 7\n",
            "Episode 99: reward: -97.306, steps: 7\n",
            "Episode 100: reward: -97.106, steps: 7\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 58 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.01, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_57\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_57 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_142 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_143 (Dense)           (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_144 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,069\n",
            "Trainable params: 5,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 259s 26ms/step - reward: -20.7445\n",
            "2107 episodes - episode_reward: -98.456 [-100.142, -93.806] - loss: 72.555 - mae: 76.745 - mean_q: -61.024\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 261s 26ms/step - reward: -16.7677\n",
            "1710 episodes - episode_reward: -98.056 [-100.142, -92.326] - loss: 21.056 - mae: 85.749 - mean_q: -79.066\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 262s 26ms/step - reward: -15.8632\n",
            "done, took 783.068 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -96.440, steps: 9\n",
            "Episode 2: reward: -95.547, steps: 11\n",
            "Episode 3: reward: -97.356, steps: 7\n",
            "Episode 4: reward: -94.448, steps: 10\n",
            "Episode 5: reward: -95.178, steps: 10\n",
            "Episode 6: reward: -97.156, steps: 7\n",
            "Episode 7: reward: -97.612, steps: 6\n",
            "Episode 8: reward: -97.206, steps: 7\n",
            "Episode 9: reward: -97.562, steps: 6\n",
            "Episode 10: reward: -98.243, steps: 4\n",
            "Episode 11: reward: -97.156, steps: 7\n",
            "Episode 12: reward: -97.363, steps: 6\n",
            "Episode 13: reward: -99.251, steps: 4\n",
            "Episode 14: reward: -97.891, steps: 5\n",
            "Episode 15: reward: -95.908, steps: 10\n",
            "Episode 16: reward: -97.991, steps: 5\n",
            "Episode 17: reward: -97.891, steps: 5\n",
            "Episode 18: reward: -97.562, steps: 6\n",
            "Episode 19: reward: -97.712, steps: 6\n",
            "Episode 20: reward: -98.141, steps: 5\n",
            "Episode 21: reward: -95.810, steps: 11\n",
            "Episode 22: reward: -97.256, steps: 7\n",
            "Episode 23: reward: -97.456, steps: 7\n",
            "Episode 24: reward: -98.289, steps: 4\n",
            "Episode 25: reward: -98.243, steps: 4\n",
            "Episode 26: reward: -97.712, steps: 6\n",
            "Episode 27: reward: -97.991, steps: 5\n",
            "Episode 28: reward: -97.406, steps: 7\n",
            "Episode 29: reward: -97.356, steps: 7\n",
            "Episode 30: reward: -98.958, steps: 5\n",
            "Episode 31: reward: -97.456, steps: 7\n",
            "Episode 32: reward: -97.156, steps: 7\n",
            "Episode 33: reward: -97.712, steps: 6\n",
            "Episode 34: reward: -99.074, steps: 5\n",
            "Episode 35: reward: -97.941, steps: 5\n",
            "Episode 36: reward: -98.103, steps: 4\n",
            "Episode 37: reward: -96.640, steps: 9\n",
            "Episode 38: reward: -97.662, steps: 6\n",
            "Episode 39: reward: -97.862, steps: 6\n",
            "Episode 40: reward: -96.890, steps: 9\n",
            "Episode 41: reward: -98.645, steps: 6\n",
            "Episode 42: reward: -97.891, steps: 5\n",
            "Episode 43: reward: -97.206, steps: 7\n",
            "Episode 44: reward: -95.808, steps: 10\n",
            "Episode 45: reward: -97.812, steps: 6\n",
            "Episode 46: reward: -97.762, steps: 6\n",
            "Episode 47: reward: -95.460, steps: 11\n",
            "Episode 48: reward: -98.173, steps: 7\n",
            "Episode 49: reward: -97.891, steps: 5\n",
            "Episode 50: reward: -97.841, steps: 5\n",
            "Episode 51: reward: -97.712, steps: 6\n",
            "Episode 52: reward: -97.006, steps: 7\n",
            "Episode 53: reward: -97.606, steps: 7\n",
            "Episode 54: reward: -97.762, steps: 6\n",
            "Episode 55: reward: -97.791, steps: 5\n",
            "Episode 56: reward: -95.580, steps: 11\n",
            "Episode 57: reward: -97.941, steps: 5\n",
            "Episode 58: reward: -97.762, steps: 6\n",
            "Episode 59: reward: -97.006, steps: 7\n",
            "Episode 60: reward: -97.891, steps: 5\n",
            "Episode 61: reward: -97.941, steps: 5\n",
            "Episode 62: reward: -96.956, steps: 7\n",
            "Episode 63: reward: -98.383, steps: 4\n",
            "Episode 64: reward: -98.336, steps: 4\n",
            "Episode 65: reward: -96.670, steps: 9\n",
            "Episode 66: reward: -97.741, steps: 5\n",
            "Episode 67: reward: -96.060, steps: 9\n",
            "Episode 68: reward: -97.106, steps: 7\n",
            "Episode 69: reward: -97.762, steps: 6\n",
            "Episode 70: reward: -97.503, steps: 6\n",
            "Episode 71: reward: -97.691, steps: 5\n",
            "Episode 72: reward: -98.041, steps: 5\n",
            "Episode 73: reward: -97.762, steps: 6\n",
            "Episode 74: reward: -97.741, steps: 5\n",
            "Episode 75: reward: -98.336, steps: 4\n",
            "Episode 76: reward: -96.488, steps: 10\n",
            "Episode 77: reward: -96.640, steps: 9\n",
            "Episode 78: reward: -98.958, steps: 5\n",
            "Episode 79: reward: -95.660, steps: 9\n",
            "Episode 80: reward: -97.841, steps: 5\n",
            "Episode 81: reward: -97.940, steps: 7\n",
            "Episode 82: reward: -96.008, steps: 10\n",
            "Episode 83: reward: -97.841, steps: 5\n",
            "Episode 84: reward: -97.156, steps: 7\n",
            "Episode 85: reward: -95.808, steps: 10\n",
            "Episode 86: reward: -98.141, steps: 5\n",
            "Episode 87: reward: -97.356, steps: 7\n",
            "Episode 88: reward: -97.841, steps: 5\n",
            "Episode 89: reward: -97.512, steps: 6\n",
            "Episode 90: reward: -96.956, steps: 7\n",
            "Episode 91: reward: -97.941, steps: 5\n",
            "Episode 92: reward: -95.230, steps: 11\n",
            "Episode 93: reward: -97.406, steps: 7\n",
            "Episode 94: reward: -98.528, steps: 6\n",
            "Episode 95: reward: -97.106, steps: 7\n",
            "Episode 96: reward: -97.762, steps: 6\n",
            "Episode 97: reward: -97.712, steps: 6\n",
            "Episode 98: reward: -95.908, steps: 10\n",
            "Episode 99: reward: -96.840, steps: 9\n",
            "Episode 100: reward: -97.762, steps: 6\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 59 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005, 'adam_epsilon': 0.001, 'batch_size': 32, 'target_model_update': 0.001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 32, 'unit_2': 32}\n",
            "\n",
            "Model: \"sequential_58\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_58 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_145 (Dense)           (None, 32)                1344      \n",
            "                                                                 \n",
            " dense_146 (Dense)           (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_147 (Dense)           (None, 45)                1485      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,885\n",
            "Trainable params: 3,885\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 258s 26ms/step - reward: -20.1112\n",
            "2047 episodes - episode_reward: -98.247 [-100.135, -93.618] - loss: 202.390 - mae: 59.159 - mean_q: -39.381\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 268s 27ms/step - reward: -18.0124\n",
            "1837 episodes - episode_reward: -98.054 [-100.135, -93.394] - loss: 31.558 - mae: 86.182 - mean_q: -80.233\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 267s 27ms/step - reward: -16.4634\n",
            "done, took 793.482 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -97.662, steps: 6\n",
            "Episode 2: reward: -97.712, steps: 6\n",
            "Episode 3: reward: -95.761, steps: 10\n",
            "Episode 4: reward: -94.951, steps: 10\n",
            "Episode 5: reward: -95.945, steps: 10\n",
            "Episode 6: reward: -97.562, steps: 6\n",
            "Episode 7: reward: -97.512, steps: 6\n",
            "Episode 8: reward: -95.881, steps: 10\n",
            "Episode 9: reward: -99.647, steps: 2\n",
            "Episode 10: reward: -96.962, steps: 6\n",
            "Episode 11: reward: -97.512, steps: 6\n",
            "Episode 12: reward: -97.462, steps: 6\n",
            "Episode 13: reward: -95.881, steps: 10\n",
            "Episode 14: reward: -97.662, steps: 6\n",
            "Episode 15: reward: -94.882, steps: 13\n",
            "Episode 16: reward: -98.636, steps: 3\n",
            "Episode 17: reward: -97.362, steps: 6\n",
            "Episode 18: reward: -97.562, steps: 6\n",
            "Episode 19: reward: -97.612, steps: 6\n",
            "Episode 20: reward: -97.462, steps: 6\n",
            "Episode 21: reward: -97.762, steps: 6\n",
            "Episode 22: reward: -97.612, steps: 6\n",
            "Episode 23: reward: -97.712, steps: 6\n",
            "Episode 24: reward: -97.540, steps: 7\n",
            "Episode 25: reward: -97.612, steps: 6\n",
            "Episode 26: reward: -97.712, steps: 6\n",
            "Episode 27: reward: -97.662, steps: 6\n",
            "Episode 28: reward: -95.818, steps: 10\n",
            "Episode 29: reward: -97.512, steps: 6\n",
            "Episode 30: reward: -97.462, steps: 6\n",
            "Episode 31: reward: -95.881, steps: 10\n",
            "Episode 32: reward: -97.762, steps: 6\n",
            "Episode 33: reward: -95.761, steps: 10\n",
            "Episode 34: reward: -97.662, steps: 6\n",
            "Episode 35: reward: -96.135, steps: 10\n",
            "Episode 36: reward: -96.135, steps: 10\n",
            "Episode 37: reward: -98.025, steps: 5\n",
            "Episode 38: reward: -95.788, steps: 9\n",
            "Episode 39: reward: -96.261, steps: 10\n",
            "Episode 40: reward: -97.662, steps: 6\n",
            "Episode 41: reward: -98.996, steps: 3\n",
            "Episode 42: reward: -95.691, steps: 10\n",
            "Episode 43: reward: -97.212, steps: 6\n",
            "Episode 44: reward: -97.562, steps: 6\n",
            "Episode 45: reward: -99.647, steps: 2\n",
            "Episode 46: reward: -95.565, steps: 10\n",
            "Episode 47: reward: -95.818, steps: 10\n",
            "Episode 48: reward: -97.462, steps: 6\n",
            "Episode 49: reward: -97.612, steps: 6\n",
            "Episode 50: reward: -97.512, steps: 6\n",
            "Episode 51: reward: -96.008, steps: 10\n",
            "Episode 52: reward: -97.487, steps: 7\n",
            "Episode 53: reward: -97.162, steps: 6\n",
            "Episode 54: reward: -97.604, steps: 8\n",
            "Episode 55: reward: -97.662, steps: 6\n",
            "Episode 56: reward: -97.712, steps: 6\n",
            "Episode 57: reward: -96.201, steps: 8\n",
            "Episode 58: reward: -97.612, steps: 6\n",
            "Episode 59: reward: -95.761, steps: 10\n",
            "Episode 60: reward: -99.763, steps: 3\n",
            "Episode 61: reward: -97.212, steps: 6\n",
            "Episode 62: reward: -95.755, steps: 10\n",
            "Episode 63: reward: -97.912, steps: 6\n",
            "Episode 64: reward: -97.512, steps: 6\n",
            "Episode 65: reward: -95.881, steps: 10\n",
            "Episode 66: reward: -97.662, steps: 6\n",
            "Episode 67: reward: -97.412, steps: 6\n",
            "Episode 68: reward: -97.562, steps: 6\n",
            "Episode 69: reward: -97.512, steps: 6\n",
            "Episode 70: reward: -95.945, steps: 10\n",
            "Episode 71: reward: -95.691, steps: 10\n",
            "Episode 72: reward: -97.712, steps: 6\n",
            "Episode 73: reward: -98.224, steps: 5\n",
            "Episode 74: reward: -95.501, steps: 10\n",
            "Episode 75: reward: -97.699, steps: 5\n",
            "Episode 76: reward: -97.812, steps: 6\n",
            "Episode 77: reward: -97.812, steps: 6\n",
            "Episode 78: reward: -96.342, steps: 10\n",
            "Episode 79: reward: -97.699, steps: 5\n",
            "Episode 80: reward: -97.812, steps: 6\n",
            "Episode 81: reward: -97.262, steps: 6\n",
            "Episode 82: reward: -96.008, steps: 10\n",
            "Episode 83: reward: -98.062, steps: 6\n",
            "Episode 84: reward: -95.825, steps: 10\n",
            "Episode 85: reward: -96.331, steps: 9\n",
            "Episode 86: reward: -97.912, steps: 6\n",
            "Episode 87: reward: -95.945, steps: 10\n",
            "Episode 88: reward: -96.501, steps: 8\n",
            "Episode 89: reward: -97.762, steps: 6\n",
            "Episode 90: reward: -97.254, steps: 8\n",
            "Episode 91: reward: -97.612, steps: 6\n",
            "Episode 92: reward: -95.015, steps: 10\n",
            "Episode 93: reward: -95.761, steps: 10\n",
            "Episode 94: reward: -97.662, steps: 6\n",
            "Episode 95: reward: -96.701, steps: 8\n",
            "Episode 96: reward: -97.712, steps: 6\n",
            "Episode 97: reward: -97.254, steps: 8\n",
            "Episode 98: reward: -96.451, steps: 9\n",
            "Episode 99: reward: -97.512, steps: 6\n",
            "Episode 100: reward: -99.974, steps: 1\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 60 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0005, 'adam_epsilon': 0.0001, 'batch_size': 32, 'target_model_update': 0.0005, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 1, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_59\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_59 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_148 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_149 (Dense)           (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,221\n",
            "Trainable params: 4,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 257s 25ms/step - reward: -19.8472\n",
            "2022 episodes - episode_reward: -98.157 [-99.974, -94.455] - loss: 279.967 - mae: 47.969 - mean_q: -23.742\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 270s 27ms/step - reward: -20.1254\n",
            "2051 episodes - episode_reward: -98.125 [-100.142, -93.651] - loss: 76.161 - mae: 78.448 - mean_q: -67.854\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 271s 27ms/step - reward: -19.2927\n",
            "done, took 798.189 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -99.398, steps: 4\n",
            "Episode 2: reward: -95.244, steps: 11\n",
            "Episode 3: reward: -98.596, steps: 6\n",
            "Episode 4: reward: -98.071, steps: 4\n",
            "Episode 5: reward: -97.362, steps: 6\n",
            "Episode 6: reward: -98.784, steps: 4\n",
            "Episode 7: reward: -99.411, steps: 4\n",
            "Episode 8: reward: -98.498, steps: 4\n",
            "Episode 9: reward: -97.841, steps: 5\n",
            "Episode 10: reward: -97.873, steps: 7\n",
            "Episode 11: reward: -99.647, steps: 2\n",
            "Episode 12: reward: -97.891, steps: 5\n",
            "Episode 13: reward: -97.439, steps: 7\n",
            "Episode 14: reward: -99.974, steps: 1\n",
            "Episode 15: reward: -98.117, steps: 4\n",
            "Episode 16: reward: -97.243, steps: 7\n",
            "Episode 17: reward: -97.791, steps: 5\n",
            "Episode 18: reward: -99.398, steps: 4\n",
            "Episode 19: reward: -98.378, steps: 4\n",
            "Episode 20: reward: -99.647, steps: 2\n",
            "Episode 21: reward: -99.616, steps: 3\n",
            "Episode 22: reward: -99.398, steps: 4\n",
            "Episode 23: reward: -98.618, steps: 4\n",
            "Episode 24: reward: -97.841, steps: 5\n",
            "Episode 25: reward: -98.418, steps: 4\n",
            "Episode 26: reward: -96.030, steps: 9\n",
            "Episode 27: reward: -96.207, steps: 9\n",
            "Episode 28: reward: -99.295, steps: 3\n",
            "Episode 29: reward: -99.295, steps: 3\n",
            "Episode 30: reward: -99.411, steps: 4\n",
            "Episode 31: reward: -97.891, steps: 5\n",
            "Episode 32: reward: -99.411, steps: 4\n",
            "Episode 33: reward: -96.598, steps: 9\n",
            "Episode 34: reward: -96.866, steps: 7\n",
            "Episode 35: reward: -98.677, steps: 4\n",
            "Episode 36: reward: -98.684, steps: 4\n",
            "Episode 37: reward: -96.143, steps: 9\n",
            "Episode 38: reward: -97.206, steps: 7\n",
            "Episode 39: reward: -95.943, steps: 9\n",
            "Episode 40: reward: -97.206, steps: 7\n",
            "Episode 41: reward: -99.295, steps: 3\n",
            "Episode 42: reward: -95.520, steps: 11\n",
            "Episode 43: reward: -95.908, steps: 10\n",
            "Episode 44: reward: -98.338, steps: 4\n",
            "Episode 45: reward: -98.362, steps: 6\n",
            "Episode 46: reward: -98.298, steps: 4\n",
            "Episode 47: reward: -99.411, steps: 4\n",
            "Episode 48: reward: -99.974, steps: 1\n",
            "Episode 49: reward: -98.458, steps: 4\n",
            "Episode 50: reward: -99.305, steps: 4\n",
            "Episode 51: reward: -96.740, steps: 9\n",
            "Episode 52: reward: -99.295, steps: 3\n",
            "Episode 53: reward: -98.562, steps: 6\n",
            "Episode 54: reward: -99.028, steps: 6\n",
            "Episode 55: reward: -95.880, steps: 9\n",
            "Episode 56: reward: -99.411, steps: 4\n",
            "Episode 57: reward: -97.339, steps: 7\n",
            "Episode 58: reward: -98.098, steps: 4\n",
            "Episode 59: reward: -98.298, steps: 4\n",
            "Episode 60: reward: -98.338, steps: 4\n",
            "Episode 61: reward: -96.570, steps: 9\n",
            "Episode 62: reward: -99.411, steps: 4\n",
            "Episode 63: reward: -96.740, steps: 9\n",
            "Episode 64: reward: -97.035, steps: 8\n",
            "Episode 65: reward: -98.338, steps: 4\n",
            "Episode 66: reward: -98.584, steps: 4\n",
            "Episode 67: reward: -97.891, steps: 5\n",
            "Episode 68: reward: -98.677, steps: 4\n",
            "Episode 69: reward: -99.035, steps: 6\n",
            "Episode 70: reward: -97.306, steps: 7\n",
            "Episode 71: reward: -96.932, steps: 8\n",
            "Episode 72: reward: -97.891, steps: 5\n",
            "Episode 73: reward: -99.974, steps: 1\n",
            "Episode 74: reward: -96.133, steps: 9\n",
            "Episode 75: reward: -99.974, steps: 1\n",
            "Episode 76: reward: -99.142, steps: 6\n",
            "Episode 77: reward: -97.841, steps: 5\n",
            "Episode 78: reward: -98.341, steps: 5\n",
            "Episode 79: reward: -98.498, steps: 4\n",
            "Episode 80: reward: -99.398, steps: 4\n",
            "Episode 81: reward: -96.429, steps: 9\n",
            "Episode 82: reward: -99.974, steps: 1\n",
            "Episode 83: reward: -97.006, steps: 7\n",
            "Episode 84: reward: -99.974, steps: 1\n",
            "Episode 85: reward: -98.368, steps: 6\n",
            "Episode 86: reward: -98.117, steps: 4\n",
            "Episode 87: reward: -99.002, steps: 6\n",
            "Episode 88: reward: -96.916, steps: 7\n",
            "Episode 89: reward: -97.841, steps: 5\n",
            "Episode 90: reward: -96.874, steps: 8\n",
            "Episode 91: reward: -97.489, steps: 7\n",
            "Episode 92: reward: -98.458, steps: 4\n",
            "Episode 93: reward: -99.411, steps: 4\n",
            "Episode 94: reward: -95.520, steps: 9\n",
            "Episode 95: reward: -98.458, steps: 4\n",
            "Episode 96: reward: -97.389, steps: 7\n",
            "Episode 97: reward: -98.677, steps: 4\n",
            "Episode 98: reward: -95.485, steps: 10\n",
            "Episode 99: reward: -98.462, steps: 6\n",
            "Episode 100: reward: -99.647, steps: 2\n",
            "\n",
            "#################\n",
            "Created 2160 combinations to test.\n",
            "Test 61 from 2160\n",
            "\n",
            "{'windows_length': 1, 'adam_learning_rate': 0.0001, 'adam_epsilon': 0.1, 'batch_size': 32, 'target_model_update': 0.0001, 'dueling_option': 'avg', 'activation': 'linear', 'layers': 2, 'unit_1': 48, 'unit_2': 16}\n",
            "\n",
            "Model: \"sequential_60\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_60 (Flatten)        (None, 41)                0         \n",
            "                                                                 \n",
            " dense_150 (Dense)           (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_151 (Dense)           (None, 16)                784       \n",
            "                                                                 \n",
            " dense_152 (Dense)           (None, 45)                765       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,565\n",
            "Trainable params: 3,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 30000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "10000/10000 [==============================] - 266s 26ms/step - reward: -22.0233\n",
            "2236 episodes - episode_reward: -98.495 [-100.142, -94.354] - loss: 606.510 - mae: 34.679 - mean_q: -7.369\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 267s 27ms/step - reward: -15.1010\n",
            "1550 episodes - episode_reward: -97.426 [-99.974, -94.302] - loss: 332.307 - mae: 45.200 - mean_q: -14.582\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            " 5125/10000 [==============>...............] - ETA: 2:12 - reward: -16.0141"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f55a1a53efcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Git/Kniffel/ai/ai.py\u001b[0m in \u001b[0;36mgrid_search_test\u001b[0;34m(self, nb_steps, env_config)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             csv = self.train(\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mhyperparameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             )\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Git/Kniffel/ai/ai.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, hyperparameter, nb_steps, load_path, env_config)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mload_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Git/Kniffel/ai/ai.py\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(self, actions, hyperparameter, env, nb_steps, callbacks, load_path)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             history = agent.fit(\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             )\n\u001b[1;32m    201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# the `on_train_end` method is properly called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mdid_abort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'did_abort'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdid_abort\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;34m\"\"\" Save model at the end of training \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_episode_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36msave_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# We convert to np.array() and then to list to convert from np datatypes to native datatypes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;31m# This is necessary because json.dump cannot handle np.float32, for example.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0msorted_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Overwrite already open file. We can simply seek to the beginning since the file will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "ai.grid_search_test(nb_steps=30_000, env_config=env_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DRxDEGqhfOP5",
        "outputId": "d8a32bbb-3559-4881-9e63-df271db23f7d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 41)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 48)                2016      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 48)                2352      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 45)                2205      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,573\n",
            "Trainable params: 6,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training for 2000000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 115s 11ms/step - reward: -22.7495\n",
            "2308 episodes - episode_reward: -98.568 [-100.142, -91.900] - loss: 101.474 - mae: 81.974 - mean_q: -72.572\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 120s 12ms/step - reward: -14.4569\n",
            "1485 episodes - episode_reward: -97.354 [-100.022, -92.207] - loss: 10.960 - mae: 88.752 - mean_q: -84.068\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 119s 12ms/step - reward: -13.3460\n",
            "1374 episodes - episode_reward: -97.131 [-100.135, -91.170] - loss: 9.420 - mae: 88.506 - mean_q: -83.999\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -12.9473\n",
            "1334 episodes - episode_reward: -97.058 [-100.142, -92.703] - loss: 8.536 - mae: 88.561 - mean_q: -84.185\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 121s 12ms/step - reward: -12.4414\n",
            "1284 episodes - episode_reward: -96.893 [-100.142, -91.438] - loss: 8.581 - mae: 88.240 - mean_q: -83.768\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 124s 12ms/step - reward: -12.3895\n",
            "1279 episodes - episode_reward: -96.870 [-100.142, -89.761] - loss: 8.549 - mae: 87.909 - mean_q: -83.363\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 130s 13ms/step - reward: -12.0999\n",
            "1250 episodes - episode_reward: -96.801 [-100.142, -90.331] - loss: 7.921 - mae: 88.103 - mean_q: -83.745\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 135s 13ms/step - reward: -12.1086\n",
            "1251 episodes - episode_reward: -96.790 [-100.135, -90.325] - loss: 7.837 - mae: 87.989 - mean_q: -83.655\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 138s 14ms/step - reward: -11.7902\n",
            "1219 episodes - episode_reward: -96.720 [-100.135, -90.864] - loss: 7.409 - mae: 88.002 - mean_q: -83.805\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 139s 14ms/step - reward: -11.4379\n",
            "1184 episodes - episode_reward: -96.602 [-100.109, -89.433] - loss: 7.323 - mae: 88.005 - mean_q: -83.906\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 143s 14ms/step - reward: -11.5777\n",
            "1198 episodes - episode_reward: -96.644 [-100.142, -91.095] - loss: 7.352 - mae: 87.939 - mean_q: -83.789\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 145s 15ms/step - reward: -11.5805\n",
            "1198 episodes - episode_reward: -96.663 [-100.109, -90.354] - loss: 6.944 - mae: 88.105 - mean_q: -84.047\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 148s 15ms/step - reward: -11.6436\n",
            "1204 episodes - episode_reward: -96.710 [-100.129, -90.748] - loss: 6.775 - mae: 88.103 - mean_q: -84.089\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 147s 15ms/step - reward: -11.1088\n",
            "1151 episodes - episode_reward: -96.520 [-100.109, -90.704] - loss: 6.397 - mae: 88.266 - mean_q: -84.341\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 154s 15ms/step - reward: -11.1807\n",
            "1158 episodes - episode_reward: -96.544 [-100.142, -90.015] - loss: 6.481 - mae: 88.102 - mean_q: -84.076\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 155s 16ms/step - reward: -11.1891\n",
            "1159 episodes - episode_reward: -96.540 [-100.142, -89.751] - loss: 6.165 - mae: 88.293 - mean_q: -84.482\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 158s 16ms/step - reward: -11.2197\n",
            "1162 episodes - episode_reward: -96.558 [-100.135, -90.178] - loss: 6.037 - mae: 88.343 - mean_q: -84.558\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 157s 16ms/step - reward: -11.1001\n",
            "1150 episodes - episode_reward: -96.521 [-100.135, -90.588] - loss: 6.202 - mae: 88.146 - mean_q: -84.211\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 154s 15ms/step - reward: -10.8494\n",
            "1125 episodes - episode_reward: -96.441 [-100.142, -90.354] - loss: 6.029 - mae: 88.191 - mean_q: -84.405\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 158s 16ms/step - reward: -10.7487\n",
            "1115 episodes - episode_reward: -96.400 [-100.142, -90.280] - loss: 6.070 - mae: 88.042 - mean_q: -84.193\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 170s 17ms/step - reward: -10.8893\n",
            "1129 episodes - episode_reward: -96.453 [-100.142, -89.991] - loss: 6.142 - mae: 88.015 - mean_q: -84.175\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 175s 17ms/step - reward: -10.7070\n",
            "1111 episodes - episode_reward: -96.373 [-100.142, -90.385] - loss: 6.014 - mae: 88.022 - mean_q: -84.208\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 178s 18ms/step - reward: -10.6692\n",
            "1107 episodes - episode_reward: -96.379 [-100.135, -90.318] - loss: 6.129 - mae: 87.943 - mean_q: -84.063\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 179s 18ms/step - reward: -10.4366\n",
            "1084 episodes - episode_reward: -96.277 [-99.974, -89.718] - loss: 6.243 - mae: 87.897 - mean_q: -84.067\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 185s 18ms/step - reward: -10.4608\n",
            "1086 episodes - episode_reward: -96.324 [-100.142, -87.401] - loss: 6.576 - mae: 87.642 - mean_q: -83.642\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 187s 19ms/step - reward: -10.5103\n",
            "1091 episodes - episode_reward: -96.335 [-100.142, -91.038] - loss: 6.214 - mae: 87.894 - mean_q: -84.092\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 189s 19ms/step - reward: -10.2967\n",
            "1070 episodes - episode_reward: -96.232 [-100.142, -87.095] - loss: 5.877 - mae: 87.905 - mean_q: -84.141\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 198s 20ms/step - reward: -10.4205\n",
            "1082 episodes - episode_reward: -96.308 [-100.142, -88.764] - loss: 6.034 - mae: 87.914 - mean_q: -84.073\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 199s 20ms/step - reward: -10.5412\n",
            "1094 episodes - episode_reward: -96.353 [-100.142, -90.161] - loss: 5.667 - mae: 88.090 - mean_q: -84.360\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 203s 20ms/step - reward: -10.1214\n",
            "1052 episodes - episode_reward: -96.213 [-100.142, -86.511] - loss: 5.462 - mae: 88.186 - mean_q: -84.525\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 206s 21ms/step - reward: -10.1434\n",
            "1054 episodes - episode_reward: -96.239 [-100.142, -88.358] - loss: 5.539 - mae: 88.092 - mean_q: -84.415\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 201s 20ms/step - reward: -10.2531\n",
            "1065 episodes - episode_reward: -96.272 [-100.142, -88.694] - loss: 5.585 - mae: 87.959 - mean_q: -84.202\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 204s 20ms/step - reward: -10.0520\n",
            "1045 episodes - episode_reward: -96.192 [-100.142, -89.698] - loss: 5.351 - mae: 88.188 - mean_q: -84.563\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 209s 21ms/step - reward: -10.1026\n",
            "1050 episodes - episode_reward: -96.213 [-100.142, -89.666] - loss: 5.348 - mae: 88.142 - mean_q: -84.513\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -9.8385\n",
            "1024 episodes - episode_reward: -96.079 [-100.142, -90.142] - loss: 5.379 - mae: 88.093 - mean_q: -84.436\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 225s 22ms/step - reward: -10.1738\n",
            "1057 episodes - episode_reward: -96.252 [-100.142, -90.086] - loss: 5.275 - mae: 88.065 - mean_q: -84.393\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 229s 23ms/step - reward: -10.0113\n",
            "1041 episodes - episode_reward: -96.170 [-100.142, -89.911] - loss: 5.416 - mae: 88.019 - mean_q: -84.335\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 224s 22ms/step - reward: -9.8798\n",
            "1028 episodes - episode_reward: -96.107 [-100.142, -90.568] - loss: 5.346 - mae: 88.049 - mean_q: -84.377\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 226s 23ms/step - reward: -10.1106\n",
            "1051 episodes - episode_reward: -96.204 [-100.142, -89.241] - loss: 5.592 - mae: 87.867 - mean_q: -84.064\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 233s 23ms/step - reward: -10.0566\n",
            "1045 episodes - episode_reward: -96.231 [-100.142, -90.192] - loss: 5.705 - mae: 87.796 - mean_q: -83.990\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 232s 23ms/step - reward: -10.1066\n",
            "1050 episodes - episode_reward: -96.255 [-100.142, -90.366] - loss: 5.568 - mae: 87.854 - mean_q: -84.140\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 232s 23ms/step - reward: -9.9415\n",
            "1034 episodes - episode_reward: -96.146 [-100.142, -90.081] - loss: 5.185 - mae: 88.089 - mean_q: -84.513\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 229s 23ms/step - reward: -10.1777\n",
            "1057 episodes - episode_reward: -96.291 [-100.142, -87.091] - loss: 5.567 - mae: 87.737 - mean_q: -83.982\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 242s 24ms/step - reward: -9.6990\n",
            "1010 episodes - episode_reward: -96.029 [-100.142, -88.231] - loss: 5.134 - mae: 88.055 - mean_q: -84.466\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 235s 24ms/step - reward: -9.8637\n",
            "1026 episodes - episode_reward: -96.137 [-100.142, -90.031] - loss: 5.227 - mae: 87.946 - mean_q: -84.247\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 236s 24ms/step - reward: -9.9726\n",
            "1037 episodes - episode_reward: -96.166 [-100.142, -90.171] - loss: 5.135 - mae: 88.006 - mean_q: -84.439\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 242s 24ms/step - reward: -9.5871\n",
            "999 episodes - episode_reward: -95.966 [-100.142, -87.911] - loss: 5.275 - mae: 87.914 - mean_q: -84.234\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 251s 25ms/step - reward: -9.9505\n",
            "1035 episodes - episode_reward: -96.145 [-100.142, -89.812] - loss: 5.061 - mae: 88.017 - mean_q: -84.410\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 251s 25ms/step - reward: -10.0599\n",
            "1046 episodes - episode_reward: -96.174 [-100.142, -87.114] - loss: 5.145 - mae: 87.959 - mean_q: -84.334\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 262s 26ms/step - reward: -10.0290\n",
            "1043 episodes - episode_reward: -96.155 [-100.142, -89.504] - loss: 5.136 - mae: 87.991 - mean_q: -84.370\n",
            "\n",
            "Interval 51 (500000 steps performed)\n",
            "10000/10000 [==============================] - 262s 26ms/step - reward: -9.8008\n",
            "1020 episodes - episode_reward: -96.084 [-100.142, -90.381] - loss: 4.982 - mae: 88.049 - mean_q: -84.476\n",
            "\n",
            "Interval 52 (510000 steps performed)\n",
            "10000/10000 [==============================] - 246s 25ms/step - reward: -9.7565\n",
            "1016 episodes - episode_reward: -96.028 [-100.142, -90.321] - loss: 5.011 - mae: 88.016 - mean_q: -84.421\n",
            "\n",
            "Interval 53 (520000 steps performed)\n",
            "10000/10000 [==============================] - 238s 24ms/step - reward: -9.9493\n",
            "1035 episodes - episode_reward: -96.129 [-100.142, -87.917] - loss: 5.079 - mae: 87.942 - mean_q: -84.359\n",
            "\n",
            "Interval 54 (530000 steps performed)\n",
            "10000/10000 [==============================] - 232s 23ms/step - reward: -10.0900\n",
            "1049 episodes - episode_reward: -96.186 [-100.022, -88.931] - loss: 5.141 - mae: 87.927 - mean_q: -84.357\n",
            "\n",
            "Interval 55 (540000 steps performed)\n",
            "10000/10000 [==============================] - 224s 22ms/step - reward: -9.8075\n",
            "1021 episodes - episode_reward: -96.057 [-100.142, -86.538] - loss: 5.204 - mae: 87.883 - mean_q: -84.245\n",
            "\n",
            "Interval 56 (550000 steps performed)\n",
            "10000/10000 [==============================] - 224s 22ms/step - reward: -9.8414\n",
            "1024 episodes - episode_reward: -96.111 [-100.142, -89.558] - loss: 5.141 - mae: 88.041 - mean_q: -84.454\n",
            "\n",
            "Interval 57 (560000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -9.6427\n",
            "1005 episodes - episode_reward: -95.945 [-100.142, -89.718] - loss: 5.436 - mae: 87.667 - mean_q: -83.899\n",
            "\n",
            "Interval 58 (570000 steps performed)\n",
            "10000/10000 [==============================] - 230s 23ms/step - reward: -9.7269\n",
            "1013 episodes - episode_reward: -96.023 [-100.142, -89.578] - loss: 5.345 - mae: 87.793 - mean_q: -84.119\n",
            "\n",
            "Interval 59 (580000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.7656\n",
            "1017 episodes - episode_reward: -96.021 [-100.142, -88.318] - loss: 5.484 - mae: 87.655 - mean_q: -83.878\n",
            "\n",
            "Interval 60 (590000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -9.7651\n",
            "1017 episodes - episode_reward: -96.021 [-100.142, -88.925] - loss: 5.292 - mae: 87.818 - mean_q: -84.148\n",
            "\n",
            "Interval 61 (600000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -9.6471\n",
            "1005 episodes - episode_reward: -95.992 [-100.142, -90.068] - loss: 5.605 - mae: 87.543 - mean_q: -83.738\n",
            "\n",
            "Interval 62 (610000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -9.6651\n",
            "1007 episodes - episode_reward: -95.978 [-100.142, -90.001] - loss: 5.642 - mae: 87.544 - mean_q: -83.726\n",
            "\n",
            "Interval 63 (620000 steps performed)\n",
            "10000/10000 [==============================] - 218s 22ms/step - reward: -9.8246\n",
            "1023 episodes - episode_reward: -96.035 [-100.142, -88.419] - loss: 6.114 - mae: 87.386 - mean_q: -83.496\n",
            "\n",
            "Interval 64 (630000 steps performed)\n",
            "10000/10000 [==============================] - 217s 22ms/step - reward: -9.6334\n",
            "1004 episodes - episode_reward: -95.949 [-100.142, -87.335] - loss: 5.493 - mae: 87.632 - mean_q: -83.909\n",
            "\n",
            "Interval 65 (640000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.7333\n",
            "1014 episodes - episode_reward: -95.993 [-100.142, -88.694] - loss: 5.543 - mae: 87.533 - mean_q: -83.728\n",
            "\n",
            "Interval 66 (650000 steps performed)\n",
            "10000/10000 [==============================] - 219s 22ms/step - reward: -9.7279\n",
            "1013 episodes - episode_reward: -96.026 [-100.142, -87.785] - loss: 5.359 - mae: 87.708 - mean_q: -83.962\n",
            "\n",
            "Interval 67 (660000 steps performed)\n",
            "10000/10000 [==============================] - 219s 22ms/step - reward: -9.8641\n",
            "1027 episodes - episode_reward: -96.051 [-100.135, -89.431] - loss: 5.467 - mae: 87.611 - mean_q: -83.811\n",
            "\n",
            "Interval 68 (670000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -10.4201\n",
            "1081 episodes - episode_reward: -96.390 [-100.142, -89.562] - loss: 5.907 - mae: 87.120 - mean_q: -83.144\n",
            "\n",
            "Interval 69 (680000 steps performed)\n",
            "10000/10000 [==============================] - 220s 22ms/step - reward: -9.7566\n",
            "1016 episodes - episode_reward: -96.031 [-100.142, -89.048] - loss: 5.231 - mae: 87.705 - mean_q: -83.977\n",
            "\n",
            "Interval 70 (690000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.9046\n",
            "1031 episodes - episode_reward: -96.068 [-100.142, -89.393] - loss: 5.602 - mae: 87.383 - mean_q: -83.565\n",
            "\n",
            "Interval 71 (700000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.7739\n",
            "1017 episodes - episode_reward: -96.109 [-100.142, -88.178] - loss: 5.689 - mae: 87.251 - mean_q: -83.308\n",
            "\n",
            "Interval 72 (710000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.7497\n",
            "1015 episodes - episode_reward: -96.056 [-100.142, -89.829] - loss: 5.151 - mae: 87.792 - mean_q: -84.171\n",
            "\n",
            "Interval 73 (720000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.7448\n",
            "1015 episodes - episode_reward: -96.008 [-100.142, -89.142] - loss: 5.137 - mae: 87.733 - mean_q: -84.042\n",
            "\n",
            "Interval 74 (730000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -10.0673\n",
            "1046 episodes - episode_reward: -96.245 [-100.142, -90.221] - loss: 5.377 - mae: 87.572 - mean_q: -83.854\n",
            "\n",
            "Interval 75 (740000 steps performed)\n",
            "10000/10000 [==============================] - 225s 22ms/step - reward: -9.7783\n",
            "1018 episodes - episode_reward: -96.051 [-100.142, -87.938] - loss: 5.165 - mae: 87.749 - mean_q: -84.057\n",
            "\n",
            "Interval 76 (750000 steps performed)\n",
            "10000/10000 [==============================] - 226s 23ms/step - reward: -9.4441\n",
            "985 episodes - episode_reward: -95.881 [-100.142, -88.968] - loss: 5.228 - mae: 87.673 - mean_q: -83.944\n",
            "\n",
            "Interval 77 (760000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.7267\n",
            "1013 episodes - episode_reward: -96.017 [-100.135, -87.158] - loss: 5.159 - mae: 87.681 - mean_q: -83.992\n",
            "\n",
            "Interval 78 (770000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.5836\n",
            "999 episodes - episode_reward: -95.931 [-100.142, -87.918] - loss: 5.137 - mae: 87.736 - mean_q: -84.028\n",
            "\n",
            "Interval 79 (780000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.6752\n",
            "1008 episodes - episode_reward: -95.985 [-100.142, -86.343] - loss: 5.188 - mae: 87.722 - mean_q: -84.059\n",
            "\n",
            "Interval 80 (790000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.8590\n",
            "1026 episodes - episode_reward: -96.095 [-100.142, -87.744] - loss: 5.119 - mae: 87.822 - mean_q: -84.168\n",
            "\n",
            "Interval 81 (800000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -10.1947\n",
            "1059 episodes - episode_reward: -96.266 [-100.142, -90.604] - loss: 6.000 - mae: 87.007 - mean_q: -83.052\n",
            "\n",
            "Interval 82 (810000 steps performed)\n",
            "10000/10000 [==============================] - 221s 22ms/step - reward: -9.6582\n",
            "1006 episodes - episode_reward: -96.007 [-100.142, -88.221] - loss: 5.344 - mae: 87.543 - mean_q: -83.743\n",
            "\n",
            "Interval 83 (820000 steps performed)\n",
            "10000/10000 [==============================] - 223s 22ms/step - reward: -9.5970\n",
            "1000 episodes - episode_reward: -95.968 [-100.142, -88.708] - loss: 5.015 - mae: 87.872 - mean_q: -84.273\n",
            "\n",
            "Interval 84 (830000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.6261\n",
            "1003 episodes - episode_reward: -95.975 [-100.142, -87.899] - loss: 5.186 - mae: 87.658 - mean_q: -83.948\n",
            "\n",
            "Interval 85 (840000 steps performed)\n",
            "10000/10000 [==============================] - 222s 22ms/step - reward: -9.5275\n",
            "993 episodes - episode_reward: -95.945 [-100.142, -88.332] - loss: 5.175 - mae: 87.729 - mean_q: -84.051\n",
            "\n",
            "Interval 86 (850000 steps performed)\n",
            " 9011/10000 [==========================>...] - ETA: 22s - reward: -9.5399done, took 17291.935 seconds\n",
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: -94.333, steps: 14\n",
            "Episode 2: reward: -90.524, steps: 18\n",
            "Episode 3: reward: -96.258, steps: 10\n",
            "Episode 4: reward: -91.831, steps: 18\n",
            "Episode 5: reward: -93.707, steps: 14\n",
            "Episode 6: reward: -95.398, steps: 10\n",
            "Episode 7: reward: -93.873, steps: 14\n",
            "Episode 8: reward: -96.358, steps: 10\n",
            "Episode 9: reward: -91.904, steps: 18\n",
            "Episode 10: reward: -95.285, steps: 10\n",
            "Episode 11: reward: -95.475, steps: 10\n",
            "Episode 12: reward: -94.150, steps: 14\n",
            "Episode 13: reward: -92.933, steps: 14\n",
            "Episode 14: reward: -96.588, steps: 10\n",
            "Episode 15: reward: -93.827, steps: 14\n",
            "Episode 16: reward: -95.948, steps: 10\n",
            "Episode 17: reward: -95.695, steps: 10\n",
            "Episode 18: reward: -99.647, steps: 2\n",
            "Episode 19: reward: -96.538, steps: 10\n",
            "Episode 20: reward: -93.687, steps: 14\n",
            "Episode 21: reward: -94.120, steps: 14\n",
            "Episode 22: reward: -91.438, steps: 18\n",
            "Episode 23: reward: -93.047, steps: 14\n",
            "Episode 24: reward: -94.838, steps: 10\n",
            "Episode 25: reward: -93.865, steps: 13\n",
            "Episode 26: reward: -94.473, steps: 14\n",
            "Episode 27: reward: -94.017, steps: 14\n",
            "Episode 28: reward: -89.454, steps: 22\n",
            "Episode 29: reward: -95.641, steps: 10\n",
            "Episode 30: reward: -96.011, steps: 10\n",
            "Episode 31: reward: -91.034, steps: 18\n",
            "Episode 32: reward: -94.828, steps: 10\n",
            "Episode 33: reward: -95.765, steps: 10\n",
            "Episode 34: reward: -95.948, steps: 10\n",
            "Episode 35: reward: -93.423, steps: 14\n",
            "Episode 36: reward: -96.010, steps: 9\n",
            "Episode 37: reward: -90.455, steps: 17\n",
            "Episode 38: reward: -97.312, steps: 6\n",
            "Episode 39: reward: -92.890, steps: 14\n",
            "Episode 40: reward: -90.748, steps: 18\n",
            "Episode 41: reward: -90.074, steps: 18\n",
            "Episode 42: reward: -87.574, steps: 22\n",
            "Episode 43: reward: -96.608, steps: 10\n",
            "Episode 44: reward: -94.473, steps: 14\n",
            "Episode 45: reward: -95.948, steps: 10\n",
            "Episode 46: reward: -89.881, steps: 18\n",
            "Episode 47: reward: -96.730, steps: 9\n",
            "Episode 48: reward: -91.828, steps: 18\n",
            "Episode 49: reward: -96.158, steps: 10\n",
            "Episode 50: reward: -90.348, steps: 18\n",
            "Episode 51: reward: -93.533, steps: 14\n",
            "Episode 52: reward: -93.903, steps: 14\n",
            "Episode 53: reward: -92.890, steps: 14\n",
            "Episode 54: reward: -95.895, steps: 10\n",
            "Episode 55: reward: -90.964, steps: 18\n",
            "Episode 56: reward: -94.280, steps: 14\n",
            "Episode 57: reward: -93.467, steps: 14\n",
            "Episode 58: reward: -95.870, steps: 9\n",
            "Episode 59: reward: -94.350, steps: 14\n",
            "Episode 60: reward: -95.701, steps: 10\n",
            "Episode 61: reward: -96.538, steps: 10\n",
            "Episode 62: reward: -95.695, steps: 10\n",
            "Episode 63: reward: -93.477, steps: 14\n",
            "Episode 64: reward: -93.630, steps: 14\n",
            "Episode 65: reward: -90.538, steps: 18\n",
            "Episode 66: reward: -93.827, steps: 14\n",
            "Episode 67: reward: -94.633, steps: 14\n",
            "Episode 68: reward: -93.810, steps: 14\n",
            "Episode 69: reward: -93.947, steps: 14\n",
            "Episode 70: reward: -97.362, steps: 6\n",
            "Episode 71: reward: -93.740, steps: 14\n",
            "Episode 72: reward: -94.588, steps: 13\n",
            "Episode 73: reward: -92.148, steps: 18\n",
            "Episode 74: reward: -90.894, steps: 18\n",
            "Episode 75: reward: -95.228, steps: 10\n",
            "Episode 76: reward: -95.215, steps: 10\n",
            "Episode 77: reward: -96.580, steps: 9\n",
            "Episode 78: reward: -93.687, steps: 14\n",
            "Episode 79: reward: -96.538, steps: 10\n",
            "Episode 80: reward: -93.630, steps: 14\n",
            "Episode 81: reward: -93.533, steps: 14\n",
            "Episode 82: reward: -95.551, steps: 10\n",
            "Episode 83: reward: -96.738, steps: 10\n",
            "Episode 84: reward: -94.281, steps: 13\n",
            "Episode 85: reward: -94.347, steps: 14\n",
            "Episode 86: reward: -95.948, steps: 10\n",
            "Episode 87: reward: -93.877, steps: 14\n",
            "Episode 88: reward: -94.267, steps: 14\n",
            "Episode 89: reward: -93.610, steps: 14\n",
            "Episode 90: reward: -91.254, steps: 18\n",
            "Episode 91: reward: -94.650, steps: 14\n",
            "Episode 92: reward: -94.713, steps: 14\n",
            "Episode 93: reward: -93.863, steps: 14\n",
            "Episode 94: reward: -96.288, steps: 10\n",
            "Episode 95: reward: -92.517, steps: 14\n",
            "Episode 96: reward: -96.201, steps: 10\n",
            "Episode 97: reward: -91.078, steps: 18\n",
            "Episode 98: reward: -94.097, steps: 14\n",
            "Episode 99: reward: -96.011, steps: 10\n",
            "Episode 100: reward: -90.931, steps: 18\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'17299.409345;2000000;-96.34152733108866;-86.34307692307692;-100.14205128205128;-94.0920846153846;-87.57435897435897;-99.64743589743588;17.85206143896524;55;0;100;100;1;0.00146;32;0.01;1e-05;avg;linear;2;48;48\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "hyperparameter = {\n",
        "    \"windows_length\": 1,\n",
        "    \"adam_learning_rate\": 0.00146,\n",
        "    \"batch_size\": 32,\n",
        "    \"target_model_update\": 0.01,\n",
        "    \"adam_epsilon\": 0.00001,\n",
        "    \"dueling_option\": \"avg\",\n",
        "    \"activation\": \"linear\",\n",
        "    \"layers\": 2,\n",
        "    \"unit_1\": 48,\n",
        "    \"unit_2\": 48,\n",
        "}\n",
        "\n",
        "#ai.train(\n",
        "#    hyperparameter=hyperparameter,\n",
        "#    nb_steps=2_000_000,\n",
        "#    env_config=env_config\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-FMgpdHFMog"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ai-model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "7724bf7a85847efc5bb6a1829c232f0862f78efa3c8535cdc9aa81401d00c18c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.11 ('ai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}